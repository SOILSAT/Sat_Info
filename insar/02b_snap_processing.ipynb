{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to form interferograms using ESA SNAP. Shifting to SNAP because of possible NASA funding cuts; might impact the support for both ISCE2 and ISCE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_help():\n",
    "\n",
    "    cmd = f'{GPT_PATH} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def help_cmd(cmdname):\n",
    "\n",
    "    cmd = f'{GPT_PATH} {cmdname} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def project_dir(work_dir):\n",
    "    \"\"\"\n",
    "    This function reads in a string that you wish to make your working directory \n",
    "    for the InSAR project, and creates a data directory to store the data for ISCE2 and mintpy\n",
    "    work-dir = str\n",
    "        path to the directory created in 01_get_slc.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    #creates file on your desktop containing the work of this notebook\n",
    "    os.makedirs(work_dir, exist_ok=True, mode=0o777)\n",
    "    \n",
    "    # file inside work_dir for isce2 interferometry\n",
    "    if_dir = os.path.join(work_dir,'InSAR/interferometry')\n",
    "    os.makedirs(if_dir, exist_ok=True, mode=0o777)\n",
    "    \n",
    "    # file inside work_dir for mintpy time-series\n",
    "    ts_dir = os.path.join(work_dir,'InSAR/time_series')\n",
    "    os.makedirs(ts_dir, exist_ok=True, mode=0o777)\n",
    "\n",
    "    return if_dir, ts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GPT path, number of processors, and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_PATH = '/home/clay/esa-snap/bin/gpt'          #linux\n",
    "# GPT_PATH = '/Applications/esa-snap/bin/gpt'         #mac\n",
    "NUM_PROCESSORS = 12                                 # 24 total on my linux, 14 total on mac\n",
    "MEMORY_SIZE = '40G'                                 # 96G total on my linux, 24G total on mac\n",
    "SNAPHU_PATH = '/usr/local/bin/snaphu'               # path to snaphu for unwrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SLC images that were downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have downloaded .zip files covering your AOI from ASF Vertex\n",
    "# enter the file directory below\n",
    "slc_zips = '/home/clay/Documents/SabineRS/Sentinel-1/SLC/ASCENDING/136/93'\n",
    "\n",
    "slc_zips_list = sorted(os.listdir(slc_zips), key=lambda x: datetime.strptime(x[17:25], '%Y%m%d'))\n",
    "slc_zips_dirs = [os.path.join(slc_zips, slc) for slc in slc_zips_list]\n",
    "slc_zips_dates = [slc[17:25] for slc in slc_zips_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories for SNAP interferogram and MintPy\n",
    "Needs for MintPy are:\n",
    "1. Wrapped Ifg\n",
    "2. Elevation Band\n",
    "3. Coherence Band\n",
    "4. Unwrapped ifg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory structure much simpler than ISCE:\n",
    "1. Ifg directory\n",
    "    - stores wrapped ifg .dim and .data files (optional) \n",
    "    - stores coherence band .dim and .data files\n",
    "    - stores elevation band .dim and .data files\n",
    "    - stores unwrapped ifg .dim and .data files\n",
    "2. Referance DEM .dim and .data file\n",
    "3. MintPy directory\n",
    "    - MintPy .txt config file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '/home/clay/Documents/SabineRS'\n",
    "work_dir = os.path.join(proj_dir, 'Sentinel-1')\n",
    "if_dir, ts_dir= project_dir(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which polarizations and DEM you want to use for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can define this here for the rest of the notebook\n",
    "# typically VV has highest coherence due to reliance on surface scattering\n",
    "# VH largely impacted by volumetric scattering present in vegetated areas\n",
    "\n",
    "POLARISATIONS = 'VV' \n",
    "\n",
    "# repeated a lot, so leaving this here as well\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TOPSAR-Split\n",
    "- will try to add something in here where can input an aoi and automatically identify the swaths needed\n",
    "- wkt aoi can be passed to get the bursts, but not sure if it also works for subswaths\n",
    "- ABraun recommends splitting before applying orbit file, maybe will save time?\n",
    "- do for all images\n",
    "\n",
    "- May need to just go in manually for the reference file to identify subswaths and bursts :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this, could be interesting to keep both polarisations\n",
    "# would use VV for the interferograms but could include VH for coherence time-series?\n",
    "# this uses all subswaths. Will probably take longer but it make\n",
    "\n",
    "#after manual inspection\n",
    "# maybe add a for loop to do more than one subswath (e.g. IW2 and IW3)\n",
    "SUBSWATH = 'IW3'\n",
    "FIRSTBURST = 2\n",
    "LASTBURST = 6\n",
    "\n",
    "# with open('/Users/clayc/Documents/Dissertation/SabineRS/wkt_aoi.txt') as f:        \n",
    "#     lines = f.readlines()\n",
    "# aoi = lines[0]\n",
    "\n",
    "splits_path = os.path.join(if_dir, '01_split')\n",
    "os.makedirs(splits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "split_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    split_outpaths.append(os.path.join(splits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    split_cmd = f'{GPT_PATH} TOPSAR-Split -Ssource={os.path.join(slc_zips,file)} -PselectedPolarisations={POLARISATIONS} -Psubswath=\"{SUBSWATH}\" -PfirstBurstIndex={FIRSTBURST} -PlastBurstIndex={LASTBURST} -t {split_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(split_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply Orbit File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORBIT_TYPE = 'Sentinel Precise (Auto Download)'     # str, options include'Sentinel Precise (Auto Download)', 'Sentinel Restituted (Auto Download)', 'DORIS Preliminary POR (ENVISAT)', 'DORIS Precise VOR (ENVISAT) (Auto Download)', 'DELFT Precise (ENVISAT, ERS1&2) (Auto Download)', 'PRARE Precise (ERS1&2) (Auto Download)', 'Kompsat5 Precise'\n",
    "ORBIT_DEGREE = 3                                     # int\n",
    "CONTINUE_ON_FAIL = False\n",
    "\n",
    "orbits_path = os.path.join(if_dir, '02_orbit')\n",
    "os.makedirs(orbits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "orbit_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    orbit_outpaths.append(os.path.join(orbits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    orbit_cmd = f'{GPT_PATH} Apply-Orbit-File -Ssource={split_outpaths[i]} -PcontinueOnFail={CONTINUE_ON_FAIL} -PorbitType=\"{ORBIT_TYPE}\" -PpolyDegree={ORBIT_DEGREE} -t {orbit_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(orbit_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(splits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Back-geocoding Co-reg\n",
    "- this one is funky. Need to pass the two files for the ifg together?\n",
    "- pass with -SsourceProducts?\n",
    "- look into this more, may need to pass the two SLCs being use to produce each ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbits_path = os.path.join(if_dir, '02_orbit')\n",
    "os.makedirs(orbits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "orbit_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    orbit_outpaths.append(os.path.join(orbits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "pairs = []\n",
    "# Generate triplets for every consecutive set of 3 images\n",
    "for i in range(len(orbit_outpaths)-2):  \n",
    "    # Form the triplet from 3 consecutive images\n",
    "    A = orbit_outpaths[i]\n",
    "    B = orbit_outpaths[i+1]\n",
    "    C = orbit_outpaths[i+2]\n",
    "    \n",
    "    # Create the three pairs from each triplet\n",
    "    pairs.append((A,B))  # i and i+1\n",
    "    pairs.append((B,C))  # i+1 and i+2 \n",
    "    pairs.append((C,A))  # i+2 and i\n",
    "\n",
    "# Remove any duplicate pairs\n",
    "pairs = list(set(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(pairs[:3]):\n",
    "    filename = f'{pairs[i][0][-12:-4]}_{pairs[i][1][-12:-4]}.dim'\n",
    "    pair_str=f'{pairs[i][0]},{pairs[i][1]}'\n",
    "    print(filename)\n",
    "    print(pair_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### EXAMPLE .XML FILE FOR back geocoding\n",
    "# format similar to the nc_export.xml I created\n",
    "\n",
    "# <graph id=\"Graph\">\n",
    "#   <version>1.0</version>\n",
    "#   <node id=\"Back-Geocoding\">\n",
    "#     <operator>Back-Geocoding</operator>\n",
    "#     <sources>\n",
    "#       <sourceProduct.2 refid=\"ProductSet-Reader\"/>\n",
    "#     </sources>\n",
    "#     <parameters class=\"com.bc.ceres.binding.dom.XppDomElement\">\n",
    "#       <demName>${demName}</demName>\n",
    "#       <demResamplingMethod>${demResamplingMethod}</demResamplingMethod>\n",
    "#       <externalDEMFile/>\n",
    "#       <externalDEMNoDataValue>0.0</externalDEMNoDataValue>\n",
    "#       <resamplingType>${resamplingType}</resamplingType>\n",
    "#       <maskOutAreaWithoutElevation>${maskOutAreaWithoutElevation}</maskOutAreaWithoutElevation>\n",
    "#       <outputRangeAzimuthOffset>${outputRangeAzimuthOffset}</outputRangeAzimuthOffset>\n",
    "#       <outputDerampDemodPhase>${outputDerampDemodPhase}</outputDerampDemodPhase>\n",
    "#       <disableReramp>${disableReramp}</disableReramp>\n",
    "#     </parameters>\n",
    "#   </node>\n",
    "#   <node id=\"Write\">\n",
    "#     <operator>Write</operator>\n",
    "#     <sources>\n",
    "#       <sourceProduct refid=\"Back-Geocoding\"/>\n",
    "#     </sources>\n",
    "#     <parameters class=\"com.bc.ceres.binding.dom.XppDomElement\">\n",
    "#       <file>${output}</file>\n",
    "#       <formatName>BEAM-DIMAP</formatName>\n",
    "#     </parameters>\n",
    "#   </node>\n",
    "#   <node id=\"ProductSet-Reader\">\n",
    "#     <operator>ProductSet-Reader</operator>\n",
    "#     <sources/>\n",
    "#     <parameters class=\"com.bc.ceres.binding.dom.XppDomElement\">\n",
    "#       <fileList>${fileList}</fileList>\n",
    "#     </parameters>\n",
    "#   </node>\n",
    "# </graph>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_RESAMPLE_METHOD = 'BICUBIC_INTERPOLATION'       # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "RESAMPLE_METHOD = 'BISINC_5_POINT_INTERPOLATION'    # str, options inlcude 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "\n",
    "geocodes_path = os.path.join(if_dir, '03_geocode')\n",
    "os.makedirs(geocodes_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "geocode_outpaths = []\n",
    "for i, pair in enumerate(pairs):\n",
    "    geocode_outpaths.append(os.path.join(geocodes_path, f'{pairs[i][0][-12:-4]}_{pairs[i][1][-12:-4]}.dim'))\n",
    "    ref = pairs[i][0]\n",
    "    sec = pairs[i][1]\n",
    "    # pair_str=f'{pairs[i][0]},{pairs[i][1]}'\n",
    "\n",
    "    geocode_cmd = f'{GPT_PATH} Back-Geocoding -SsourceProducts=\"{ref}\" \"{sec}\" -PdemName=\"{DEM_NAME}\" -PdemResamplingMethod=\"{DEM_RESAMPLE_METHOD}\" -PresamplingType={RESAMPLE_METHOD} -t {geocode_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(geocode_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# shutil.rmtree(orbits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. ESD (only if using more than one burst. which is pretty typical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERENCE_THRESHOLD = 0.3           # (0, 1]\n",
    "ESD_ESTIMATOR = 'Periodogram'       # Average or Periodogram\n",
    "\n",
    "#documentation not very good, assuming this is just resampling done before estimating ESD?\n",
    "# will look at STEP forum for more info\n",
    "WIN_ACC_AZI = 16                    # 2, 4, 8, 16, 32, 64\n",
    "WIN_ACC_RANGE = 16                  # 2, 4, 8, 16, 32, 64\n",
    "WIN_HEIGHT = 512                    # 32, 64, 128, 256, 512, 1024, 2048\n",
    "WIN_OVERSAMPLING = 128              # 32, 64, 128, 256\n",
    "WIN_WIDTH = 512                     # 32, 64, 128, 256, 512, 1024, 2048\n",
    "\n",
    "INTEGRATION_METHOD = 'L1 and L2'    # L1, L2, L1 and L2\n",
    "NUM_BLOCKS_PER_OVERLAP = 10         # [1,20]\n",
    "OVERALL_AZI_SHIFT = 0.0             \n",
    "OVERALL_RANGE_SHIFT = 0.0           \n",
    "WEIGHT_FUNCTION = 'Inv Quadratic'   # None, Linear, Quadratic, Inv Quadratic\n",
    "XCOR_THRESHOLD = 0.1                #(0, *)\n",
    "\n",
    "esds_path = os.path.join(if_dir, '04_ESD')\n",
    "os.makedirs(esds_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "esd_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    esd_outpaths.append(os.path.join(esds_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    esd_cmd = f'{GPT_PATH} Enhanced-Spectral-Diversity -Ssource={geocode_outpaths[i]} -PcohThreshold={COHERENCE_THRESHOLD} -PesdEstimator={ESD_ESTIMATOR} -PfineWinAccAzimuth={WIN_ACC_AZI} -PfineWinAccRange={WIN_ACC_RANGE} -PfineWinHeightStr={WIN_HEIGHT} -PfineWinOversampling={WIN_OVERSAMPLING} -PfineWinWidthStr={WIN_WIDTH} -PintegrationMethod=\"{INTEGRATION_METHOD}\" -PnumBlocksPerOverlap={NUM_BLOCKS_PER_OVERLAP} -PoverallAzimuthShift={OVERALL_AZI_SHIFT} PoverallRangeShift={OVERALL_RANGE_SHIFT} -PweightFunc=\"{WEIGHT_FUNCTION}\" -PxCorrThreshold={XCOR_THRESHOLD} -t {esd_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(esd_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(geocodes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Generate Ifg\n",
    "- may leave out the coherence and elevation bands from here. Could be added in individual steps later? Going to test first though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERENCE_AZI_WIN = 10              \n",
    "COHERENCE_RANGE_WIN = 10\n",
    "OUTPUT_ELEVATION = True                 # boolean\n",
    "OUTPUT_FLAT_EARTH_PHASE = False         # boolean\n",
    "OUTPUT_LAT_LON = False                  # boolean\n",
    "OUTPUT_TOPO_PHASE = False               # boolean\n",
    "SQUARE_PIXEL = True                     # boolean\n",
    "\n",
    "# these are for flat earth phase estimation\n",
    "FEP_NUM_POINTS = 501                    # 301, 401, 501, 601, 701, 801, 901, 1001\n",
    "FEP_POLY_DEGREE = 5                     # 1, 2, 3, 4, 5, 6, 7, 8\n",
    "\n",
    "SUBTRACT_FEP = True                     # boolean\n",
    "SUBTRACT_TOPO_PHASE = False             # boolean   \n",
    "TILE_EXTENSION_PERCENT = 100\n",
    "\n",
    "ifgs_path = os.path.join(if_dir, '05_interferograms')\n",
    "os.makedirs(ifgs_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "ifg_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    ifg_outpaths.append(os.path.join(ifgs_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    ifg_cmd = f'{GPT_PATH} Interferogram -SsourceProduct={esd_outpaths[i]} -PcohWinAz={COHERENCE_AZI_WIN} -PcohWinRg={COHERENCE_RANGE_WIN} -PdemName={DEM_NAME} -PorbitDegree={ORBIT_DEGREE} -PoutputElevation={OUTPUT_ELEVATION} -PoutputFlatEarthPhase={OUTPUT_FLAT_EARTH_PHASE} -PoutputLatLon={OUTPUT_LAT_LON} -PoutputTopoPhase={OUTPUT_TOPO_PHASE} -PsquarePixel={SQUARE_PIXEL} -PsrpNumberPoints={FEP_NUM_POINTS} -PsrpPolynomialDegree={FEP_POLY_DEGREE} -PsubtractFlatEarthPhase={SUBTRACT_FEP} -PsubtractTopographicPhase={SUBTRACT_TOPO_PHASE} -PtileExtensionsPercent={TILE_EXTENSION_PERCENT} -t {ifg_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(ifg_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(esds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TOPSAR-Deburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debursts_path = os.path.join(if_dir, '06_deburst')\n",
    "os.makedirs(debursts_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "deburst_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    deburst_outpaths.append(os.path.join(debursts_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    deburst_cmd = f'{GPT_PATH} TOPSAR-Deburst -Ssource={ifg_outpaths[i]} -PselectedPolarisations={POLARISATIONS} -t {deburst_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(deburst_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(ifgs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multilook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_PIXEL = False    # boolean\n",
    "AZI_LOOKS = '1'         # sentinel-1 is 22m in azimuth\n",
    "RANGE_LOOKS = '5'       # sentinel-1 is between 2.7 and 3.5m in range, depending on topography\n",
    "SOURCE_BANDS = ''       # reference band used to multilook\n",
    "\n",
    "mlooks_path = os.path.join(if_dir, '07_multilook')\n",
    "os.makedirs(mlooks_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "mlook_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    mlook_outpaths.append(os.path.join(mlooks_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    mlook_cmd = f'{GPT_PATH} Multilook -Ssource={deburst_outpaths[i]} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {mlook_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(mlook_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(debursts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Topo-phase removal\n",
    "- may not be needed since it can be done within ifg generation? Should probably test on a single ifg to compare differences and identify ideal workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_ELEVATION = True                 # boolean\n",
    "OUTPUT_LAT_LON = False                  # boolean\n",
    "OUTPUT_TOPO_PHASE = False               # boolean\n",
    "TILE_EXTENSION_PERCENT = 100\n",
    "\n",
    "topos_path = os.path.join(if_dir, '08_topophaseremoval')\n",
    "os.makedirs(topos_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "topo_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    topo_outpaths.append(os.path.join(topos_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    topo_cmd = f'{GPT_PATH} TopoPhaseRemoval -Ssource={mlook_outpaths[i]} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {topo_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(topo_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(mlooks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Goldstein Phase Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.0                     # double, (0, 1], default 1.0\n",
    "FFT_SIZE = 64                   # 32, 64, 128, 256\n",
    "COHERENCE_MASK = False          # boolean, masks out low coherence pixels during filtering\n",
    "WINDOW_SIZE = 3                 # 3, 5, 7\n",
    "\n",
    "filters_path = os.path.join(if_dir, '09_goldsteinfiltering')\n",
    "os.makedirs(filters_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "filter_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    filter_outpaths.append(os.path.join(filters_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    filter_cmd = f'{GPT_PATH} GoldsteinPhaseFiltering -Ssource={topo_outpaths[i]} -Palpha={ALPHA} -PcoherenceThreshold={COHERENCE_THRESHOLD} -PFFTSizeString={FFT_SIZE} -PuseCoherenceMask={COHERENCE_MASK} -PwindowSizeString={WINDOW_SIZE} -t {filter_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(filter_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(topos_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9a. Merge (if multiple subswaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges_path = os.path.join(if_dir, '10_merged')\n",
    "os.makedirs(merges_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "merge_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    merge_outpaths.append(os.path.join(merges_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    merge_cmd = f'{GPT_PATH} GoldsteinPhaseFiltering -SmasterProduct={filter_outpaths[i]} -t {merge_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(merge_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(filters_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9b. Subset and extract wrapped Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_METADATA = True            # boolean\n",
    "FULL_SWATH = False              # boolean\n",
    "GEO_REGION =                    # wkt region\n",
    "REFERENCE_BAND = 'VV'           # band used as reference for coordinates. CHECK THE AVAILABLE BANDS IN SNAP BEFORE PROCESSING\n",
    "SOURCE_BANDS = 'VV'\n",
    "X_SUB_SAMPLING = 1              # int, step size for subsampling in horizontal direction\n",
    "Y_SUB_SAMPLING = 1              # int, step size for subsampling in vertical direction\n",
    "# TIE_POINT_GRIDS =               # HONESTLY NO CLUE. LOOK INTO THIS MORE IN GUI\n",
    "\n",
    "subsets_path = os.path.join(if_dir, '11_subset')\n",
    "os.makedirs(subsets_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "subset_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    subset_outpaths.append(os.path.join(subsets_path, f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    subset_cmd = f'{GPT_PATH} GoldsteinPhaseFiltering -Ssource={merge_outpaths[i]} -t {subset_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(merges_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Add and extract elevation band only (needed for attribute metadata)\n",
    "- if elevation included in ifg generation, just extract and export?\n",
    "- may need to create custom .xml within gui first to be able to extract just elevation band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Add and extract coherence band\n",
    "- if coherence included in ifg generation, just extract and export?\n",
    "- may need to create custom .xml within gui first to be able to extract just coherence band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. SNAPHU Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_cmd('SnaphuExport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. SNAPHU unwrap\n",
    "- use subprocess\n",
    "- CLI example: snaphu -f snaphu.conf Phase_ifg_VV_28Mar2010_02May2010.snaphu.img 5191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. SNAPHU Import => Unwrapped Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Terrain Correction for 8b, 9, 10, and 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGN_STANDARD_GRID = False\n",
    "RADIOMETRIC_NORMALIZATION = True\n",
    "AUX_FILE = 'Product Auxiliary File'   # str, options include 'Latest Auxiliary File', 'Product Auxiliary File', 'External Auxiliary File'\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'DELAUNAY_INTERPOLATION'   # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION', 'DELAUNAY_INTERPOLATION'\n",
    "IMG_RESAMPLE_METHOD = 'CUBIC_CONVOLUTION' # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "NODATA_SEA = False\n",
    "OUTPUT_COMPLEX = False\n",
    "PIXEL_SPACING_METERS = 20.0   # double\n",
    "SAVE_DEM = False\n",
    "SAVE_ANGLE_ELLIPSOID = False\n",
    "SAVE_LAT_LON = False\n",
    "SAVE_LAYOVER_SHADOW = True\n",
    "SAVE_LOCAL_ANGLE = False\n",
    "SAVE_PROJ_LOCAL_ANGLE = True\n",
    "SAVE_SOURCE_BAND = True\n",
    "SOURCE_BANDS = 'Gamma0_VV,Gamma0_VH'\n",
    "GRID_ORIGIN_X = 0   # double\n",
    "GRID_ORIGIN_Y = 0   # double\n",
    "\n",
    "# only needed if you you are using external dem\n",
    "# uncomment these and adjust your gpt command below accordingly\n",
    "#  EXTERNAL_AUX = \n",
    "# EXTERNAL_DEMFILE =\n",
    "# EXTERNAL_DEMFILE_NODATA =\n",
    "# ETERNAL_DEM_EGM = False\n",
    "\n",
    "tc_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tc_outpaths.append(os.path.join(grdpaths[8], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    tc_cmd = f'{GPT_PATH} Terrain-Correction -Ssource={tf_outpaths[i]} -PalignToStandardGrid={ALIGN_STANDARD_GRID} -PapplyRadiometricNormalization={RADIOMETRIC_NORMALIZATION} -PauxFile=\"{AUX_FILE}\" -PdemName=\"{DEM_NAME}\" -PdemResamplingMethod=\"{DEM_RESAMPLE_METHOD}\" -PimgResamplingMethod=\"{IMG_RESAMPLE_METHOD}\" -PnodataValueAtSea={NODATA_SEA} -PoutputComplex={OUTPUT_COMPLEX} -PpixelSpacingInMeter={PIXEL_SPACING_METERS} -PsaveDEM={SAVE_DEM} -PsaveIncidenceAngleFromEllipsoid={SAVE_ANGLE_ELLIPSOID} -PsaveLatLon={SAVE_LAT_LON} -PsaveLayoverShadowMask={SAVE_LAYOVER_SHADOW} -PsaveLocalIncidenceAngle={SAVE_LOCAL_ANGLE} -PsaveProjectedLocalIncidenceAngle={SAVE_PROJ_LOCAL_ANGLE} -PsaveSelectedSourceBand={SAVE_SOURCE_BAND} -PsourceBands={SOURCE_BANDS} -PstandardGridOriginX={GRID_ORIGIN_X} -PstandardGridOriginY={GRID_ORIGIN_Y} -t {tc_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(tc_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "#     os.remove(tf_outpaths[i]) # removes the just used source product\n",
    "#     shutil.rmtree(tf_outpaths[i][:-4]+'.data')\n",
    "\n",
    "# shutil.rmtree(grdpaths[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Final subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_METADATA = True\n",
    "FULL_SWATH = False\n",
    "REFERNCE_BAND = 'Beta0_VV'\n",
    "\n",
    "subset_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    subset_outpaths.append(os.path.join(grdpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    subset_cmd = f'{GPT_PATH} Subset -Ssource={mlook_outpaths[i]} -PcopyMetadata={COPY_METADATA} -PfullSwath={FULL_SWATH} -PgeoRegion=\"{aoi}\" -PreferenceBand={REFERNCE_BAND} -t {subset_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(mlook_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(mlook_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
