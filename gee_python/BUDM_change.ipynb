{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal of this notebook is to use the google earth engine to identify and create features that represent the shoreline/waterline of the wetland projects based on different parameters like NDWI, VV and VH backscatter, and individual mutispectral bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## before anything you need to visit the site below and make sure you have a google earth engine account\n",
    "## this is so you can access Sentinel-1 GRD and Sentinel-2 TOA and SR products, as well as other sensor packages and data types\n",
    "\n",
    "## visit the below website below to setup an earth engine account, enable a cloud project, and enable the ee API \n",
    "## https://developers.google.com/earth-engine/cloud/earthengine_cloud_project_setup#get-access-to-earth-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import geemap.colormaps as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only need to run this once\n",
    "## after authenticating with google earth engine you will only need to initialize each session\n",
    "\n",
    "## https://developers.google.com/earth-engine/guides/auth\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init ee cloud project you made during initial setup\n",
    "ee.Initialize(project = '') ##enter your project name here as a string to initialize exchanges with ee api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some functions for a bit easier mapping\n",
    "super simple for now, might make them better later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to add RGB images to the map.\n",
    "def add_rgb_to_map(image, map_object):\n",
    "\n",
    "    date = ee.Date(image.get('date')).format('YYYY-MM-dd').getInfo()\n",
    "    map_object.addLayer(image, {'min': 0, 'max': 2000, 'bands': ['B4', 'B3', 'B2']}, f'{date}_rgb')\n",
    "\n",
    "## Function to add spectral indices images to the map.\n",
    "def add_ind_to_map(image, map_object, band):\n",
    "\n",
    "    date = ee.Date(image.get('date')).format('YYYY-MM-dd').getInfo()\n",
    "    if band =='NDWI':\n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.ndwi}, f'{date}_{band}')\n",
    "    elif band =='NDVI': \n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.ndvi}, f'{date}_{band}')\n",
    "    elif band == 'MSAVI2':\n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.RdYlGn}, f'{date}_{band}')\n",
    "    elif band == 'BSI':\n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.Greens}, f'{date}_{band}')\n",
    "\n",
    "## Function to add spectral indices images to the map.\n",
    "def add_sar_to_map(image, map_object, target_band):\n",
    "\n",
    "    date = ee.Date(image.get('date')).format('YYYY-MM-dd').getInfo()\n",
    "    map_object.addLayer(image, {'min': -50, 'max': 1, 'bands': target_band}, f'{date}_{target_band}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functiont to create three important and popular spectral indices\n",
    "## ndvi = Normalized Difference Vegetation Index, good for vegetation health and cover\n",
    "## ndwi = Normalized Difference Water Index, good for identifying water bodies and mositure in surface\n",
    "def s2_10m_target_indices(image):\n",
    "    # Calculate NDVI\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    # Calculate NDWI\n",
    "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "    # Calculate MSAVI2\n",
    "    msavi2 = image.expression(\n",
    "        '((2 * NIR + 1) - ((2 * NIR + 1) ** 2 - 8 * (NIR - RED)) ** 0.5) / 2',\n",
    "        {\n",
    "            'NIR': image.select('B8'),\n",
    "            'RED': image.select('B4')\n",
    "        }\n",
    "    ).rename('MSAVI2')\n",
    "    \n",
    "    # Resample the SWIR1 band to match the resolution of the other bands (10 meters)\n",
    "    # swir1_resampled = image.select('B11').reproject(crs = msavi2.projection(), scale = 10)\n",
    "    \n",
    "    # Calculate the Bare-Soil Index (BSI)\n",
    "    bsi = image.expression(\n",
    "        '((SWIR1 + RED) - (NIR + BLUE)) / ((SWIR1 + RED) + (NIR + BLUE))',\n",
    "        {\n",
    "            'RED': image.select('B4'),\n",
    "            'NIR': image.select('B8'),\n",
    "            'BLUE': image.select('B2'),\n",
    "            'SWIR1': image.select('B11').resample(mode = 'bicubic') ### scale of 10m specified when exporting images\n",
    "        }\n",
    "    ).rename('BSI')\n",
    "    \n",
    "    # Add all indices as new bands to the image\n",
    "    return image.addBands([ndvi, ndwi, msavi2, bsi])\n",
    "\n",
    "## collects sentinel-1 GRD (radar, no phase) and Sentinel-2 SR (multispectral, adjusted for top of atmosphere reflectance)\n",
    "def get_sentinel_imagery(aoi, start_date, end_date, s2_cloud_cov, orbit):\n",
    "    ## Sentinel-1 ImageCollection\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "               .filterBounds(aoi)\n",
    "               .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "               .map(lambda img: img.set('date', ee.Date(img.date()).format('YYYYMMdd')))\n",
    "               .filter(ee.Filter.eq('orbitProperties_pass', orbit))\n",
    "               .select(['VV', 'VH'])\n",
    "               .sort('date')\n",
    "    )\n",
    "\n",
    "    s1= s1.map(lambda img: img.clip(aoi))\n",
    "    ## Sentinel-2 Surface Reflectance Harmonized ImageCollection\n",
    "    s2_10m = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "               .filterBounds(aoi)\n",
    "               .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "               .map(lambda img: img.set('date', ee.Date(img.date()).format('YYYYMMdd')))\n",
    "               .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', s2_cloud_cov))\n",
    "               .sort('date')\n",
    "               .select(['B2', 'B3', 'B4', 'B8', 'B11'])\n",
    "    )\n",
    "    ## Clip all images in the collection to the AOI\n",
    "    s2_10m = s2_10m.map(lambda img: img.clip(aoi))\n",
    "    ## Apply indices to the Sentinel-2 images\n",
    "    s2_10m_ndvi = s2_10m.map(s2_10m_target_indices).select(['NDVI'])\n",
    "    s2_10m_ndwi = s2_10m.map(s2_10m_target_indices).select(['NDWI'])\n",
    "    s2_10m_msavi2 = s2_10m.map(s2_10m_target_indices).select(['MSAVI2'])\n",
    "    s2_10m_bsi = s2_10m.map(s2_10m_target_indices).select(['BSI'])\n",
    "\n",
    "    \n",
    "    # return s1_VV, s1_VH, s2_10m, s2_10m_ndvi, s2_10m_ndwi, s2_10m_msavi2, s2_10m_bsi\n",
    "    return s1, s2_10m, s2_10m_ndvi, s2_10m_ndwi, s2_10m_msavi2, s2_10m_bsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fucntion to get the date of each image in the image collection\n",
    "def get_date(image):\n",
    "    return ee.Feature(None, {'date': image.date().format('YYYY-MM-dd')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get histogram of NDWI for each image\n",
    "def get_histogram(image, scale, bucket_num, band_name):\n",
    "    \"\"\"\n",
    "    Used to create ndwi histograms for the imagery\n",
    "\n",
    "    image = ee.Image\n",
    "        NDWI image to determine the shoreline from\n",
    "    scale = int\n",
    "        scale to estimate the histogram from, typically 10 to match the resolution of the RGB imagery\n",
    "    bucket_num = int\n",
    "        number of buckets to put the data into for histogram\n",
    "    band_name = str\n",
    "        the name of your target band in the image\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Reduce the image to get a histogram over the region of interest (ROI)\n",
    "    hist = image.reduceRegion(\n",
    "        reducer=ee.Reducer.histogram(maxBuckets=bucket_num),  # Adjust the number of buckets as needed\n",
    "        geometry=aoi,\n",
    "        scale=scale,  # Adjust based on image resolution\n",
    "        maxPixels=1e8\n",
    "    )\n",
    "    \n",
    "    # Get the histogram data for NDWI\n",
    "    histogram = ee.Dictionary(hist.get(band_name)).getInfo() \n",
    "    \n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otsu_trimodal_from_histogram(histogram):\n",
    "    # Extract histogram and means from the provided NDWI histogram\n",
    "    counts = np.array(histogram['histogram'])\n",
    "    means = np.array(histogram['bucketMeans'])\n",
    "    total = np.sum(counts)\n",
    "    sum_values = np.sum(means * counts)\n",
    "    overall_mean = sum_values / total\n",
    "    size = len(means)\n",
    "\n",
    "    # Define a function to compute the between-class variance (BSS)\n",
    "    def compute_intervariance(i, k):\n",
    "        # Compute for first region (A)\n",
    "        aCounts = counts[:i]\n",
    "        aCount = np.sum(aCounts)\n",
    "        aMeans = means[:i]\n",
    "        aMean = np.sum(aMeans * aCounts) / aCount if aCount != 0 else 0\n",
    "\n",
    "        # Compute for second region (B)\n",
    "        bCounts = counts[i:k]\n",
    "        bCount = np.sum(bCounts)\n",
    "        bMeans = means[i:k]\n",
    "        bMean = np.sum(bMeans * bCounts) / bCount if bCount != 0 else 0\n",
    "\n",
    "        # Compute for third region (C)\n",
    "        cCounts = counts[k:]\n",
    "        cCount = np.sum(cCounts)\n",
    "        cMeans = means[k:]\n",
    "        cMean = np.sum(cMeans * cCounts) / cCount if cCount != 0 else 0\n",
    "\n",
    "        # Return combined BSS\n",
    "        return aCount * (aMean - overall_mean) ** 2 + \\\n",
    "               bCount * (bMean - overall_mean) ** 2 + \\\n",
    "               cCount * (cMean - overall_mean) ** 2\n",
    "\n",
    "    # Initialize an empty list for BSS values\n",
    "    bss_values = []\n",
    "    \n",
    "    # Iterate through potential thresholds\n",
    "    for i in range(1, size - 1):  # Start at 1, and stop before the last element\n",
    "        for k in range(i + 1, size):  # Ensure k is always greater than i\n",
    "            bss_values.append(compute_intervariance(i, k))\n",
    "\n",
    "    # Convert BSS values to a NumPy array\n",
    "    bss_array = np.array(bss_values)\n",
    "\n",
    "    # Find indices of maximum BSS\n",
    "    max_bss_index = np.argmax(bss_array)\n",
    "    \n",
    "    # Convert the flat index back to i and k values\n",
    "    i_max = max_bss_index // (size - 1)\n",
    "    k_max = max_bss_index % (size - 1)\n",
    "\n",
    "    # Return corresponding means for the threshold\n",
    "    return means[i_max], means[k_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate statistics for NDVI and NDMI over AOIs\n",
    "def extract_statistics(image, index_name):\n",
    "    # Reduce the image over the AOIs using mean, max, and min reducers\n",
    "    stats = image.reduceRegions(\n",
    "        collection=aois,\n",
    "        reducer=ee.Reducer.minMax().combine(\n",
    "            reducer2=ee.Reducer.percentile([25, 50, 75]).combine(\n",
    "                reducer2=ee.Reducer.mean().combine(\n",
    "                    reducer2=ee.Reducer.stdDev(),\n",
    "                    sharedInputs=True\n",
    "                ),\n",
    "                sharedInputs=True\n",
    "            ),\n",
    "            sharedInputs=True\n",
    "        ),\n",
    "        scale=10\n",
    "    )\n",
    "    \n",
    "    # Add the date of the image as a property to each feature in the collection\n",
    "    date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n",
    "    stats = stats.map(lambda f: f.set('date', date))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# my function needs im_coll, index_name, and aoi\n",
    "\n",
    "def extract_stats_from_aoi(image_collection, index_name, aoi):\n",
    "    stats_ims = image_collection.map(lambda img: img.clip(aoi))\n",
    "    stats_collection = stats_ims.map(lambda img: extract_statistics(img, index_name)).flatten()\n",
    "    statslist = stats_collection.getInfo()['features']\n",
    "\n",
    "    data=[]\n",
    "    for feature in statslist:\n",
    "        properties = feature['properties']\n",
    "        data_dict = {}\n",
    "        for key in properties:\n",
    "            data_dict[f'{key}'] = properties[key]\n",
    "        data.append(data_dict)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert the date column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Sort the DataFrame by date\n",
    "    df = df.sort_values(by='date')\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_VI_ts(aoi, aoi_str, plottype):\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 11\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "    # Plotting the mean, standard deviation, and quartiles\n",
    "    if plottype == 'together':\n",
    "    \n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        for imcoll in [total_colls['s2_10m_ndvi'], total_colls['s2_10m_ndwi'], total_colls['s2_10m_msavi2']]:\n",
    "            if imcoll == total_colls['s2_10m_ndvi']:\n",
    "                df = extract_stats_from_aoi(imcoll, 'NDVI', aoi)\n",
    "                plt.plot(df['date'], df['mean'], color='green', marker='o', linestyle='-', label='Mean NDVI')\n",
    "                plt.fill_between(df['date'],\n",
    "                                df['mean'] - df['stdDev'],\n",
    "                                df['mean'] + df['stdDev'],\n",
    "                                color='green', alpha=0.2, label='NDVI ± 1 Std Dev'\n",
    "                            )\n",
    "\n",
    "                # Plot NDVI Interquartile Range (IQR)\n",
    "                plt.fill_between(df['date'],\n",
    "                                df['p25'],\n",
    "                                df['p75'],\n",
    "                                color='green', alpha=0.5, linestyle='-', label='NDVI IQR'\n",
    "                            )\n",
    "\n",
    "            elif imcoll == total_colls['s2_10m_ndwi']:\n",
    "            # Plot NDVI Mean with ±1 Std Dev Shading\n",
    "                df = extract_stats_from_aoi(imcoll, 'NDWI', aoi)\n",
    "                plt.plot(df['date'], df['mean'], color='blue', marker='o', linestyle='-', label='Mean NDWI')\n",
    "                plt.fill_between(df['date'],\n",
    "                                    df['mean'] - df['stdDev'],\n",
    "                                    df['mean'] + df['stdDev'],\n",
    "                                    color='blue', alpha=0.2, label='NDVI ± 1 Std Dev'\n",
    "                                )\n",
    "\n",
    "                # Plot NDVI Interquartile Range (IQR)\n",
    "                plt.fill_between(df['date'],\n",
    "                                    df['p25'],\n",
    "                                    df['p75'],\n",
    "                                    color='blue', alpha=0.5, linestyle='-', label='NDWI IQR'\n",
    "                                )\n",
    "                \n",
    "            elif imcoll == total_colls['s2_10m_msavi2']:\n",
    "                df = extract_stats_from_aoi(imcoll, 'MSAVI2', aoi)\n",
    "                plt.plot(df['date'], df['mean'], color='red', marker='o', linestyle='-', label='Mean MSAVI2')\n",
    "                plt.fill_between(df['date'],\n",
    "                                df['mean'] - df['stdDev'],\n",
    "                                df['mean'] + df['stdDev'],\n",
    "                                color='red', alpha=0.2, label='MSAVI2 ± 1 Std Dev'\n",
    "                            )\n",
    "\n",
    "                # Plot MSAVI2 Interquartile Range (IQR)\n",
    "                plt.fill_between(df['date'],\n",
    "                                df['p25'],\n",
    "                                df['p75'],\n",
    "                                color='red', alpha=0.5, linestyle='-', label='MSAVI2 IQR'\n",
    "                            )\n",
    "\n",
    "        # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "        storm_dates = {\n",
    "            'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "            'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "            'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "            'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "            'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "            'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            if storm[0] == 'H':\n",
    "                plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "                \n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date', fontweight = 'bold', fontsize = 11)\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'NDVI, NDWI, and MSAVI2 Change Over Time for {aoi_str}')\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "    \n",
    "    elif plottype == 'separate':\n",
    "        # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "        storm_dates = {\n",
    "            'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "            'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "            'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "            'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "            'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "            'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "        }\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(14,7))\n",
    "\n",
    "        df = extract_stats_from_aoi(total_colls['s2_10m_ndvi'], 'NDVI', aoi)\n",
    "        plt.plot(df['date'], df['mean'], color='green', marker='o', linestyle='-', label='Mean NDVI')\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['mean'] - df['stdDev'],\n",
    "                        df['mean'] + df['stdDev'],\n",
    "                        color='green', alpha=0.2, label='NDVI ± 1 Std Dev'\n",
    "                    )\n",
    "\n",
    "        # Plot NDVI Interquartile Range (IQR)\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['p25'],\n",
    "                        df['p75'],\n",
    "                        color='green', alpha=0.5, linestyle='-', label='NDVI IQR'\n",
    "                    )\n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date', fontweight = 'bold', fontsize = 11)\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'NDVI Change Over Time for {aoi_str}')\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(14,7))\n",
    "        df = extract_stats_from_aoi(total_colls['s2_10m_ndwi'], 'NDWI', aoi)\n",
    "        plt.plot(df['date'], df['mean'], color='blue', marker='o', linestyle='-', label='Mean NDWI')\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['mean'] - df['stdDev'],\n",
    "                        df['mean'] + df['stdDev'],\n",
    "                        color='blue', alpha=0.2, label='NDWI ± 1 Std Dev'\n",
    "                    )\n",
    "\n",
    "        # Plot NDWI Interquartile Range (IQR)\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['p25'],\n",
    "                        df['p75'],\n",
    "                        color='blue', alpha=0.5, linestyle='-', label='NDWI IQR'\n",
    "                    )\n",
    "        \n",
    "\n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date', fontweight = 'bold', fontsize = 11)\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'NDWI Change Over Time for {aoi_str}')\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(14,7))\n",
    "\n",
    "        df = extract_stats_from_aoi(total_colls['s2_10m_msavi2'], 'MSAVI2', aoi)\n",
    "        plt.plot(df['date'], df['mean'], color='red', marker='o', linestyle='-', label='Mean MSAVI2')\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['mean'] - df['stdDev'],\n",
    "                        df['mean'] + df['stdDev'],\n",
    "                        color='red', alpha=0.2, label='MSAVI2 ± 1 Std Dev'\n",
    "                    )\n",
    "\n",
    "        # Plot NDVI Interquartile Range (IQR)\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['p25'],\n",
    "                        df['p75'],\n",
    "                        color='red', alpha=0.5, linestyle='-', label='MSAVI2 IQR'\n",
    "                    )\n",
    "        \n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date', fontweight = 'bold', fontsize = 11)\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'MSAVI2 Change Over Time for {aoi_str}')\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_GRD_ts(aoi, aoi_str, plottype, imcoll):\n",
    "    plt.rcParams['font.family'] = 'Georgia'\n",
    "    plt.rcParams['font.size'] = 11\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    \n",
    "    # Plotting the mean, standard deviation, and quartiles\n",
    "    if plottype == 'together':\n",
    "        plt.figure(figsize=(15, 6), dpi=1000)\n",
    "\n",
    "        for band in ['VV', 'VH']:\n",
    "            df = extract_stats_from_aoi(imcoll, band, aoi)\n",
    "            if band == 'VV':\n",
    "                color = 'blue'\n",
    "            elif band == 'VH':\n",
    "                color = 'green'\n",
    "\n",
    "\n",
    "            plt.plot(df['date'], df[f'{band}_mean'], color=color, marker='o', linestyle='-', label=f'Mean {band} Backscatter')\n",
    "            plt.fill_between(df['date'],\n",
    "                            df[f'{band}_mean'] - df[f'{band}_stdDev'],\n",
    "                            df[f'{band}_mean'] + df[f'{band}_stdDev'],\n",
    "                            color=color, alpha=0.2, label=f'{band} Backscatter ± 1 Std Dev'\n",
    "                        )\n",
    "            # Plot VV Interquartile Range (IQR)\n",
    "            plt.fill_between(df['date'],\n",
    "                            df[f'{band}_p25'],\n",
    "                            df[f'{band}_p75'],\n",
    "                            color=color, alpha=0.5, linestyle='-', label=f'{band} Backscatter IQR' #(25th to 75th Percentile)\n",
    "                        )\n",
    "\n",
    "\n",
    "        # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "        storm_dates = {\n",
    "            'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "            'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "            'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "            'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "            'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "            'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "        }\n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "        \n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date', fontweight='bold', fontsize=11)\n",
    "        plt.ylabel('Backscatter Amplitude (dB)', fontweight='bold', fontsize=11)\n",
    "        plt.title(f'Sentinel-1 Backscatter Amplitude Change Over Time for {aoi_str}', fontweight='bold', fontsize=14)\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "    elif plottype == 'separate':\n",
    "        # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "        storm_dates = {\n",
    "            'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "            'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "            'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "            'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "            'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "            'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "        }\n",
    "\n",
    "        df = extract_stats_from_aoi(imcoll, 'VV', aoi)\n",
    "        plt.plot(df['date'], df['mean'], color='green', marker='o', linestyle='-', label='Mean VV Backscatter')\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['VV_mean'] - df['VV_stdDev'],\n",
    "                        df['VV_mean'] + df['VV_stdDev'],\n",
    "                        color='green', alpha=0.2, label='VV Backscatter ± 1 Std Dev'\n",
    "                    )\n",
    "\n",
    "        # Plot VV Interquartile Range (IQR)\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['VV_p25'],\n",
    "                        df['VV_p75'],\n",
    "                        color='green', alpha=0.5, linestyle='-', label='VV Backscatter IQR' #(25th to 75th Percentile)\n",
    "                    )\n",
    "\n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'Sentinel-1 VV Backscatter Amplitude Change Over Time for {aoi_str}')\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        df = extract_stats_from_aoi(imcoll, 'VH', aoi)\n",
    "        plt.plot(df['date'], df['VH_mean'], color='blue', marker='o', linestyle='-', label='Mean VH Backscatter')\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['VH_mean'] - df['VH_stdDev'],\n",
    "                        df['VH_mean'] + df['VH_stdDev'],\n",
    "                        color='blue', alpha=0.2, label='VH Backscatter ± 1 Std Dev'\n",
    "                    )\n",
    "\n",
    "        # Plot VV Interquartile Range (IQR)\n",
    "        plt.fill_between(df['date'],\n",
    "                        df['VH_p25'],\n",
    "                        df['VH_p75'],\n",
    "                        color='blue', alpha=0.5, linestyle='-', label='VH Backscatter IQR' #(25th to 75th Percentile)\n",
    "                    )\n",
    "        \n",
    "\n",
    "\n",
    "        for storm, date in storm_dates.items():\n",
    "            plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "        # Customizing the plot\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'Sentinel-1 VH Backscatter Amplitude Change Over Time for {aoi_str}')\n",
    "        plt.xticks(rotation=45, fontweight='bold')\n",
    "        plt.yticks(fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_land(rgb_coll, ndwi_coll, ndvi_coll):\n",
    "    land_area = []\n",
    "\n",
    "    rgb_ims = rgb_coll.toList(rgb_coll.size())\n",
    "    ndwi_ims = ndwi_coll.toList(ndwi_coll.size())\n",
    "    ndvi_ims = ndvi_coll.toList(ndvi_coll.size())\n",
    "\n",
    "    # Iterate over each image in the collection\n",
    "    for i in range(ndwi_coll.size().getInfo()):\n",
    "        # Get NDWI and NDVI masks\n",
    "        ndwi_image = ee.Image(ndwi_ims.get(i))\n",
    "        ndvi_image = ee.Image(ndvi_ims.get(i))\n",
    "\n",
    "        # Create water mask from NDWI and land mask from NDVI\n",
    "        watermask = ndwi_image.select('NDWI').lt(0.0)  # Water is NDWI < 0\n",
    "        landmask = ndvi_image.select('NDVI').gt(0.0)  # Land is NDVI > 0\n",
    "        \n",
    "        # Apply the combined land mask to the RGB image (where watermask AND landmask are valid)\n",
    "        combined_mask = landmask.And(watermask)\n",
    "        masked_rgb = ee.Image(rgb_ims.get(i)).updateMask(combined_mask)\n",
    "        \n",
    "        land_area.append(masked_rgb)\n",
    "\n",
    "    # Return land area as an ImageCollection\n",
    "    land_area_coll = ee.ImageCollection(land_area)\n",
    "    return land_area_coll\n",
    "\n",
    "def calculate_land_area(image):\n",
    "    pixel_area = ee.Image.pixelArea()  # Pixel area in square meters\n",
    "    \n",
    "    # Get the land area in square meters\n",
    "    land_area = image.select('B8').multiply(pixel_area)  # Use the NIR band for land area\n",
    "    \n",
    "    # Sum up the total land area for the AOI\n",
    "    total_land_area_m2 = land_area.reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        # geometry=aois,\n",
    "        scale=10,  # Sentinel-2 resolution is 10 meters\n",
    "        maxPixels=1e9\n",
    "    ).get('B8')  # Sum of the NIR band for total land area\n",
    "\n",
    "    # Convert from square meters to square kilometers\n",
    "    total_land_area_km2 = ee.Number(total_land_area_m2).divide(1e9)  # 1 km² = 1,000,000 m²\n",
    "    \n",
    "    # Ensure the total area is positive (in case of any negative values)\n",
    "    total_land_area_km2 = total_land_area_km2.abs()\n",
    "\n",
    "    # Set the total land area as a property on the image\n",
    "    return image.set('total_land_area', total_land_area_km2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_dates(s2_dates, s1_dates):\n",
    "    matched_pairs = []\n",
    "    \n",
    "    for s2_date in s2_dates:\n",
    "        # Find the Sentinel-1 date with the smallest difference\n",
    "        closest_s1_date = min(s1_dates, key=lambda s1_date: abs(s1_date - s2_date))\n",
    "        matched_pairs.append((s2_date, closest_s1_date))\n",
    "    \n",
    "    return matched_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the existing ImageCollections by a specific date\n",
    "def filter_image_by_date(collection, date_str):\n",
    "    date = ee.Date(date_str)\n",
    "    # Filter the collection by the specified date and return the first image\n",
    "    return collection.filterDate(date, date.advance(1, 'day')).first()\n",
    "\n",
    "# Function to match Sentinel-1 and Sentinel-2 images using the date pairs\n",
    "def match_collections_by_dates(s2_collection, s1_collection, matched_dates):\n",
    "\n",
    "    matched_images = []\n",
    "    \n",
    "    for s2_date, s1_date in matched_dates:\n",
    "        # Filter the collections by the matched dates\n",
    "        s2_img = filter_image_by_date(s2_collection, s2_date)\n",
    "        s1_img = filter_image_by_date(s1_collection, s1_date)\n",
    "        \n",
    "        # Combine the images (stack bands) from both Sentinel-2 and Sentinel-1\n",
    "        combined_img = ee.Image.cat(s2_img, s1_img)\n",
    "        \n",
    "        # Add the combined image to the list\n",
    "        matched_images.append(combined_img)\n",
    "    \n",
    "    # Convert the list of combined images into an ImageCollection\n",
    "    return ee.ImageCollection(matched_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------//\n",
    "# 1.SPECKLE FILTERS\n",
    "# ---------------------------------------------------------------------------//\n",
    "\n",
    "def boxcar(image, KERNEL_SIZE):\n",
    "    \"\"\"\n",
    "    Apply boxcar filter on every image in the collection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ee.Image\n",
    "        Image to be filtered\n",
    "    KERNEL_SIZE : positive odd integer\n",
    "        Neighbourhood window size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.Image\n",
    "        Filtered Image\n",
    "\n",
    "    \"\"\"\n",
    "    bandNames = image.bandNames().remove('angle')\n",
    "      #Define a boxcar kernel\n",
    "    kernel = ee.Kernel.square((KERNEL_SIZE/2), units='pixels', normalize=True)\n",
    "     #Apply boxcar\n",
    "    output = image.select(bandNames).convolve(kernel).rename(bandNames)\n",
    "    return image.addBands(output, None, True)\n",
    "\n",
    "def leefilter(image, KERNEL_SIZE):\n",
    "    \"\"\"\n",
    "    Lee Filter applied to one image.\n",
    "    It is implemented as described in\n",
    "    J. S. Lee, “Digital image enhancement and noise filtering by use of local statistics,”\n",
    "    IEEE Pattern Anal. Machine Intell., vol. PAMI-2, pp. 165–168, Mar. 1980.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ee.Image\n",
    "        Image to be filtered\n",
    "    KERNEL_SIZE : positive odd integer\n",
    "        Neighbourhood window size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.Image\n",
    "        Filtered Image\n",
    "\n",
    "    \"\"\"\n",
    "    bandNames = image.bandNames().remove('angle')\n",
    "\n",
    "    # S1-GRD images are multilooked 5 times in range\n",
    "    enl = 5\n",
    "    # Compute the speckle standard deviation\n",
    "    eta = 1.0/math.sqrt(enl)\n",
    "    eta = ee.Image.constant(eta)\n",
    "\n",
    "    # MMSE estimator\n",
    "    # Neighbourhood mean and variance\n",
    "    oneImg = ee.Image.constant(1)\n",
    "    # Estimate stats\n",
    "    reducers = ee.Reducer.mean().combine(\n",
    "                      reducer2= ee.Reducer.variance()\n",
    "                      ,sharedInputs= True\n",
    "                      )\n",
    "    stats = (image.select(bandNames).reduceNeighborhood(\n",
    "                      reducer= reducers\n",
    "                          ,kernel= ee.Kernel.square(KERNEL_SIZE/2, 'pixels')\n",
    "                              ,optimization= 'window'))\n",
    "    meanBand = bandNames.map(lambda bandName: ee.String(bandName).cat('_mean'))\n",
    "    varBand = bandNames.map(lambda bandName:  ee.String(bandName).cat('_variance'))\n",
    "\n",
    "    z_bar = stats.select(meanBand)\n",
    "    varz = stats.select(varBand)\n",
    "    # Estimate weight \n",
    "    varx = (varz.subtract(z_bar.pow(2).multiply(eta.pow(2)))).divide(oneImg.add(eta.pow(2)))\n",
    "    b = varx.divide(varz)\n",
    "  \n",
    "    # if b is negative set it to zero\n",
    "    new_b = b.where(b.lt(0), 0)\n",
    "    output = oneImg.subtract(new_b).multiply(z_bar.abs()).add(new_b.multiply(image.select(bandNames)))\n",
    "    output = output.rename(bandNames)\n",
    "    return image.addBands(output, None, True)\n",
    "\n",
    "\n",
    "def gammamap(image,KERNEL_SIZE): \n",
    "    \n",
    "    \"\"\"\n",
    "    Gamma Maximum a-posterior Filter applied to one image. It is implemented as described in \n",
    "    Lopes A., Nezry, E., Touzi, R., and Laur, H., 1990.  \n",
    "    Maximum A Posteriori Speckle Filtering and First Order texture Models in SAR Images.  \n",
    "    International  Geoscience  and  Remote  Sensing  Symposium (IGARSS).\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ee.Image\n",
    "        Image to be filtered\n",
    "    KERNEL_SIZE : positive odd integer\n",
    "        Neighbourhood window size\n",
    "    Returns\n",
    "    -------\n",
    "    ee.Image\n",
    "        Filtered Image\n",
    "    \"\"\"\n",
    "    enl = 5\n",
    "    bandNames = image.bandNames().remove('angle')\n",
    "    #local mean\n",
    "    reducers = ee.Reducer.mean().combine( \\\n",
    "                      reducer2= ee.Reducer.stdDev(), \\\n",
    "                      sharedInputs= True\n",
    "                      )\n",
    "    stats = (image.select(bandNames).reduceNeighborhood( \\\n",
    "                      reducer= reducers, \\\n",
    "                          kernel= ee.Kernel.square(KERNEL_SIZE/2,'pixels'), \\\n",
    "                              optimization= 'window'))\n",
    "    meanBand = bandNames.map(lambda bandName: ee.String(bandName).cat('_mean'))\n",
    "    stdDevBand = bandNames.map(lambda bandName:  ee.String(bandName).cat('_stdDev'))\n",
    "        \n",
    "    z = stats.select(meanBand)\n",
    "    sigz = stats.select(stdDevBand)\n",
    "    \n",
    "    #local observed coefficient of variation\n",
    "    ci = sigz.divide(z)\n",
    "    #noise coefficient of variation (or noise sigma)\n",
    "    cu = 1.0/math.sqrt(enl)\n",
    "    #threshold for the observed coefficient of variation\n",
    "    cmax = math.sqrt(2.0) * cu\n",
    "    cu = ee.Image.constant(cu)\n",
    "    cmax = ee.Image.constant(cmax)\n",
    "    enlImg = ee.Image.constant(enl)\n",
    "    oneImg = ee.Image.constant(1)\n",
    "    twoImg = ee.Image.constant(2)\n",
    "\n",
    "    alpha = oneImg.add(cu.pow(2)).divide(ci.pow(2).subtract(cu.pow(2)))\n",
    "\n",
    "    #Implements the Gamma MAP filter described in equation 11 in Lopez et al. 1990\n",
    "    q = image.select(bandNames).expression('z**2 * (z * alpha - enl - 1)**2 + 4 * alpha * enl * b() * z', { 'z': z,  'alpha':alpha,'enl': enl})\n",
    "    rHat = z.multiply(alpha.subtract(enlImg).subtract(oneImg)).add(q.sqrt()).divide(twoImg.multiply(alpha))\n",
    "  \n",
    "    #if ci <= cu then its a homogenous region ->> boxcar filter\n",
    "    zHat = (z.updateMask(ci.lte(cu))).rename(bandNames)\n",
    "    #if cmax > ci > cu then its a textured medium ->> apply Gamma MAP filter\n",
    "    rHat = (rHat.updateMask(ci.gt(cu)).updateMask(ci.lt(cmax))).rename(bandNames)\n",
    "    #ci>cmax then its strong signal ->> retain\n",
    "    x = image.select(bandNames).updateMask(ci.gte(cmax)).rename(bandNames)  \n",
    "    #Merge\n",
    "    output = ee.ImageCollection([zHat,rHat,x]).sum()\n",
    "    return image.addBands(output, None, True)\n",
    "\n",
    "def RefinedLee(image):\n",
    "    \"\"\"\n",
    "    This filter is modified from the implementation by Guido Lemoine \n",
    "    Source: Lemoine et al. https://code.earthengine.google.com/5d1ed0a0f0417f098fdfd2fa137c3d0c\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: ee.Image\n",
    "        Image to be filtered\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: ee.Image\n",
    "        Filtered Image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    bandNames = image.bandNames().remove('angle')\n",
    "\n",
    "    def inner(b):\n",
    "\n",
    "        img = image.select([b]);\n",
    "    \n",
    "        # img must be linear, i.e. not in dB!\n",
    "        # Set up 3x3 kernels \n",
    "        weights3 = ee.List.repeat(ee.List.repeat(1,3),3);\n",
    "        kernel3 = ee.Kernel.fixed(3,3, weights3, 1, 1, False);\n",
    "  \n",
    "        mean3 = img.reduceNeighborhood(ee.Reducer.mean(), kernel3);\n",
    "        variance3 = img.reduceNeighborhood(ee.Reducer.variance(), kernel3);\n",
    "  \n",
    "        # Use a sample of the 3x3 windows inside a 7x7 windows to determine gradients and directions\n",
    "        sample_weights = ee.List([[0,0,0,0,0,0,0], [0,1,0,1,0,1,0],[0,0,0,0,0,0,0], [0,1,0,1,0,1,0], [0,0,0,0,0,0,0], [0,1,0,1,0,1,0],[0,0,0,0,0,0,0]]);\n",
    "  \n",
    "        sample_kernel = ee.Kernel.fixed(7,7, sample_weights, 3,3, False);\n",
    "  \n",
    "        # Calculate mean and variance for the sampled windows and store as 9 bands\n",
    "        sample_mean = mean3.neighborhoodToBands(sample_kernel); \n",
    "        sample_var = variance3.neighborhoodToBands(sample_kernel);\n",
    "  \n",
    "        # Determine the 4 gradients for the sampled windows\n",
    "        gradients = sample_mean.select(1).subtract(sample_mean.select(7)).abs();\n",
    "        gradients = gradients.addBands(sample_mean.select(6).subtract(sample_mean.select(2)).abs());\n",
    "        gradients = gradients.addBands(sample_mean.select(3).subtract(sample_mean.select(5)).abs());\n",
    "        gradients = gradients.addBands(sample_mean.select(0).subtract(sample_mean.select(8)).abs());\n",
    "  \n",
    "        # And find the maximum gradient amongst gradient bands\n",
    "        max_gradient = gradients.reduce(ee.Reducer.max());\n",
    "  \n",
    "        # Create a mask for band pixels that are the maximum gradient\n",
    "        gradmask = gradients.eq(max_gradient);\n",
    "  \n",
    "        # duplicate gradmask bands: each gradient represents 2 directions\n",
    "        gradmask = gradmask.addBands(gradmask);\n",
    "  \n",
    "        # Determine the 8 directions\n",
    "        directions = sample_mean.select(1).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(7))).multiply(1);\n",
    "        directions = directions.addBands(sample_mean.select(6).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(2))).multiply(2));\n",
    "        directions = directions.addBands(sample_mean.select(3).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(5))).multiply(3));\n",
    "        directions = directions.addBands(sample_mean.select(0).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(8))).multiply(4));\n",
    "        # The next 4 are the not() of the previous 4\n",
    "        directions = directions.addBands(directions.select(0).Not().multiply(5));\n",
    "        directions = directions.addBands(directions.select(1).Not().multiply(6));\n",
    "        directions = directions.addBands(directions.select(2).Not().multiply(7));\n",
    "        directions = directions.addBands(directions.select(3).Not().multiply(8));\n",
    "  \n",
    "        # Mask all values that are not 1-8\n",
    "        directions = directions.updateMask(gradmask);\n",
    "  \n",
    "        # \"collapse\" the stack into a singe band image (due to masking, each pixel has just one value (1-8) in it's directional band, and is otherwise masked)\n",
    "        directions = directions.reduce(ee.Reducer.sum());  \n",
    "  \n",
    "        sample_stats = sample_var.divide(sample_mean.multiply(sample_mean));\n",
    "  \n",
    "        #Calculate localNoiseVariance\n",
    "        sigmaV = sample_stats.toArray().arraySort().arraySlice(0,0,5).arrayReduce(ee.Reducer.mean(), [0]);\n",
    "  \n",
    "        # Set up the 7*7 kernels for directional statistics\n",
    "        rect_weights = ee.List.repeat(ee.List.repeat(0,7),3).cat(ee.List.repeat(ee.List.repeat(1,7),4));\n",
    "  \n",
    "        diag_weights = ee.List([[1,0,0,0,0,0,0], [1,1,0,0,0,0,0], [1,1,1,0,0,0,0], [1,1,1,1,0,0,0], [1,1,1,1,1,0,0], [1,1,1,1,1,1,0], [1,1,1,1,1,1,1]]);\n",
    "  \n",
    "        rect_kernel = ee.Kernel.fixed(7,7, rect_weights, 3, 3, False);\n",
    "        diag_kernel = ee.Kernel.fixed(7,7, diag_weights, 3, 3, False);\n",
    "  \n",
    "        # Create stacks for mean and variance using the original kernels. Mask with relevant direction.\n",
    "        dir_mean = img.reduceNeighborhood(ee.Reducer.mean(), rect_kernel).updateMask(directions.eq(1));\n",
    "        dir_var = img.reduceNeighborhood(ee.Reducer.variance(), rect_kernel).updateMask(directions.eq(1));\n",
    "  \n",
    "        dir_mean = dir_mean.addBands(img.reduceNeighborhood(ee.Reducer.mean(), diag_kernel).updateMask(directions.eq(2)));\n",
    "        dir_var = dir_var.addBands(img.reduceNeighborhood(ee.Reducer.variance(), diag_kernel).updateMask(directions.eq(2)));\n",
    "  \n",
    "        # and add the bands for rotated kernels\n",
    "        for i in range(1, 4):\n",
    "            dir_mean = dir_mean.addBands(img.reduceNeighborhood(ee.Reducer.mean(), rect_kernel.rotate(i)).updateMask(directions.eq(2*i+1)))\n",
    "            dir_var = dir_var.addBands(img.reduceNeighborhood(ee.Reducer.variance(), rect_kernel.rotate(i)).updateMask(directions.eq(2*i+1)))\n",
    "            dir_mean = dir_mean.addBands(img.reduceNeighborhood(ee.Reducer.mean(), diag_kernel.rotate(i)).updateMask(directions.eq(2*i+2)))\n",
    "            dir_var = dir_var.addBands(img.reduceNeighborhood(ee.Reducer.variance(), diag_kernel.rotate(i)).updateMask(directions.eq(2*i+2)))\n",
    "\n",
    "  \n",
    "        # \"collapse\" the stack into a single band image (due to masking, each pixel has just one value in it's directional band, and is otherwise masked)\n",
    "        dir_mean = dir_mean.reduce(ee.Reducer.sum());\n",
    "        dir_var = dir_var.reduce(ee.Reducer.sum());\n",
    "  \n",
    "        # A finally generate the filtered value\n",
    "        varX = dir_var.subtract(dir_mean.multiply(dir_mean).multiply(sigmaV)).divide(sigmaV.add(1.0))\n",
    "  \n",
    "        b = varX.divide(dir_var)\n",
    "        result = dir_mean.add(b.multiply(img.subtract(dir_mean)))\n",
    "  \n",
    "        return result.arrayProject([0]).arrayFlatten([['sum']]).float()\n",
    "    \n",
    "    result = ee.ImageCollection(bandNames.map(inner)).toBands().rename(bandNames).copyProperties(image)\n",
    "    \n",
    "    return image.addBands(result, None, True) \n",
    "\n",
    "\n",
    "\n",
    "def leesigma(image,KERNEL_SIZE):\n",
    "    \"\"\"\n",
    "    Implements the improved lee sigma filter to one image. \n",
    "    It is implemented as described in, Lee, J.-S. Wen, J.-H. Ainsworth, T.L. Chen, K.-S. Chen, A.J. \n",
    "    Improved sigma filter for speckle filtering of SAR imagery. \n",
    "    IEEE Trans. Geosci. Remote Sens. 2009, 47, 202–213.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ee.Image\n",
    "        Image to be filtered\n",
    "    KERNEL_SIZE : positive odd integer\n",
    "        Neighbourhood window size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.Image\n",
    "        Filtered Image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #parameters\n",
    "    Tk = ee.Image.constant(7) #number of bright pixels in a 3x3 window\n",
    "    sigma = 0.9\n",
    "    enl = 4\n",
    "    target_kernel = 3\n",
    "    bandNames = image.bandNames().remove('angle')\n",
    "  \n",
    "    #compute the 98 percentile intensity \n",
    "    z98 = ee.Dictionary(image.select(bandNames).reduceRegion(\n",
    "                reducer= ee.Reducer.percentile([98]),\n",
    "                geometry= image.geometry(),\n",
    "                scale=10,\n",
    "                maxPixels=1e13\n",
    "            )).toImage()\n",
    "  \n",
    "\n",
    "    #select the strong scatterers to retain\n",
    "    brightPixel = image.select(bandNames).gte(z98)\n",
    "    K = brightPixel.reduceNeighborhood(ee.Reducer.countDistinctNonNull()\n",
    "            ,ee.Kernel.square(target_kernel/2)) \n",
    "    retainPixel = K.gte(Tk)\n",
    "  \n",
    "  \n",
    "    #compute the a-priori mean within a 3x3 local window\n",
    "    #original noise standard deviation since the data is 5 look\n",
    "    eta = 1.0/math.sqrt(enl) \n",
    "    eta = ee.Image.constant(eta)\n",
    "    #MMSE applied to estimate the apriori mean\n",
    "    reducers = ee.Reducer.mean().combine( \\\n",
    "                      reducer2= ee.Reducer.variance(), \\\n",
    "                      sharedInputs= True\n",
    "                      )\n",
    "    stats = image.select(bandNames).reduceNeighborhood( \\\n",
    "                      reducer= reducers, \\\n",
    "                          kernel= ee.Kernel.square(target_kernel/2,'pixels'), \\\n",
    "                              optimization= 'window')\n",
    "    meanBand = bandNames.map(lambda bandName: ee.String(bandName).cat('_mean'))\n",
    "    varBand = bandNames.map(lambda bandName:  ee.String(bandName).cat('_variance'))\n",
    "        \n",
    "    z_bar = stats.select(meanBand)\n",
    "    varz = stats.select(varBand)\n",
    "    \n",
    "    oneImg = ee.Image.constant(1)\n",
    "    varx = (varz.subtract(z_bar.abs().pow(2).multiply(eta.pow(2)))).divide(oneImg.add(eta.pow(2)))\n",
    "    b = varx.divide(varz)\n",
    "    xTilde = oneImg.subtract(b).multiply(z_bar.abs()).add(b.multiply(image.select(bandNames)))\n",
    "  \n",
    "    #step 3: compute the sigma range\n",
    "    #Lookup table (J.S.Lee et al 2009) for range and eta values for intensity (only 4 look is shown here)\n",
    "    LUT = ee.Dictionary({0.5: ee.Dictionary({'I1': 0.694,'I2': 1.385,'eta': 0.1921}),\n",
    "                                 0.6: ee.Dictionary({'I1': 0.630,'I2': 1.495,'eta': 0.2348}),\n",
    "                                 0.7: ee.Dictionary({'I1': 0.560,'I2': 1.627,'eta': 0.2825}),\n",
    "                                 0.8: ee.Dictionary({'I1': 0.480,'I2': 1.804,'eta': 0.3354}),\n",
    "                                 0.9: ee.Dictionary({'I1': 0.378,'I2': 2.094,'eta': 0.3991}),\n",
    "                                 0.95: ee.Dictionary({'I1': 0.302,'I2': 2.360,'eta': 0.4391})});\n",
    "  \n",
    "    #extract data from lookup\n",
    "    sigmaImage = ee.Dictionary(LUT.get(str(sigma))).toImage()\n",
    "    I1 = sigmaImage.select('I1')\n",
    "    I2 = sigmaImage.select('I2')\n",
    "    #new speckle sigma\n",
    "    nEta = sigmaImage.select('eta')\n",
    "    #establish the sigma ranges\n",
    "    I1 = I1.multiply(xTilde)\n",
    "    I2 = I2.multiply(xTilde)\n",
    "  \n",
    "    #step 3: apply MMSE filter for pixels in the sigma range\n",
    "    #MMSE estimator\n",
    "    mask = image.select(bandNames).gte(I1).Or(image.select(bandNames).lte(I2))\n",
    "    z = image.select(bandNames).updateMask(mask)\n",
    "  \n",
    "    stats = z.reduceNeighborhood( \\\n",
    "                      reducer= reducers, \\\n",
    "                          kernel= ee.Kernel.square(KERNEL_SIZE/2,'pixels'), \\\n",
    "                              optimization= 'window')\n",
    "        \n",
    "    z_bar = stats.select(meanBand)\n",
    "    varz = stats.select(varBand)\n",
    "    \n",
    "    \n",
    "    varx = (varz.subtract(z_bar.abs().pow(2).multiply(nEta.pow(2)))).divide(oneImg.add(nEta.pow(2)))\n",
    "    b = varx.divide(varz)\n",
    "    #if b is negative set it to zero\n",
    "    new_b = b.where(b.lt(0), 0)\n",
    "    xHat = oneImg.subtract(new_b).multiply(z_bar.abs()).add(new_b.multiply(z))\n",
    "  \n",
    "    #remove the applied masks and merge the retained pixels and the filtered pixels\n",
    "    xHat = image.select(bandNames).updateMask(retainPixel).unmask(xHat)\n",
    "    output = ee.Image(xHat).rename(bandNames)\n",
    "    return image.addBands(output, None, True)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------//\n",
    "# 2. MONO-TEMPORAL SPECKLE FILTER (WRAPPER)\n",
    "#---------------------------------------------------------------------------//\n",
    "\n",
    "\n",
    "def MonoTemporal_Filter(coll,KERNEL_SIZE, SPECKLE_FILTER) :\n",
    "    \"\"\"\n",
    "    A wrapper function for monotemporal filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coll : ee Image collection\n",
    "        the image collection to be filtered\n",
    "    KERNEL_SIZE : odd integer\n",
    "        Spatial Neighbourhood window\n",
    "    SPECKLE_FILTER : String\n",
    "        Type of speckle filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.ImageCollection\n",
    "        An image collection where a mono-temporal filter is applied to each \n",
    "        image individually\n",
    "\n",
    "    \"\"\"\n",
    "    def _filter(image):    \n",
    "       if (SPECKLE_FILTER=='BOXCAR'):\n",
    "          _filtered = boxcar(image, KERNEL_SIZE)\n",
    "       elif (SPECKLE_FILTER=='LEE'):\n",
    "          _filtered = leefilter(image, KERNEL_SIZE)\n",
    "       elif (SPECKLE_FILTER=='GAMMA MAP'):\n",
    "          _filtered = gammamap(image, KERNEL_SIZE)\n",
    "       elif (SPECKLE_FILTER=='REFINED LEE'):\n",
    "          _filtered = RefinedLee(image)\n",
    "       elif (SPECKLE_FILTER=='LEE SIGMA'):\n",
    "          _filtered = leesigma(image, KERNEL_SIZE)\n",
    "       return _filtered\n",
    "    return coll.map(_filter)\n",
    "\n",
    "# ---------------------------------------------------------------------------//\n",
    "# 3. MULTI-TEMPORAL SPECKLE FILTER\n",
    "# ---------------------------------------------------------------------------//\n",
    "\n",
    "def MultiTemporal_Filter(coll,KERNEL_SIZE, SPECKLE_FILTER,NR_OF_IMAGES):\n",
    "    \"\"\"\n",
    "\n",
    "    A wrapper function for multi-temporal filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coll : ee Image collection\n",
    "        the image collection to be filtered\n",
    "    KERNEL_SIZE : odd integer\n",
    "        Spatial Neighbourhood window\n",
    "    SPECKLE_FILTER : String\n",
    "        Type of speckle filter\n",
    "    NR_OF_IMAGES : positive integer\n",
    "        Number of images to use in multi-temporal filtering\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.ImageCollection\n",
    "        An image collection where a multi-temporal filter is applied to each\n",
    "        image individually\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    def Quegan(image) :\n",
    "        \"\"\"\n",
    "        The following Multi-temporal speckle filters are implemented as described in\n",
    "        S. Quegan and J. J. Yu, “Filtering of multichannel SAR images,” \n",
    "        IEEE Trans Geosci. Remote Sensing, vol. 39, Nov. 2001.\n",
    "        \n",
    "        this function will filter the collection used for the multi-temporal part\n",
    "        it takes care of:\n",
    "        - same image geometry (i.e relative orbit)\n",
    "        - full overlap of image\n",
    "        - amount of images taken for filtering \n",
    "            -- all before\n",
    "           -- if not enough, images taken after the image to filter are added\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : ee.Image\n",
    "            Image to be filtered\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ee.Image\n",
    "            Filtered image\n",
    "\n",
    "        \"\"\"\n",
    "        def setresample(image):\n",
    "                return image.resample()\n",
    "            \n",
    "        def get_filtered_collection(image):\n",
    "            \"\"\"\n",
    "            Generate a dedicated image collection\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            image : ee.Image\n",
    "                Image whose geometry is used to define the new collection\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            ee Image collection\n",
    "\n",
    "            \"\"\"\n",
    "  \n",
    "            #filter collection over are and by relative orbit\n",
    "            s1_coll = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \\\n",
    "                .filterBounds(image.geometry()) \\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', ee.List(image.get('transmitterReceiverPolarisation')).get(-1))) \\\n",
    "                .filter(ee.Filter.Or(ee.Filter.eq('relativeOrbitNumber_stop', image.get('relativeOrbitNumber_stop')), \\\n",
    "                                     ee.Filter.eq('relativeOrbitNumber_stop', image.get('relativeOrbitNumber_start'))\n",
    "                )).map(setresample)\n",
    "      \n",
    "            #a function that takes the image and checks for the overlap\n",
    "            def check_overlap(_image):\n",
    "                \"\"\"\n",
    "                get all S1 frames from this date intersecting with the image bounds\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                _image : ee.Image\n",
    "                    Image to check the overlap with\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                ee Image Collection\n",
    "                    A collection with matching geometry\n",
    "\n",
    "                \"\"\"\n",
    "                \n",
    "                # get all S1 frames from this date intersecting with the image bounds\n",
    "                s1 = s1_coll.filterDate(_image.date(), _image.date().advance(1, 'day'))\n",
    "                # intersect those images with the image to filter\n",
    "                intersect = image.geometry().intersection(s1.geometry().dissolve(), 10)\n",
    "                # check if intersect is sufficient\n",
    "                valid_date = ee.Algorithms.If(intersect.area(10).divide(image.geometry().area(10)).gt(0.95), \\\n",
    "                                              _image.date().format('YYYY-MM-dd')\n",
    "                                              )\n",
    "                return ee.Feature(None, {'date': valid_date})\n",
    "      \n",
    "      \n",
    "            # this function will pick up the acq dates for fully overlapping acquisitions before the image acquistion\n",
    "            dates_before = s1_coll.filterDate('2014-01-01', image.date().advance(1, 'day')) \\\n",
    "                                    .sort('system:time_start', False).limit(5*NR_OF_IMAGES) \\\n",
    "                                    .map(check_overlap).distinct('date').aggregate_array('date')\n",
    "    \n",
    "            # if the images before are not enough, we add images from after the image acquisition \n",
    "            # this will only be the case at the beginning of S1 mission\n",
    "            dates = ee.List(ee.Algorithms.If( \\\n",
    "                                             dates_before.size().gte(NR_OF_IMAGES), \\\n",
    "                                                 dates_before.slice(0, NR_OF_IMAGES), \\\n",
    "                                                     s1_coll \\\n",
    "                                                         .filterDate(image.date(), '2100-01-01') \\\n",
    "                                                             .sort('system:time_start', True).limit(5*NR_OF_IMAGES) \\\n",
    "                                                                 .map(check_overlap) \\\n",
    "                                                                     .distinct('date') \\\n",
    "                                                                         .aggregate_array('date') \\\n",
    "                                                                             .cat(dates_before).distinct().sort().slice(0, NR_OF_IMAGES)\n",
    "                                                                             )\n",
    "                                                )\n",
    "    \n",
    "            #now we re-filter the collection to get the right acquisitions for multi-temporal filtering\n",
    "            return ee.ImageCollection(dates.map(lambda date: s1_coll.filterDate(date, ee.Date(date).advance(1,'day')).toList(s1_coll.size())).flatten())\n",
    "      \n",
    "          \n",
    "  \n",
    "        #we get our dedicated image collection for that image\n",
    "        s1 = get_filtered_collection(image)\n",
    "  \n",
    "        bands = image.bandNames().remove('angle')\n",
    "        s1 = s1.select(bands)\n",
    "        meanBands = bands.map(lambda bandName: ee.String(bandName).cat('_mean'))\n",
    "        ratioBands = bands.map(lambda bandName: ee.String(bandName).cat('_ratio'))\n",
    "        count_img = s1.reduce(ee.Reducer.count())\n",
    "\n",
    "        def inner(image):\n",
    "            \"\"\"\n",
    "            Creats an image whose bands are the filtered image and image ratio\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            image : ee.Image\n",
    "                Image to be filtered\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            ee.Image\n",
    "                Filtered image and image ratio\n",
    "\n",
    "            \"\"\"\n",
    "            if (SPECKLE_FILTER=='BOXCAR'):\n",
    "                _filtered = boxcar(image, KERNEL_SIZE).select(bands).rename(meanBands) \n",
    "            elif (SPECKLE_FILTER=='LEE'):\n",
    "                _filtered = leefilter(image, KERNEL_SIZE).select(bands).rename(meanBands)\n",
    "            elif (SPECKLE_FILTER=='GAMMA MAP'):\n",
    "                _filtered = gammamap(image, KERNEL_SIZE).select(bands).rename(meanBands)\n",
    "            elif (SPECKLE_FILTER=='REFINED LEE'):\n",
    "                _filtered = RefinedLee(image).select(bands).rename(meanBands)\n",
    "            elif (SPECKLE_FILTER=='LEE SIGMA'):\n",
    "                _filtered = leesigma(image, KERNEL_SIZE).select(bands).rename(meanBands)\n",
    "    \n",
    "            _ratio = image.select(bands).divide(_filtered).rename(ratioBands) \n",
    "            return _filtered.addBands(_ratio)\n",
    "\n",
    "        isum = s1.map(inner).select(ratioBands).reduce(ee.Reducer.sum())\n",
    "        filtered = inner(image).select(meanBands)\n",
    "        divide = filtered.divide(count_img)\n",
    "        output = divide.multiply(isum).rename(bands)\n",
    "\n",
    "        return image.addBands(output, None, True)\n",
    "    return coll.map(Quegan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interactive map for you to draw a polygon to signify your aoi\n",
    "\n",
    "## Create a map centered at a specific location\n",
    "m = geemap.Map(center=[20, 0], zoom=2, basemap='HYBRID')\n",
    "## Add drawing tools\n",
    "m.add_draw_control()\n",
    "## Display the map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the drawn features\n",
    "draw_features = m.draw_features[0]\n",
    "## Establish ee.Polygon from drawn area of interest to collect imagery\n",
    "aoi = ee.Geometry.Polygon(draw_features.getInfo()['geometry']['coordinates'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2019-09-01' ## start date of search window\n",
    "end_date = '2024-10-18' ## end date of search window\n",
    "s2_cloud_cov = 1 ## percentage of clouds in sentinel-2 multispectral imagery, less means you see more surface\n",
    "orbit = 'ASCENDING' ## orbit for imagery\n",
    "\n",
    "collections = {}\n",
    "for i in range(int(start_date[:4]), int(end_date[:4])):\n",
    "    collections[f's1_{i}_{i+1}'], collections[f's2_10m_{i}_{i+1}'], collections[f's2_10m_ndvi_{i}_{i+1}'], collections[f's2_10m_ndwi_{i}_{i+1}'], collections[f's2_10m_msavi2_{i}_{i+1}'], collections[f's2_10m_bsi_{i}_{i+1}']  = get_sentinel_imagery(aoi, f'{i}{start_date[4:]}', f'{i+1}{end_date[4:]}', s2_cloud_cov, orbit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_colls = {}\n",
    "total_colls['s1'], total_colls['s2_10m'], total_colls['s2_10m_ndvi'], total_colls['s2_10m_ndwi'], total_colls['s2_10m_msavi2'], total_colls['s2_10m_bsi']= get_sentinel_imagery(aoi, start_date, end_date, s2_cloud_cov, orbit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colls = []\n",
    "for i, coll in enumerate(collections):\n",
    "    if collections[coll].size().getInfo() != 0: # removes image collections that are empty\n",
    "        colls.append(coll)\n",
    "\n",
    "for i, coll in enumerate(colls):\n",
    "    print(f'{i}: {coll}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Distribution of Spectral Indices Values for Select Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no mask\n",
    "\n",
    "# all I need for segmenting images into coastlines would be\n",
    "# Need to make this applicable for each image in image collection\n",
    "# it will read in a specified band of a passed image collection, then segment it using Otsu's method\n",
    "# It should create a new image collection of \n",
    "#   1) Water Bodies, 2) Land Bodies, 3) Shorelines\n",
    "# those can then be used to measure horizontal changes in shoreline over time\n",
    "\n",
    "# Get the first image in the collection (or map over the collection if needed)\n",
    "s2_date_list = collections[colls[4]].map(get_date).aggregate_array('date').getInfo()\n",
    "ndwi_image = collections[colls[4]].first()\n",
    "ndwi_image = ndwi_image.updateMask(ndwi_image.lt(0.0))\n",
    "\n",
    "ndwi_date = s2_date_list[0]\n",
    "ndwi_histogram = get_histogram(ndwi_image, 10, 150, 'NDWI')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "ndwi_values = ndwi_histogram['bucketMeans']\n",
    "ndwi_counts = ndwi_histogram['histogram']\n",
    "\n",
    "\n",
    "# second date\n",
    "# Get the first image in the collection (or map over the collection if needed)\n",
    "s2_date_list2 = collections[colls[25]].map(get_date).aggregate_array('date').getInfo()\n",
    "ndwi_image2 = collections[colls[25]].first()\n",
    "ndwi_image2 = ndwi_image2.updateMask(ndwi_image2.lt(0.0))\n",
    "\n",
    "\n",
    "ndwi_date2 = s2_date_list2[0]\n",
    "ndwi_histogram2 = get_histogram(ndwi_image2, 10, 150, 'NDWI')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "ndwi_values2 = ndwi_histogram2['bucketMeans']\n",
    "ndwi_counts2 = ndwi_histogram2['histogram']\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(ndwi_values, ndwi_counts, width=0.02, color='blue', alpha=0.7, label = ndwi_date)\n",
    "plt.bar(ndwi_values2, ndwi_counts2, width=0.02, color='red', alpha=0.4, label = ndwi_date2)\n",
    "plt.title(f\"NDWI Histogram\")\n",
    "plt.xlabel(\"NDWI Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "edge_i, edge_k = otsu_trimodal_from_histogram(ndwi_histogram)\n",
    "if edge_i < edge_k:\n",
    "    ndwi_thresh = [edge_i, edge_k]\n",
    "else:\n",
    "    ndwi_thresh = [edge_k, edge_i]\n",
    "print(f'NDWI Thershold 1 {ndwi_date}= {edge_i}')\n",
    "print(f'NDWI Thershold 2 {ndwi_date}= {edge_k}')\n",
    "\n",
    "\n",
    "edge_i2, edge_k2 = otsu_trimodal_from_histogram(ndwi_histogram2)\n",
    "if edge_i2 < edge_k2:\n",
    "    ndwi_thresh2 = [edge_i2, edge_k2]\n",
    "else:\n",
    "    ndwi_thresh2 = [edge_k2, edge_i2]\n",
    "\n",
    "print(f'NDWI Thershold 1 {ndwi_date2}= {edge_i2}')\n",
    "print(f'NDWI Thershold 2 {ndwi_date2}= {edge_k2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no mask\n",
    "\n",
    "# all I need for segmenting images into coastlines would be\n",
    "# Need to make this applicable for each image in image collection\n",
    "# it will read in a specified band of a passed image collection, then segment it using Otsu's method\n",
    "# It should create a new image collection of \n",
    "#   1) Water Bodies, 0) Land Bodies, 3) Shorelines\n",
    "# those can then be used to measure horizontal changes in shoreline over time\n",
    "\n",
    "# Get the first image in the collection (or map over the collection if needed)\n",
    "s2_date_list = collections[colls[3]].map(get_date).aggregate_array('date').getInfo()\n",
    "ndvi_image = collections[colls[3]].first()\n",
    "ndvi_image = ndvi_image.updateMask(ndvi_image.gte(0.0))     ### mask to remove the water and low vegetation areas\n",
    "\n",
    "ndvi_date = s2_date_list[0]\n",
    "ndvi_histogram = get_histogram(ndvi_image, 10, 150, 'NDVI')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "ndvi_values = ndvi_histogram['bucketMeans']\n",
    "ndvi_counts = ndvi_histogram['histogram']\n",
    "\n",
    "\n",
    "# second date\n",
    "# Get the first image in the collection (or map over the collection if needed)\n",
    "s1_date_list2 = collections[colls[24]].map(get_date).aggregate_array('date').getInfo()\n",
    "ndvi_image2 = collections[colls[24]].first()\n",
    "ndvi_image2 = ndvi_image2.updateMask(ndvi_image2.gte(0.0))\n",
    "\n",
    "ndvi_date2 = s1_date_list2[0]\n",
    "ndvi_histogram2 = get_histogram(ndvi_image2, 10, 150, 'NDVI')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "ndvi_values2 = ndvi_histogram2['bucketMeans']\n",
    "ndvi_counts2 = ndvi_histogram2['histogram']\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(ndvi_values, ndvi_counts, width=0.02, color='blue', alpha=0.7, label = ndvi_date)\n",
    "plt.bar(ndvi_values2, ndvi_counts2, width=0.02, color='red', alpha=0.4, label = ndvi_date2)\n",
    "plt.title(f\"NDVI Histogram\")\n",
    "plt.xlabel(\"NDVI Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "edge_i, edge_k = otsu_trimodal_from_histogram(ndvi_histogram)\n",
    "if edge_i < edge_k:\n",
    "    ndvi_thresh = [edge_i, edge_k]\n",
    "else:\n",
    "    ndvi_thresh = [edge_k, edge_i]\n",
    "\n",
    "edge_i2, edge_k2 = otsu_trimodal_from_histogram(ndvi_histogram2)\n",
    "if edge_i2 < edge_k2:\n",
    "    ndvi_thresh2 = [edge_i2, edge_k2]\n",
    "else:\n",
    "    ndvi_thresh2 = [edge_k2, edge_i2]\n",
    "\n",
    "print(f'NDVI Thersholds for {ndvi_date} = {ndvi_thresh}')\n",
    "print(f'NDVI Thersholds for {ndvi_date2} = {ndvi_thresh2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = collections[colls[5]].map(get_date).aggregate_array('date').getInfo()\n",
    "msavi2_image = collections[colls[5]].first()\n",
    "msavi2_image = msavi2_image.updateMask(msavi2_image.gte(0.0))     ### mask to remove the water and low vegetation areas\n",
    "\n",
    "msavi2_date = s2_date_list[0]\n",
    "msavi2_histogram = get_histogram(msavi2_image, 10, 150, 'MSAVI2')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "msavi2_values = msavi2_histogram['bucketMeans']\n",
    "msavi2_counts = msavi2_histogram['histogram']\n",
    "\n",
    "\n",
    "# second date\n",
    "# Get the first image in the collection (or map over the collection if needed)\n",
    "s2_date_list2 = collections[colls[26]].map(get_date).aggregate_array('date').getInfo()\n",
    "msavi2_image2 = collections[colls[26]].first()\n",
    "msavi2_image2 = msavi2_image2.updateMask(msavi2_image2.gte(0.0))\n",
    "\n",
    "msavi2_date2 = s2_date_list2[0]\n",
    "msavi2_histogram2 = get_histogram(msavi2_image2, 10, 150, 'MSAVI2')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "msavi2_values2 = msavi2_histogram2['bucketMeans']\n",
    "msavi2_counts2 = msavi2_histogram2['histogram']\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(msavi2_values, msavi2_counts, width=0.02, color='blue', alpha=0.7, label = msavi2_date)\n",
    "plt.bar(msavi2_values2, msavi2_counts2, width=0.02, color='red', alpha=0.4, label = msavi2_date2)\n",
    "plt.title(f\"MSAVI2 Histogram\")\n",
    "plt.xlabel(\"MSAVI2 Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "edge_i, edge_k = otsu_trimodal_from_histogram(msavi2_histogram)\n",
    "if edge_i < edge_k:\n",
    "    msavi2_thresh = [edge_i, edge_k]\n",
    "else:\n",
    "    msavi2_thresh = [edge_k, edge_i]\n",
    "\n",
    "edge_i2, edge_k2 = otsu_trimodal_from_histogram(msavi2_histogram2)\n",
    "if edge_i2 < edge_k2:\n",
    "    msavi2_thresh2 = [edge_i2, edge_k2]\n",
    "else:\n",
    "    msavi2_thresh2 = [edge_k2, edge_i2]\n",
    "\n",
    "print(f'MSAVI2 Thersholds for {msavi2_date} = {msavi2_thresh}')\n",
    "print(f'MSAVI2 Thersholds for {msavi2_date2} = {msavi2_thresh2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = collections[colls[6]].map(get_date).aggregate_array('date').getInfo()\n",
    "bsi_image = collections[colls[6]].first()\n",
    "\n",
    "bsi_date = s2_date_list[0]\n",
    "bsi_histogram = get_histogram(bsi_image, 10, 150, 'BSI')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "bsi_values = bsi_histogram['bucketMeans']\n",
    "bsi_counts = bsi_histogram['histogram']\n",
    "\n",
    "\n",
    "# second date\n",
    "# Get the first image in the collection (or map over the collection if needed)\n",
    "s2_date_list2 = collections[colls[27]].map(get_date).aggregate_array('date').getInfo()\n",
    "bsi_image2 = collections[colls[27]].first()\n",
    "\n",
    "bsi_date2 = s2_date_list2[0]\n",
    "bsi_histogram2 = get_histogram(bsi_image2, 10, 150, 'BSI')\n",
    "\n",
    "# Extract the histogram values for plotting\n",
    "bsi_values2 = bsi_histogram2['bucketMeans']\n",
    "bsi_counts2 = bsi_histogram2['histogram']\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bsi_values, bsi_counts, width=0.02, color='blue', alpha=0.7, label = bsi_date)\n",
    "plt.bar(bsi_values2, bsi_counts2, width=0.02, color='red', alpha=0.4, label = bsi_date2)\n",
    "plt.title(f\"BSI Histogram\")\n",
    "plt.xlabel(\"BSI Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "edge_i, edge_k = otsu_trimodal_from_histogram(bsi_histogram)\n",
    "if edge_i < edge_k:\n",
    "    bsi_thresh = [edge_i, edge_k]\n",
    "else:\n",
    "    bsi_thresh = [edge_k, edge_i]\n",
    "\n",
    "edge_i2, edge_k2 = otsu_trimodal_from_histogram(bsi_histogram2)\n",
    "if edge_i2 < edge_k2:\n",
    "    bsi_thresh2 = [edge_i2, edge_k2]\n",
    "else:\n",
    "    bsi_thresh2 = [edge_k2, edge_i2]\n",
    "\n",
    "print(f'BSI Thersholds for {bsi_date} = {bsi_thresh}')\n",
    "print(f'BSI Thersholds for {bsi_date2} = {bsi_thresh2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment water bodies from land/marsh\n",
    "- will use pixels to segment the areas, can be used for area change measurements\n",
    "- can get the edges of the segmented pixels for horizontal changes along transects\n",
    "- Segment using different methods (Otsu's thresholoding on NDWI, Object-based change detection on NDWI and/or GLCMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = total_colls['s2_10m'].map(get_date).aggregate_array('date').getInfo()\n",
    "s2_date = s2_date_list[0]\n",
    "\n",
    "## Create a map centered at a specific location\n",
    "m = geemap.Map()\n",
    "m.centerObject(aoi, 12)\n",
    "\n",
    "# Add the original rgb image for context\n",
    "m.addLayer(total_colls['s2_10m'].first(), {'min': 0, 'max': 2000, 'bands': ['B4', 'B3', 'B2']}, f'{s2_date} RGB')\n",
    "\n",
    "\n",
    "#draw the specific aois you want to get time series info from\n",
    "# for now needs to be an area\n",
    "# later will add stuff for transects\n",
    "m.add_draw_control()\n",
    "m.addLayerControl()\n",
    "display(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to add another \n",
    "## Get the drawn features\n",
    "drawn_features = m.draw_features\n",
    "\n",
    "aois = ee.FeatureCollection(drawn_features)\n",
    "\n",
    "sites = {}\n",
    "for i, site in enumerate(drawn_features):\n",
    "    sites[f'Site {i}'] = ee.Geometry.Polygon(site.getInfo()['geometry']['coordinates'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shoreline and Veg line detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get info of specific areas of information\n",
    "- can be specific areas where change has occurred, in my application probably sediment enrichment\n",
    "- (working) get a time series of NDVI and NDWI for the area delineated by the drawn aois and shoreline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = total_colls['s2_10m'].map(get_date).aggregate_array('date').getInfo()\n",
    "\n",
    "clipped_rgb = total_colls['s2_10m'].map(lambda img: img.clip(aois))\n",
    "clipped_s1 = total_colls['s1'].map(lambda img: img.clip(aois))\n",
    "clipped_ndvi = total_colls['s2_10m_ndvi'].map(lambda img: img.clip(aois))\n",
    "clipped_ndwi = total_colls['s2_10m_ndwi'].map(lambda img: img.clip(aois))\n",
    "clipped_msavi2 = total_colls['s2_10m_msavi2'].map(lambda img: img.clip(aois))\n",
    "clipped_bsi = total_colls['s2_10m_bsi'].map(lambda img: img.clip(aois))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot time series for the sites you identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sites:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_VI_ts(sites['Site 0'], 'Unit 1D', 'separate')\n",
    "# plot_VI_ts(sites['Site 2'], 'Unit 1B', 'together')\n",
    "# plot_VI_ts(sites['Site 0'], 'Unit 1D', 'together') # site finished in Sept 2019\n",
    "# plot_VI_ts(sites['Site 1'], 'Unit 1E', 'together') # site finished in Sept 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_GRD_ts(sites['Site 0'], 'Unit 1D', 'together', total_colls['s1'])\n",
    "# plot_GRD_ts(sites['Site 2'], 'Unit 1B', 'together', total_colls['s1'])\n",
    "# plot_GRD_ts(sites['Site 0'], 'Unit 1D', 'together', total_colls['s1']) # site finished in Sept 2019\n",
    "# plot_GRD_ts(sites['Site 1'], 'Unit 1E', 'together', total_colls['s1']) # site finished in Sept 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_VI_ts_combined(aoi, aoi_str, plottype):\n",
    "#     # Plotting the mean, standard deviation, and quartiles\n",
    "#     if plottype == 'together':\n",
    "    \n",
    "#         plt.figure(figsize=(14, 7))\n",
    "\n",
    "#         for imcoll in [total_colls['s2_10m_ndvi'], total_colls['s2_10m_ndwi']]:\n",
    "#             if imcoll == total_colls['s2_10m_ndvi']:\n",
    "#                 df_msavi2 = extract_stats_from_aoi(total_colls['s2_10m_msavi2'], 'MSAVI2', aoi)\n",
    "#                 df_ndvi = extract_stats_from_aoi(imcoll, 'NDVI', aoi)\n",
    "\n",
    "#                 # Replace NDVI/MSAVI2 values with NDVI where MSAVI2 > 0.6\n",
    "#                 df_combined = df_msavi2.copy()\n",
    "#                 condition = df_msavi2['mean'] > 0.6\n",
    "#                 df_combined.loc[condition, 'mean'] = df_ndvi.loc[condition, 'mean']\n",
    "#                 df_combined.loc[condition, 'stdDev'] = df_ndvi.loc[condition, 'stdDev']\n",
    "#                 df_combined.loc[condition, 'p25'] = df_ndvi.loc[condition, 'p25']\n",
    "#                 df_combined.loc[condition, 'p75'] = df_ndvi.loc[condition, 'p75']\n",
    "\n",
    "#                 plt.plot(df_combined['date'], df_combined['mean'], color='green', marker='o', linestyle='-', label='Mean NDVI/MSAVI2 (with NDVI substitution)')\n",
    "#                 plt.fill_between(df_combined['date'],\n",
    "#                                 df_combined['mean'] - df_combined['stdDev'],\n",
    "#                                 df_combined['mean'] + df_combined['stdDev'],\n",
    "#                                 color='green', alpha=0.2, label='NDVI/MSAVI2 ± 1 Std Dev (with NDVI substitution)'\n",
    "#                             )\n",
    "\n",
    "#                 # Plot NDVI/MSAVI2 Interquartile Range (IQR)\n",
    "#                 plt.fill_between(df_combined['date'],\n",
    "#                                 df_combined['p25'],\n",
    "#                                 df_combined['p75'],\n",
    "#                                 color='green', alpha=0.5, linestyle='-', label='NDVI/MSAVI2 IQR (with NDVI substitution)'\n",
    "#                             )\n",
    "\n",
    "#             elif imcoll == total_colls['s2_10m_ndwi']:\n",
    "#                 df_ndwi = extract_stats_from_aoi(imcoll, 'NDWI', aoi)\n",
    "#                 plt.plot(df_ndwi['date'], df_ndwi['mean'], color='blue', marker='o', linestyle='-', label='Mean NDWI')\n",
    "#                 plt.fill_between(df_ndwi['date'],\n",
    "#                                     df_ndwi['mean'] - df_ndwi['stdDev'],\n",
    "#                                     df_ndwi['mean'] + df_ndwi['stdDev'],\n",
    "#                                     color='blue', alpha=0.2, label='NDWI ± 1 Std Dev'\n",
    "#                                 )\n",
    "\n",
    "#                 # Plot NDWI Interquartile Range (IQR)\n",
    "#                 plt.fill_between(df_ndwi['date'],\n",
    "#                                     df_ndwi['p25'],\n",
    "#                                     df_ndwi['p75'],\n",
    "#                                     color='blue', alpha=0.5, linestyle='-', label='NDWI IQR (25th to 75th Percentile)'\n",
    "#                                 )\n",
    "        \n",
    "\n",
    "\n",
    "#         # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "#         storm_dates = {\n",
    "#             'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "#             'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "#             'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "#             'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "#             'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "#             'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "#         }\n",
    "\n",
    "#         for storm, date in storm_dates.items():\n",
    "#             if storm[0] == 'H':\n",
    "#                 plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "#         # Customizing the plot\n",
    "#         plt.xlabel('Date')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.title(f'NDVI/MSAVI2 and NDWI Change Over Time for {aoi_str}')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.grid(True)\n",
    "\n",
    "#         # Show plot\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif plottype == 'separate':\n",
    "        \n",
    "#         plt.figure(figsize=(14,7))\n",
    "\n",
    "#         df_ndvi = extract_stats_from_aoi(total_colls['s2_10m_ndvi'], 'NDVI', aoi)\n",
    "#         df_msavi2 = extract_stats_from_aoi(total_colls['s2_10m_msavi2'], 'MSAVI2', aoi)\n",
    "\n",
    "#         # Replace NDVI/MSAVI2 values with NDVI where MSAVI2 > 0.6\n",
    "#         df_combined = df_msavi2.copy()\n",
    "#         condition = df_msavi2['mean'] > 0.6\n",
    "#         df_combined.loc[condition, 'mean'] = df_ndvi.loc[condition, 'mean']\n",
    "#         df_combined.loc[condition, 'stdDev'] = df_ndvi.loc[condition, 'stdDev']\n",
    "#         df_combined.loc[condition, 'p25'] = df_ndvi.loc[condition, 'p25']\n",
    "#         df_combined.loc[condition, 'p75'] = df_ndvi.loc[condition, 'p75']\n",
    "\n",
    "#         plt.plot(df_combined['date'], df_combined['mean'], color='green', marker='o', linestyle='-', label='Mean NDVI/MSAVI2 (with NDVI substitution)')\n",
    "#         plt.fill_between(df_combined['date'],\n",
    "#                         df_combined['mean'] - df_combined['stdDev'],\n",
    "#                         df_combined['mean'] + df_combined['stdDev'],\n",
    "#                         color='green', alpha=0.2, label='NDVI/MSAVI2 ± 1 Std Dev (with NDVI substitution)'\n",
    "#                     )\n",
    "\n",
    "#         # Plot NDVI/MSAVI2 Interquartile Range (IQR)\n",
    "#         plt.fill_between(df_combined['date'],\n",
    "#                         df_combined['p25'],\n",
    "#                         df_combined['p75'],\n",
    "#                         color='green', alpha=0.5, linestyle='-', label='NDVI/MSAVI2 IQR (with NDVI substitution)'\n",
    "#                     )\n",
    "        \n",
    "#         # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "#         storm_dates = {\n",
    "#             'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "#             'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "#             'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "#             'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "#             'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "#             'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "#         }\n",
    "\n",
    "#         for storm, date in storm_dates.items():\n",
    "#             if storm[0] == 'H':\n",
    "#                 plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "#         # Customizing the plot\n",
    "#         plt.xlabel('Date')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.title(f'Vegetation Change Over Time for {aoi_str}')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.grid(True)\n",
    "\n",
    "#         # Show plot\n",
    "#         plt.show()\n",
    "\n",
    "#         plt.figure(figsize=(14,7))\n",
    "\n",
    "#         df = extract_stats_from_aoi(total_colls['s2_10m_ndwi'], 'NDWI', aoi)\n",
    "#         plt.plot(df['date'], df['mean'], color='blue', marker='o', linestyle='-', label='Mean NDWI')\n",
    "#         plt.fill_between(df['date'],\n",
    "#                         df['mean'] - df['stdDev'],\n",
    "#                         df['mean'] + df['stdDev'],\n",
    "#                         color='blue', alpha=0.2, label='NDWI ± 1 Std Dev'\n",
    "#                     )\n",
    "\n",
    "#         # Plot NDWI Interquartile Range (IQR)\n",
    "#         plt.fill_between(df['date'],\n",
    "#                         df['p25'],\n",
    "#                         df['p75'],\n",
    "#                         color='blue', alpha=0.5, linestyle='-', label='NDWI IQR (25th to 75th Percentile)'\n",
    "#                     )\n",
    "        \n",
    "#         # Add vertical lines for Hurricane Laura and Hurricane Delta\n",
    "#         storm_dates = {\n",
    "#             'Hurricane Laura': pd.to_datetime('2020-08-27'),\n",
    "#             'Hurricane Delta': pd.to_datetime('2020-10-09'),\n",
    "#             'Hurricane Zeta': pd.to_datetime('2020-10-28'),\n",
    "#             'Hurricane Ida': pd.to_datetime('2021-08-29'),\n",
    "#             'Hurricane Beryl': pd.to_datetime('2024-07-24'),\n",
    "#             'Hurricane Francine': pd.to_datetime('2024-09-11')\n",
    "#         }\n",
    "\n",
    "#         for storm, date in storm_dates.items():\n",
    "#             if storm[0] == 'H':\n",
    "#                 plt.axvline(x=date, color='purple', linestyle='--', linewidth=2)\n",
    "\n",
    "#         # Customizing the plot\n",
    "#         plt.xlabel('Date')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.title(f'NDWI Change Over Time for {aoi_str}')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.grid(True)\n",
    "\n",
    "#         # Show plot\n",
    "#         plt.show()\n",
    "\n",
    "# ## Time series plot to show changes in NDVI and NDWI\n",
    "# plot_VI_ts_combined(sites['Site 4'], 'Unit 1A', 'together')\n",
    "# plot_VI_ts_combined(sites['Site 3'], 'Unit 1B', 'together')\n",
    "# plot_VI_ts_combined(sites['Site 1'], 'Unit 1D', 'together') # site finished in Sept 2019\n",
    "# plot_VI_ts_combined(sites['Site 2'], 'Unit 1E', 'together') # site finished in Sept 2019\n",
    "# plot_VI_ts_combined(sites['Site 0'], '2023 Site', 'together') #Site Created in late 2023\n",
    "\n",
    "\n",
    "# plot_VI_ts_combined(sites['Site 9'], 'Cycle 1', 'together')\n",
    "# plot_VI_ts_combined(sites['Site 6'], 'Cycle 2', 'together')\n",
    "# plot_VI_ts_combined(sites['Site 5'], 'Cycle 3', 'together') \n",
    "# plot_VI_ts_combined(sites['Site 7'], 'Cycle 4', 'together') \n",
    "# plot_VI_ts_combined(sites['Site 8'], 'Cycle 5', 'together') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean NDVI/NDWI Above Q3: small proportion of higher values pulling the mean upwards—perhaps due to localized greening or a recent vegetation growth spurt affecting only part of the area.\n",
    "- Mean NDVI/NDWI Below Q1: extreme low values are present, pulling the mean down—potentially due to vegetation loss, seasonal dry periods, or land cover change in part of the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract time series of Land Area Change (m2) for the marsh creation sites defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, site in enumerate(sites):\n",
    "# Apply land extraction to each NDWI and NDVI image\n",
    "land_images = extract_land(\n",
    "    total_colls['s2_10m'].map(lambda img: img.clip(aoi)), \n",
    "    total_colls['s2_10m_ndwi'].map(lambda img: img.clip(aoi)), \n",
    "    total_colls['s2_10m_ndvi'].map(lambda img: img.clip(aoi))\n",
    ")\n",
    "\n",
    "# Loop over the sites to calculate land area and get dates\n",
    "land_area_lists = []\n",
    "dates_lists = []\n",
    "\n",
    "for idx, site_name in enumerate(sites):\n",
    "    land_area_images = land_images.map(lambda img: img.clip(sites[f'Site {idx}'])).map(calculate_land_area)\n",
    "    land_area_list = land_area_images.aggregate_array('total_land_area').getInfo()\n",
    "    dates_list = land_area_images.aggregate_array('system:time_start').getInfo()\n",
    "    land_area_lists.append(land_area_list)\n",
    "    dates_lists.append(dates_list)\n",
    "\n",
    "# Convert the UNIX timestamps to readable dates using the first site's dates as a reference\n",
    "dates = [datetime.utcfromtimestamp(date / 1000).strftime('%Y-%m-%d') for date in dates_lists[0]]\n",
    "df_data = {'Date': pd.to_datetime(dates)}  # Convert to datetime format\n",
    "\n",
    "# Add land area for each site to the dataframe\n",
    "for idx, site_name in enumerate(sites):\n",
    "    df_data[f'Land Area (sq km), {site_name}'] = land_area_lists[idx]\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "# Correct storm dates in 'YYYYMMDD' format and convert to datetime\n",
    "storm_dates = {\n",
    "    'Hurricane Laura': datetime.strptime('2020-08-27', '%Y-%m-%d'),\n",
    "    'Hurricane Delta': datetime.strptime('2020-10-09', '%Y-%m-%d'),\n",
    "    'Hurricane Zeta': datetime.strptime('2020-10-28', '%Y-%m-%d'),\n",
    "    'Hurricane Ida': datetime.strptime('2021-08-29', '%Y-%m-%d'),\n",
    "    'Hurricane Beryl': datetime.strptime('2024-07-24', '%Y-%m-%d'),\n",
    "    'Hurricane Francine': datetime.strptime('2024-09-14', '%Y-%m-%d')\n",
    "}\n",
    "\n",
    "# Define specific colors for each storm\n",
    "storm_colors = {\n",
    "    'Hurricane Laura': '#1f77b4',  # Dark Blue\n",
    "    'Hurricane Delta': '#2ca02c',  # Dark Green\n",
    "    'Hurricane Zeta': '#d62728',   # Dark Red\n",
    "    'Hurricane Ida': '#ff7f0e',    # Dark Orange\n",
    "    'Hurricane Beryl': '#9467bd',  # Purple\n",
    "    'Hurricane Francine': '#000000'  # Black\n",
    "}\n",
    "\n",
    "# Plotting the land area over time for each site\n",
    "plt.figure(figsize=(15, 6), dpi = 1000)\n",
    "\n",
    "for idx, site_name in enumerate(sites):\n",
    "    if site_name == 'Site 0':\n",
    "        plt.plot(df['Date'], df[f'Land Area (sq km), {site_name}'], marker='o') #label=f'Land Area, Unit 1D'\n",
    "    elif site_name == 'Site 1':\n",
    "        plt.plot(df['Date'], df[f'Land Area (sq km), {site_name}'], marker='o', label=f'Land Area, Unit 1E')     \n",
    "    elif site_name == 'Site 2':\n",
    "        plt.plot(df['Date'], df[f'Land Area (sq km), {site_name}'], marker='o', label=f'Land Area, Unit 1B')\n",
    "    elif site_name == 'Site 3':\n",
    "        plt.plot(df['Date'], df[f'Land Area (sq km), {site_name}'], marker='o', label=f'Land Area, Unit 1A') \n",
    "\n",
    "# Adding hurricane event lines\n",
    "for storm, date in storm_dates.items():\n",
    "    plt.axvline(x=date, color=storm_colors[storm], linestyle='--', linewidth=1.5, label=f'{storm}') #, label=f'{storm}}'\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel('Date', fontweight = 'bold', fontsize = 14)\n",
    "plt.ylabel('Land Area (sq km)', fontweight = 'bold', fontsize = 14)\n",
    "plt.title(f'Land Area Changes Over Time for Unit 1D', fontweight = 'bold', fontsize = 16)\n",
    "plt.xticks(rotation=45, fontweight = 'bold')\n",
    "plt.yticks(fontweight = 'bold')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_colls['s2_10m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_colls['s1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR Supervised Classification from Sentinel-2 Land Masking\n",
    "- these results are not very good, the dielectrics within the sediment causes the returned VV and VH signals to be noisy, or blend in with the surrounding water areas\n",
    "- will try to train on VV/VH ratio, instead of the two bands indidividually, can also include GLCM texture features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1filt = MultiTemporal_Filter(total_colls['s1'], 3, 'LEE SIGMA', total_colls['s1'].size().getInfo())\n",
    "# s1filt = MonoTemporal_Filter(total_colls['s1'], 5, 'LEE SIGMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = land_images.map(get_date).aggregate_array('date').getInfo()\n",
    "# s1_date_list = total_colls['s1'].map(get_date).aggregate_array('date').getInfo()\n",
    "s1_date_list = s1filt.map(get_date).aggregate_array('date').getInfo()\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "sentinel1_dates_dt = [datetime.strptime(date, '%Y-%m-%d') for date in s1_date_list]\n",
    "sentinel2_dates_dt = [datetime.strptime(date, '%Y-%m-%d') for date in s2_date_list]\n",
    "\n",
    "# Find the closest Sentinel-1 dates for each Sentinel-2 date\n",
    "matched_dates = find_closest_dates(sentinel2_dates_dt, sentinel1_dates_dt)\n",
    "\n",
    "matched_collection = match_collections_by_dates(land_images, s1filt, matched_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample land pixels from each image in the collection\n",
    "def sample_land_pixels(image):\n",
    "    # Sample the pixels for all bands, and include the 'land_label' as a property\n",
    "    sampled_pixels = image.sample(\n",
    "        # region=roi,                   # Specify the region of interest\n",
    "        scale=10,                     # Resolution of the data\n",
    "        numPixels=1000,               # Adjust for how many samples you want\n",
    "        geometries=True               # Include geometries for spatial data if necessary\n",
    "    )\n",
    "    \n",
    "    return sampled_pixels\n",
    "\n",
    "# Map the function over the entire collection\n",
    "sampled_land_collection = matched_collection.map(sample_land_pixels)\n",
    "\n",
    "# Flatten the collection into a single FeatureCollection (one table for all images)\n",
    "all_sampled_pixels = sampled_land_collection.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to label Sentinel-1 pixels based on the RGB mask\n",
    "def label_pixels(image):\n",
    "    # Use the mask from the RGB image to create the land/water label\n",
    "    land_mask = image.select(['B8']).mask().reduce(ee.Reducer.min())\n",
    "\n",
    "    # Label the Sentinel-1 pixels: 1 for land, 0 for water\n",
    "    labeled_backscatter = image.select('VV').where(land_mask.eq(1), 1).where(land_mask.eq(0), 0).rename('label')\n",
    "    # sar_wlm.append(labeled_backscatter.set('system:time_start', match_sar.get('system:time_start')))\n",
    "\n",
    "    # Return the labeled Sentinel-1 image\n",
    "    # return labeled_backscatter.set('system:time_start', match_sar.get('system:time_start'))\n",
    "    # return ee.ImageCollection(sar_wlm)\n",
    "    return image.addBands(labeled_backscatter)\n",
    "\n",
    "# Map the labeling function over the Sentinel-1 collection\n",
    "labeled_collection = matched_collection.map(label_pixels).select('VV', 'VH', 'label')\n",
    "\n",
    "# Print or display a few of the labeled images to verify\n",
    "print(labeled_collection.size().getInfo())  # Prints the number of images processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample from each image in the collection\n",
    "def sample_image(image):\n",
    "    return image.sample(\n",
    "        region=aoi,         # Define your area of interest\n",
    "        scale=10,            # Set the scale (resolution) to sample from\n",
    "        numPixels=5000,      # Set the number of pixels to sample (adjust based on your data)\n",
    "        geometries=True      # Set to True if you want to keep the pixel's geometry\n",
    "    )\n",
    "\n",
    "# Use map() to sample each image in the collection\n",
    "sampled_images = labeled_collection.map(sample_image)\n",
    "training_data = sampled_images.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "classifier = ee.Classifier.smileRandomForest(numberOfTrees=10).train(\n",
    "    features=training_data,\n",
    "    classProperty='label',\n",
    "    inputProperties=['VV', 'VH']  # Adjust based on your feature bands\n",
    ")\n",
    "\n",
    "def classify_ims(image):\n",
    "    label = image.classify(classifier).rename('land_label')\n",
    "    return image.addBands([label])\n",
    "\n",
    "# Apply the classifier to an image (or image collection)\n",
    "classified_images = s1filt.map(classify_ims).select(['land_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maptest = geemap.Map()\n",
    "maptest.centerObject(aoi, 12)\n",
    "\n",
    "class_ims = classified_images.toList(classified_images.size())\n",
    "\n",
    "for i in range(classified_images.size().getInfo()//60):\n",
    "    image = ee.Image(class_ims.get(-i))\n",
    "    maptest.addLayer(image.select('land_label'), {'min': 0, 'max': 1, 'palette': ['blue', 'green']}, f'Labeled Sentinel-1 {i}')\n",
    "    # maptest.addLayer(image.select('label'), {'min': 0, 'max': 1, 'palette': ['blue', 'green']}, f'test{i}')\n",
    "    rgbims = total_colls['s2_10m'].toList(total_colls['s2_10m'].size())\n",
    "    add_rgb_to_map(ee.Image(rgbims.get(-i)), maptest)\n",
    "\n",
    "    # add_sar_to_map(image, maptest, 'VV')\n",
    "    # add_sar_to_map(image, maptest, 'VH')\n",
    "\n",
    "# Display the maptest.\n",
    "maptest.addLayerControl(position = 'topright')\n",
    "maptest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "1. Include VV/VH ratio in supervised classification training\n",
    "2. Include GLCM texture features in supervised classification training\n",
    "3. Time series of area change for Creation Sites using Sentinel-1, compare with Sentinel-2 time series\n",
    "4. Export imagery?\n",
    "\n",
    "May need to export the s1 and s2 (rgb, ndvi, ndwi, masvi2, bsi) images for creating a more robust model for classification, similar to Pena et el 2024.\n",
    "\n",
    "- time series of area change from pixel change detection for objects\n",
    "- examine the ndwi, ndvi, msavi2, vv amplitude, and vh amplitude in the objects as they change throughout time, compare with area changes?\n",
    "\n",
    "\n",
    "Once able to spatially estimate shoreline/area changes, use those to estimate area changes for the marsh creation sites. (Once the area is estiamted, the volumetric changes can be estimated using closure phase corrected InSAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
