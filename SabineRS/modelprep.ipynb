{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import rasterio\n",
    "from rasterio import windows\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dask\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labelpath):\n",
    "    otsu_ims = [os.path.join(labelpath, f'otsu/{file}') for file in os.listdir(os.path.join(labelpath, f'otsu')) if file.endswith('.tif')]\n",
    "    kmeans_ims = [os.path.join(labelpath, f'kmeans/{file}') for file in os.listdir(os.path.join(labelpath, f'kmeans')) if file.endswith('.tif')]\n",
    "    gmm_ims = [os.path.join(labelpath, f'gmm/{file}') for file in os.listdir(os.path.join(labelpath, f'gmm')) if file.endswith('.tif')]\n",
    "    majority_ims = [os.path.join(labelpath, f'majority/{file}') for file in os.listdir(os.path.join(labelpath, f'majority')) if file.endswith('.tif')]\n",
    "\n",
    "    \n",
    "    otsu_ims = sorted(otsu_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    kmeans_ims = sorted(kmeans_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    gmm_ims = sorted(gmm_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    majority_ims = sorted(majority_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return otsu_ims, kmeans_ims, gmm_ims, majority_ims\n",
    "\n",
    "def get_grd(grdpath):\n",
    "    orig_ims = [os.path.join(grdpath, file) for file in os.listdir(grdpath) if file.endswith('.tif')]\n",
    "    orig_ims = sorted(orig_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return orig_ims\n",
    "\n",
    "def get_glcm(glcmpath):\n",
    "    orig_glcms = [os.path.join(glcmpath, file) for file in os.listdir(glcmpath) if file.endswith('.tif')]\n",
    "    orig_glcms = sorted(orig_glcms, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return orig_glcms\n",
    "def find_closest_dates(labels, backscatter_ims, glcm_ims, max_days=12):\n",
    "    closest_dates = []  # To store the closest matches for each label\n",
    "\n",
    "    # Iterate through each label\n",
    "    for label in labels:\n",
    "        label_date = datetime.strptime(label[-14:-4], '%Y-%m-%d')  # Extract date from label\n",
    "        min_diff = max_days + 1  # Initialize minimum difference as larger than max_days\n",
    "        closest_backscatter = None  # To store the closest backscatter match\n",
    "        closest_glcm = None  # To store the closest GLCM match\n",
    "\n",
    "        # Iterate through both backscatter and GLCM images\n",
    "        for backscatter, glcm in zip(backscatter_ims, glcm_ims):\n",
    "            backscatter_date = datetime.strptime(backscatter[-14:-4], '%Y-%m-%d')  # Extract date from backscatter\n",
    "            glcm_date = datetime.strptime(glcm[-14:-4], '%Y-%m-%d')  # Extract date from GLCM\n",
    "\n",
    "            # Calculate the absolute difference in days\n",
    "            day_difference = abs((backscatter_date - label_date).days)\n",
    "\n",
    "            # Check if the difference is within max_days and closer than the current minimum\n",
    "            if day_difference <= max_days and day_difference < min_diff:\n",
    "                min_diff = day_difference\n",
    "                closest_backscatter = backscatter\n",
    "                closest_glcm = glcm\n",
    "\n",
    "        # Store the closest matches for the current label\n",
    "        closest_dates.append((label, closest_backscatter, closest_glcm))\n",
    "\n",
    "    return closest_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Imagery for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### WSL #########################\n",
    "# labels = get_labels('/mnt/d/SabineRS/s2classifications')\n",
    "# backscatter_ims = get_grd('/mnt/d/SabineRS/GRD/3_ratio')\n",
    "# glcm_ims = get_glcm('/mnt/d/SabineRS/GRD/2_registered/glcm')\n",
    "\n",
    "###################### Linux #########################\n",
    "otsu_ims, kmeans_ims, gmm_ims, majority_ims = get_labels('/home/wcc/Desktop/SabineRS/MSI/s2classifications')\n",
    "backscatter_ims = get_grd('/home/wcc/Desktop/SabineRS/GRD/3_ratio')\n",
    "glcm_ims = get_glcm('/home/wcc/Desktop/SabineRS/GRD/2_registered/glcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the Sentinel-1 backscatter and glcm  with labels according to date\n",
    "labeledPairs = find_closest_dates(majority_ims, backscatter_ims, glcm_ims)\n",
    "\n",
    "# Filter out tuples that contain any None entries\n",
    "# no close matches between S2 labels and S1 images\n",
    "filtered_data = [entry[:2] for entry in labeledPairs if None not in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1matches = [set[1] for set in filtered_data]\n",
    "s1_X = [i for i in backscatter_ims if i not in s1matches]   # unlabeled S1 data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for set in filtered_data:\n",
    "    with rasterio.open(set[1]) as src:\n",
    "        vv = src.read(1).astype(np.float32)\n",
    "        vh = src.read(2).astype(np.float32)\n",
    "        rvi = src.read(3).astype(np.float32)\n",
    "        sdwi = src.read(4).astype(np.float32)\n",
    "\n",
    "        # Convert from dB to linear scale\n",
    "        vv_linear = 10 ** (vv / 10)\n",
    "        vh_linear = 10 ** (vh / 10)\n",
    "\n",
    "        vv_lin_norm = (vv_linear - vv_linear.min()) / (vv_linear.max() - vv_linear.min())\n",
    "        vh_lin_norm = (vh_linear - vh_linear.min()) / (vh_linear.max() - vh_linear.min())\n",
    "        rvi_norm = (rvi - rvi.min()) / (rvi.max() - rvi.min())\n",
    "        sdwi_norm = (sdwi - sdwi.min()) / (sdwi.max() - sdwi.min())\n",
    "\n",
    "        s1_data = np.stack([vv_lin_norm, vh_lin_norm, rvi_norm, sdwi_norm], axis=-1)\n",
    "\n",
    "    with rasterio.open(set[0]) as src:\n",
    "        s2_labels = src.read(1).astype(np.int32)\n",
    "    \n",
    "    X_train.append(s1_data)\n",
    "    y_train.append(s2_labels)\n",
    "\n",
    "\n",
    "X_unlabeled = []\n",
    "\n",
    "for im in s1_X:\n",
    "    with rasterio.open(im) as src:\n",
    "        vv = src.read(1).astype(np.float32)\n",
    "        vh = src.read(2).astype(np.float32)\n",
    "        rvi = src.read(3).astype(np.float32)\n",
    "        sdwi = src.read(4).astype(np.float32)\n",
    "\n",
    "        # Convert from dB to linear scale\n",
    "        vv_linear = 10 ** (vv / 10)\n",
    "        vh_linear = 10 ** (vh / 10)\n",
    "\n",
    "        vv_lin_norm = (vv_linear - vv_linear.min()) / (vv_linear.max() - vv_linear.min())\n",
    "        vh_lin_norm = (vh_linear - vh_linear.min()) / (vh_linear.max() - vh_linear.min())\n",
    "        rvi_norm = (rvi - rvi.min()) / (rvi.max() - rvi.min())\n",
    "        sdwi_norm = (sdwi - sdwi.min()) / (sdwi.max() - sdwi.min())\n",
    "\n",
    "        s1_unlab_data = np.stack([vv_lin_norm, vh_lin_norm, rvi_norm, sdwi_norm], axis=-1)\n",
    "\n",
    "    X_unlabeled.append(s1_unlab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each stacked image and its labels for RF training\n",
    "X_train_flattened = np.vstack([img.reshape(-1, 4) for img in X_train])  # Shape: (total_pixels, 4)\n",
    "y_train_flattened = np.hstack([label.flatten() for label in y_train])  # Shape: (total_pixels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_unlabeled to a numpy array for compatibility with all models\n",
    "X_unlabeled = np.array(X_unlabeled)\n",
    "X_unlabeled_flattened = np.vstack([img.reshape(-1, X_unlabeled.shape[-1]) for img in X_unlabeled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classifiers on small dataset to test feasibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask Client (optional, depending on setup)\n",
    "client = Client()  # This will use your local resources or a Dask cluster if configured\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_flattened, y_train_flattened, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual model training and evaluation functions\n",
    "@dask.delayed\n",
    "def train_rf(X_train, y_train, X_val, y_val, X_unlabeled):\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_val_pred_rf = rf_model.predict(X_val)\n",
    "    rf_unlabeled_pred = rf_model.predict(X_unlabeled_flattened).reshape(X_unlabeled.shape[:2])\n",
    "    \n",
    "    print(\"Random Forest Results:\")\n",
    "    print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred_rf))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred_rf))\n",
    "    print(\"\\nRandom Forest Predicted Labels for Unlabeled Data:\", rf_unlabeled_pred)\n",
    "    \n",
    "    return rf_unlabeled_pred\n",
    "\n",
    "@dask.delayed\n",
    "def train_xgb(X_train, y_train, X_val, y_val, X_unlabeled):\n",
    "    xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "    xgb_unlabeled_pred = xgb_model.predict(X_unlabeled_flattened).reshape(X_unlabeled.shape[:2])\n",
    "    \n",
    "    print(\"\\nXGBoost Results:\")\n",
    "    print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred_xgb))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred_xgb))\n",
    "    print(\"\\nXGBoost Predicted Labels for Unlabeled Data:\", xgb_unlabeled_pred)\n",
    "    \n",
    "    return xgb_unlabeled_pred\n",
    "\n",
    "@dask.delayed\n",
    "def train_mlp(X_train, y_train, X_val, y_val, X_unlabeled):\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=100, random_state=42)\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    y_val_pred_mlp = mlp_model.predict(X_val)\n",
    "    mlp_unlabeled_pred = mlp_model.predict(X_unlabeled_flattened).reshape(X_unlabeled.shape[:2])\n",
    "    \n",
    "    print(\"\\nMLP Results:\")\n",
    "    print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred_mlp))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred_mlp))\n",
    "    print(\"\\nMLP Predicted Labels for Unlabeled Data:\", mlp_unlabeled_pred)\n",
    "    \n",
    "    return mlp_unlabeled_pred\n",
    "\n",
    "@dask.delayed\n",
    "def train_svm(X_train, y_train, X_val, y_val, X_unlabeled):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    svm_model = NuSVC(random_state=42)\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    y_val_pred_svm = svm_model.predict(X_val_scaled)\n",
    "    svm_unlabeled_pred = svm_model.predict(scaler.transform(X_unlabeled_flattened)).reshape(X_unlabeled.shape[:2])\n",
    "    \n",
    "    print(\"SVM Results:\")\n",
    "    print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred_svm))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred_svm))\n",
    "    \n",
    "    return svm_unlabeled_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcc/tools/miniforge/envs/gis/lib/python3.11/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 2.07 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run models in parallel\n",
    "rf_future = train_rf(X_train_split, y_train_split, X_val_split, y_val_split, X_unlabeled)\n",
    "xgb_future = train_xgb(X_train_split, y_train_split, X_val_split, y_val_split, X_unlabeled)\n",
    "mlp_future = train_mlp(X_train_split, y_train_split, X_val_split, y_val_split, X_unlabeled)\n",
    "svm_future = train_svm(X_train_split, y_train_split, X_val_split, y_val_split, X_unlabeled)\n",
    "\n",
    "# Trigger computations for all models\n",
    "results = dask.compute(rf_future, xgb_future, mlp_future, svm_future)\n",
    "\n",
    "# Optionally, close client when done\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to visualize some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume rf_unlabeled_pred, xgb_unlabeled_pred, and mlp_unlabeled_pred \n",
    "# are the predictions reshaped to (height, width)\n",
    "\n",
    "# Example shapes (adjust according to your data)\n",
    "height, width = rf_unlabeled_pred.shape\n",
    "\n",
    "# Set up a single figure with three subplots in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Visualize the Random Forest predictions\n",
    "axes[0].imshow(rf_unlabeled_pred[0], cmap='viridis', vmin=0, vmax=2)  # Adjust vmin/vmax based on your class labels\n",
    "axes[0].set_title(\"Random Forest Predictions\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Visualize the XGBoost predictions\n",
    "axes[1].imshow(xgb_unlabeled_pred[0], cmap='viridis', vmin=0, vmax=2)\n",
    "axes[1].set_title(\"XGBoost Predictions\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Visualize the MLP predictions\n",
    "axes[2].imshow(mlp_unlabeled_pred[0], cmap='viridis', vmin=0, vmax=2)\n",
    "axes[2].set_title(\"MLP Predictions\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Add a color bar to indicate classes (optional)\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax=axes, orientation='vertical', shrink=0.6, aspect=10)\n",
    "cbar.set_label('Class Label', rotation=270, labelpad=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Morphological Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphological operators if needed\n",
    "\n",
    "cleaned_ims = {\"otsu\": [],\n",
    "               \"kmeans\": [], \n",
    "               \"gmm\": []\n",
    "               }\n",
    "\n",
    "for i, (method, entry) in enumerate(zip(classification_methods, [relabeled_images['otsu'], relabeled_images[\"kmeans\"], relabeled_images['gmm']])):\n",
    "    for j, im in enumerate(entry):\n",
    "        # Define a square kernel; adjust the size as needed\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "\n",
    "        # apply morphological functions to eliminate isolated pixels from each class\n",
    "        subaqueous = (im == 0).astype(np.uint8)\n",
    "        subaerial = (im == 1).astype(np.uint8)\n",
    "\n",
    "        ######## KMeans\n",
    "        # Apply opening to remove small isolated pixels\n",
    "        subaerial_cleaned = cv2.morphologyEx(subaerial, cv2.MORPH_OPEN, kernel)\n",
    "        subaqueous_cleaned = cv2.morphologyEx(subaqueous, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Apply closing to fill small holes\n",
    "        subaerial_cleaned = cv2.morphologyEx(subaerial_cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "        subaqueous_cleaned = cv2.morphologyEx(subaqueous_cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Reconstruct the classified image\n",
    "        cleaned_classified_image = (subaqueous_cleaned * i +\n",
    "                                    subaqueous_cleaned * 1)      \n",
    "\n",
    "        # Add the processed relabeled image to the dictionary\n",
    "        cleaned_ims[method].append(cleaned_classified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check resulting classes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truthing\n",
    "- get water extent maps from various sources to serve as ground truth data for confirming the classification results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://global-surface-water.appspot.com/download\n",
    "2. USGS LandCover\n",
    "3. Copernicus Water and Wetness Product?\n",
    "4. Chesapeake Conservancy High-Resolution Land Cover Dataset\n",
    "5. RAMSAR Wetlands Sites\n",
    "6. MODIS Land Cover Type Product (MCD12Q1)\n",
    "7. Sentinel-2 Labeled Datasets for Wetland Classification\n",
    "8. OSM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
