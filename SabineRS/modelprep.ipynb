{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import rasterio\n",
    "from rasterio import windows\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labelpath):\n",
    "    otsu_ims = [os.path.join(labelpath, f'otsu/{file}') for file in os.listdir(os.path.join(labelpath, f'otsu')) if file.endswith('.tif')]\n",
    "    kmeans_ims = [os.path.join(labelpath, f'kmeans/{file}') for file in os.listdir(os.path.join(labelpath, f'kmeans')) if file.endswith('.tif')]\n",
    "    gmm_ims = [os.path.join(labelpath, f'gmm/{file}') for file in os.listdir(os.path.join(labelpath, f'gmm')) if file.endswith('.tif')]\n",
    "    majority_ims = [os.path.join(labelpath, f'majority/{file}') for file in os.listdir(os.path.join(labelpath, f'majority')) if file.endswith('.tif')]\n",
    "\n",
    "    \n",
    "    otsu_ims = sorted(otsu_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    kmeans_ims = sorted(kmeans_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    gmm_ims = sorted(gmm_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    majority_ims = sorted(majority_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return otsu_ims, kmeans_ims, gmm_ims, majority_ims\n",
    "\n",
    "def get_grd(grdpath):\n",
    "    orig_ims = [os.path.join(grdpath, file) for file in os.listdir(grdpath) if file.endswith('.tif')]\n",
    "    orig_ims = sorted(orig_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return orig_ims\n",
    "\n",
    "def get_glcm(glcmpath):\n",
    "    orig_glcms = [os.path.join(glcmpath, file) for file in os.listdir(glcmpath) if file.endswith('.tif')]\n",
    "    orig_glcms = sorted(orig_glcms, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return orig_glcms\n",
    "def find_closest_dates(labels, backscatter_ims, glcm_ims, max_days=12):\n",
    "    closest_dates = []  # To store the closest matches for each label\n",
    "\n",
    "    # Iterate through each label\n",
    "    for label in labels:\n",
    "        label_date = datetime.strptime(label[-14:-4], '%Y-%m-%d')  # Extract date from label\n",
    "        min_diff = max_days + 1  # Initialize minimum difference as larger than max_days\n",
    "        closest_backscatter = None  # To store the closest backscatter match\n",
    "        closest_glcm = None  # To store the closest GLCM match\n",
    "\n",
    "        # Iterate through both backscatter and GLCM images\n",
    "        for backscatter, glcm in zip(backscatter_ims, glcm_ims):\n",
    "            backscatter_date = datetime.strptime(backscatter[-14:-4], '%Y-%m-%d')  # Extract date from backscatter\n",
    "            glcm_date = datetime.strptime(glcm[-14:-4], '%Y-%m-%d')  # Extract date from GLCM\n",
    "\n",
    "            # Calculate the absolute difference in days\n",
    "            day_difference = abs((backscatter_date - label_date).days)\n",
    "\n",
    "            # Check if the difference is within max_days and closer than the current minimum\n",
    "            if day_difference <= max_days and day_difference < min_diff:\n",
    "                min_diff = day_difference\n",
    "                closest_backscatter = backscatter\n",
    "                closest_glcm = glcm\n",
    "\n",
    "        # Store the closest matches for the current label\n",
    "        closest_dates.append((label, closest_backscatter, closest_glcm))\n",
    "\n",
    "    return closest_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Imagery for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### WSL #########################\n",
    "# labels = get_labels('/mnt/d/SabineRS/s2classifications')\n",
    "# backscatter_ims = get_grd('/mnt/d/SabineRS/GRD/3_ratio')\n",
    "# glcm_ims = get_glcm('/mnt/d/SabineRS/GRD/2_registered/glcm')\n",
    "\n",
    "###################### Linux #########################\n",
    "otsu_ims, kmeans_ims, gmm_ims, majority_ims = get_labels('/home/wcc/Desktop/SabineRS/MSI/s2classifications')\n",
    "backscatter_ims = get_grd('/home/wcc/Desktop/SabineRS/GRD/3_ratio')\n",
    "glcm_ims = get_glcm('/home/wcc/Desktop/SabineRS/GRD/2_registered/glcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the Sentinel-1 backscatter and glcm  with labels according to date\n",
    "labeledPairs = find_closest_dates(majority_ims, backscatter_ims, glcm_ims)\n",
    "\n",
    "# Filter out tuples that contain any None entries\n",
    "# no close matches between S2 labels and S1 images\n",
    "filtered_data = [entry[:2] for entry in labeledPairs if None not in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1matches = [set[1] for set in filtered_data]\n",
    "s1_X = [i for i in backscatter_ims if i not in s1matches]   # unlabeled S1 data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_pair(s2_path, s1_path):\n",
    "    # Load Sentinel-2 labels\n",
    "    with rasterio.open(s2_path) as src:\n",
    "        s2_labels = src.read(1).astype(np.int32)  # Assume the labels are in the first band\n",
    "\n",
    "    # Load Sentinel-1 four-band image\n",
    "    with rasterio.open(s1_path) as src:\n",
    "        s1_data = np.stack([src.read(i).astype(np.float32) for i in range(1, 5)], axis=-1)\n",
    "\n",
    "    return s2_labels, s1_data\n",
    "\n",
    "def dynamic_tile_image(image, overlap=0.2):\n",
    "    # Calculate tile size based on half the height and width\n",
    "    tile_height = image.shape[0] // 2\n",
    "    tile_width = image.shape[1] // 2\n",
    "    tile_size = (tile_height, tile_width)\n",
    "    \n",
    "    overlap_pixels_x = int(tile_width * overlap)\n",
    "    overlap_pixels_y = int(tile_height * overlap)\n",
    "    \n",
    "    stride_x = tile_width - overlap_pixels_x\n",
    "    stride_y = tile_height - overlap_pixels_y\n",
    "\n",
    "    tiles = []\n",
    "    \n",
    "    for y in range(0, image.shape[0] - tile_height + 1, stride_y):\n",
    "        for x in range(0, image.shape[1] - tile_width + 1, stride_x):\n",
    "            tile = image[y:y + tile_height, x:x + tile_width]\n",
    "            if tile.shape[:2] == (tile_height, tile_width):\n",
    "                tiles.append(tile)\n",
    "\n",
    "    return tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_unlabeled = []\n",
    "\n",
    "for set in filtered_data:\n",
    "    with rasterio.open(set[1]) as src:\n",
    "        vv = src.read(1).astype(np.float32)\n",
    "        vh = src.read(2).astype(np.float32)\n",
    "        rvi = src.read(3).astype(np.float32)\n",
    "        sdwi = src.read(4).astype(np.float32)\n",
    "\n",
    "        # Convert from dB to linear scale\n",
    "        vv_linear = 10 ** (vv / 10)\n",
    "        vh_linear = 10 ** (vh / 10)\n",
    "\n",
    "        vv_lin_norm = (vv_linear - vv_linear.min()) / (vv_linear.max() - vv_linear.min())\n",
    "        vh_lin_norm = (vh_linear - vh_linear.min()) / (vh_linear.max() - vh_linear.min())\n",
    "        rvi_norm = (rvi - rvi.min()) / (rvi.max() - rvi.min())\n",
    "        sdwi_norm = (sdwi - sdwi.min()) / (sdwi.max() - sdwi.min())\n",
    "\n",
    "        s1_data = np.stack([vv_lin_norm, vh_lin_norm, rvi_norm, sdwi_norm], axis=-1)\n",
    "\n",
    "    with rasterio.open(set[0]) as src:\n",
    "        s2_labels = src.read(1).astype(np.int32)\n",
    "    \n",
    "    # s1_lab_tiles = dynamic_tile_image(s1_data, overlap=0.2)\n",
    "    X_train.append(s1_data)\n",
    "    # s2_tiles = dynamic_tile_image(s2_labels, overlap=0.2)\n",
    "    y_train.append(s2_labels)\n",
    "\n",
    "for im in s1_X:\n",
    "    with rasterio.open(im) as src:\n",
    "        vv = src.read(1).astype(np.float32)\n",
    "        vh = src.read(2).astype(np.float32)\n",
    "        rvi = src.read(3).astype(np.float32)\n",
    "        sdwi = src.read(4).astype(np.float32)\n",
    "\n",
    "        # Convert from dB to linear scale\n",
    "        vv_linear = 10 ** (vv / 10)\n",
    "        vh_linear = 10 ** (vh / 10)\n",
    "\n",
    "        vv_lin_norm = (vv_linear - vv_linear.min()) / (vv_linear.max() - vv_linear.min())\n",
    "        vh_lin_norm = (vh_linear - vh_linear.min()) / (vh_linear.max() - vh_linear.min())\n",
    "        rvi_norm = (rvi - rvi.min()) / (rvi.max() - rvi.min())\n",
    "        sdwi_norm = (sdwi - sdwi.min()) / (sdwi.max() - sdwi.min())\n",
    "\n",
    "        s1_unlab_data = np.stack([vv_lin_norm, vh_lin_norm, rvi_norm, sdwi_norm], axis=-1)\n",
    "\n",
    "    # Tile images with dynamically calculated size\n",
    "    # s1_unlab_tiles = dynamic_tile_image(s1_X_data, overlap = 0.2)\n",
    "    X_unlabeled.append(s1_unlab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each stacked image and its labels for RF training\n",
    "X_train_flattened = np.vstack([img.reshape(-1, 4) for img in X_train])  # Shape: (total_pixels, 4)\n",
    "y_train_flattened = np.hstack([label.flatten() for label in y_train])  # Shape: (total_pixels,)\n",
    "X_unlabeled_flattened = np.hstack([img.reshape(-1, 4) for img in X_unlabeled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classifiers on small dataset to test feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and validation sets (e.g., 80% train, 20% validation)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_flattened, y_train_flattened, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=100, random_state=42)\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "y_val_pred_rf = rf_model.predict(X_val_split)\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val_split, y_val_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_split, y_val_pred_rf))\n",
    "\n",
    "# Predict on unlabeled data using RF\n",
    "rf_unlabeled_pred = rf_model.predict(X_unlabeled.reshape(-1, X_unlabeled.shape[-1])).reshape(X_unlabeled.shape[:2])\n",
    "print(\"\\nRandom Forest Predicted Labels for Unlabeled Data:\", rf_unlabeled_pred)\n",
    "\n",
    "# Train and evaluate XGBoost model\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val_split)\n",
    "print(\"\\nXGBoost Results:\")\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val_split, y_val_pred_xgb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_split, y_val_pred_xgb))\n",
    "\n",
    "# Predict on unlabeled data using XGBoost\n",
    "xgb_unlabeled_pred = xgb_model.predict(X_unlabeled.reshape(-1, X_unlabeled.shape[-1])).reshape(X_unlabeled.shape[:2])\n",
    "print(\"\\nXGBoost Predicted Labels for Unlabeled Data:\", xgb_unlabeled_pred)\n",
    "\n",
    "# Train and evaluate MLP model\n",
    "mlp_model.fit(X_train_split, y_train_split)\n",
    "y_val_pred_mlp = mlp_model.predict(X_val_split)\n",
    "print(\"\\nMLP Results:\")\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val_split, y_val_pred_mlp))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_split, y_val_pred_mlp))\n",
    "\n",
    "# Predict on unlabeled data using MLP\n",
    "mlp_unlabeled_pred = mlp_model.predict(X_unlabeled.reshape(-1, X_unlabeled.shape[-1])).reshape(X_unlabeled.shape[:2])\n",
    "print(\"\\nMLP Predicted Labels for Unlabeled Data:\", mlp_unlabeled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assume rf_unlabeled_pred, xgb_unlabeled_pred, and mlp_unlabeled_pred \n",
    "# are the predictions reshaped to (height, width)\n",
    "\n",
    "# Example shapes (adjust according to your data)\n",
    "height, width = rf_unlabeled_pred.shape\n",
    "\n",
    "# Set up a single figure with three subplots in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Visualize the Random Forest predictions\n",
    "axes[0].imshow(rf_unlabeled_pred[0], cmap='viridis', vmin=0, vmax=2)  # Adjust vmin/vmax based on your class labels\n",
    "axes[0].set_title(\"Random Forest Predictions\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Visualize the XGBoost predictions\n",
    "axes[1].imshow(xgb_unlabeled_pred[0], cmap='viridis', vmin=0, vmax=2)\n",
    "axes[1].set_title(\"XGBoost Predictions\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Visualize the MLP predictions\n",
    "axes[2].imshow(mlp_unlabeled_pred[0], cmap='viridis', vmin=0, vmax=2)\n",
    "axes[2].set_title(\"MLP Predictions\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Add a color bar to indicate classes (optional)\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax=axes, orientation='vertical', shrink=0.6, aspect=10)\n",
    "cbar.set_label('Class Label', rotation=270, labelpad=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphological operators if needed\n",
    "\n",
    "cleaned_ims = {\"otsu\": [],\n",
    "               \"kmeans\": [], \n",
    "               \"gmm\": []\n",
    "               }\n",
    "\n",
    "for i, (method, entry) in enumerate(zip(classification_methods, [relabeled_images['otsu'], relabeled_images[\"kmeans\"], relabeled_images['gmm']])):\n",
    "    for j, im in enumerate(entry):\n",
    "        # Define a square kernel; adjust the size as needed\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "\n",
    "        # apply morphological functions to eliminate isolated pixels from each class\n",
    "        subaqueous = (im == 0).astype(np.uint8)\n",
    "        subaerial = (im == 1).astype(np.uint8)\n",
    "\n",
    "        ######## KMeans\n",
    "        # Apply opening to remove small isolated pixels\n",
    "        subaerial_cleaned = cv2.morphologyEx(subaerial, cv2.MORPH_OPEN, kernel)\n",
    "        subaqueous_cleaned = cv2.morphologyEx(subaqueous, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Apply closing to fill small holes\n",
    "        subaerial_cleaned = cv2.morphologyEx(subaerial_cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "        subaqueous_cleaned = cv2.morphologyEx(subaqueous_cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Reconstruct the classified image\n",
    "        cleaned_classified_image = (subaqueous_cleaned * i +\n",
    "                                    subaqueous_cleaned * 1)      \n",
    "\n",
    "        # Add the processed relabeled image to the dictionary\n",
    "        cleaned_ims[method].append(cleaned_classified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train NN on larger dataset for many sites if good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_model(input_shape=(128, 128, 4), num_classes=3):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile and train\n",
    "model = build_model(input_shape=(128, 128, 4), num_classes=3)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `X_unlabeled` is your preprocessed and tiled unlabeled S1 data\n",
    "predictions = model.predict(X_unlabeled)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use if labels are integer-encoded\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are prepared (possibly by tiling the images)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate the accuracy of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truthing\n",
    "- get water extent maps from various sources to serve as ground truth data for confirming the classification results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://global-surface-water.appspot.com/download\n",
    "2. USGS LandCover\n",
    "3. Copernicus Water and Wetness Product?\n",
    "4. Chesapeake Conservancy High-Resolution Land Cover Dataset\n",
    "5. RAMSAR Wetlands Sites\n",
    "6. MODIS Land Cover Type Product (MCD12Q1)\n",
    "7. Sentinel-2 Labeled Datasets for Wetland Classification\n",
    "8. OSM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
