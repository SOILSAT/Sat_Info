{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 10:29:06.345996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-07 10:29:06.346581: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-07 10:29:06.348646: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-07 10:29:06.353942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730996946.362546    8099 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730996946.365063    8099 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 10:29:06.375231: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import rasterio\n",
    "from rasterio import windows\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, UpSampling2D, Cropping2D, MaxPooling2D, Dropout, BatchNormalization, Conv2DTranspose, concatenate, Flatten, Dense, UpSampling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labelpath):\n",
    "    otsu_ims = [os.path.join(labelpath, f'otsu/{file}') for file in os.listdir(os.path.join(labelpath, f'otsu')) if file.endswith('.tif')]\n",
    "    kmeans_ims = [os.path.join(labelpath, f'kmeans/{file}') for file in os.listdir(os.path.join(labelpath, f'kmeans')) if file.endswith('.tif')]\n",
    "    gmm_ims = [os.path.join(labelpath, f'gmm/{file}') for file in os.listdir(os.path.join(labelpath, f'gmm')) if file.endswith('.tif')]\n",
    "    majority_ims = [os.path.join(labelpath, f'majority/{file}') for file in os.listdir(os.path.join(labelpath, f'majority')) if file.endswith('.tif')]\n",
    "\n",
    "    \n",
    "    otsu_ims = sorted(otsu_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    kmeans_ims = sorted(kmeans_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    gmm_ims = sorted(gmm_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "    majority_ims = sorted(majority_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return otsu_ims, kmeans_ims, gmm_ims, majority_ims\n",
    "\n",
    "def get_grd(grdpath):\n",
    "    orig_ims = [os.path.join(grdpath, file) for file in os.listdir(grdpath) if file.endswith('.tif')]\n",
    "    orig_ims = sorted(orig_ims, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return orig_ims\n",
    "\n",
    "def get_glcm(glcmpath):\n",
    "    orig_glcms = [os.path.join(glcmpath, file) for file in os.listdir(glcmpath) if file.endswith('.tif')]\n",
    "    orig_glcms = sorted(orig_glcms, key=lambda x: datetime.strptime(x[-14:-4], '%Y-%m-%d'))\n",
    "\n",
    "    return orig_glcms\n",
    "def find_closest_dates(labels, backscatter_ims, glcm_ims, max_days=12):\n",
    "    closest_dates = []  # To store the closest matches for each label\n",
    "\n",
    "    # Iterate through each label\n",
    "    for label in labels:\n",
    "        label_date = datetime.strptime(label[-14:-4], '%Y-%m-%d')  # Extract date from label\n",
    "        min_diff = max_days + 1  # Initialize minimum difference as larger than max_days\n",
    "        closest_backscatter = None  # To store the closest backscatter match\n",
    "        closest_glcm = None  # To store the closest GLCM match\n",
    "\n",
    "        # Iterate through both backscatter and GLCM images\n",
    "        for backscatter, glcm in zip(backscatter_ims, glcm_ims):\n",
    "            backscatter_date = datetime.strptime(backscatter[-14:-4], '%Y-%m-%d')  # Extract date from backscatter\n",
    "            glcm_date = datetime.strptime(glcm[-14:-4], '%Y-%m-%d')  # Extract date from GLCM\n",
    "\n",
    "            # Calculate the absolute difference in days\n",
    "            day_difference = abs((backscatter_date - label_date).days)\n",
    "\n",
    "            # Check if the difference is within max_days and closer than the current minimum\n",
    "            if day_difference <= max_days and day_difference < min_diff:\n",
    "                min_diff = day_difference\n",
    "                closest_backscatter = backscatter\n",
    "                closest_glcm = glcm\n",
    "\n",
    "        # Store the closest matches for the current label\n",
    "        closest_dates.append((label, closest_backscatter, closest_glcm))\n",
    "\n",
    "    return closest_dates\n",
    "\n",
    "def stack_data(filtered_data, unlab_s1, unlab_glcm):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_unlabeled = []\n",
    "\n",
    "    for set in filtered_data:\n",
    "        with rasterio.open(set[2]) as glcm_src:\n",
    "            VV_contrast = glcm_src.read(1).astype(np.float32)\n",
    "            VV_asm = glcm_src.read(2).astype(np.float32)\n",
    "            VV_diss = glcm_src.read(3).astype(np.float32)\n",
    "            VV_idm = glcm_src.read(4).astype(np.float32)\n",
    "            VV_corr = glcm_src.read(5).astype(np.float32)\n",
    "            VV_var = glcm_src.read(6).astype(np.float32)\n",
    "            VV_ent = glcm_src.read(7).astype(np.float32)\n",
    "            VH_contrast = glcm_src.read(8).astype(np.float32)\n",
    "            VH_asm = glcm_src.read(9).astype(np.float32)\n",
    "            VH_diss = glcm_src.read(10).astype(np.float32)\n",
    "            VH_idm = glcm_src.read(11).astype(np.float32)\n",
    "            VH_corr = glcm_src.read(12).astype(np.float32)\n",
    "            VH_var = glcm_src.read(13).astype(np.float32)\n",
    "            VH_ent = glcm_src.read(14).astype(np.float32)\n",
    "\n",
    "\n",
    "            VV_contrast = (VV_contrast - VV_contrast.min()) / (VV_contrast.max() - VV_contrast.min())\n",
    "            VV_asm = (VV_asm- VV_asm.min()) / (VV_asm.max() - VV_asm.min())\n",
    "            VV_diss = (VV_diss - VV_diss.min()) / (VV_diss.max() - VV_diss.min())\n",
    "            VV_idm = (VV_idm - VV_idm.min()) / (VV_idm.max() - VV_idm.min())\n",
    "            VV_corr = (VV_corr - VV_corr.min()) / (VV_corr.max() - VV_corr.min())\n",
    "            VV_var = (VV_var - VV_var.min()) / (VV_var.max() - VV_var.min())\n",
    "            VV_ent = (VV_ent - VV_ent.min()) / (VV_ent.max() - VV_ent.min())\n",
    "            VH_contrast = (VH_contrast - VH_contrast.min()) / (VH_contrast.max() - VH_contrast.min())\n",
    "            VH_asm = (VH_asm- VH_asm.min()) / (VH_asm.max() - VH_asm.min())\n",
    "            VH_diss = (VH_diss - VH_diss.min()) / (VH_diss.max() - VH_diss.min())\n",
    "            VH_idm = (VH_idm - VH_idm.min()) / (VH_idm.max() - VH_idm.min())\n",
    "            VH_corr = (VH_corr - VH_corr.min()) / (VH_corr.max() - VH_corr.min())\n",
    "            VH_var = (VH_var - VH_var.min()) / (VH_var.max() - VH_var.min())\n",
    "            VH_ent = (VH_ent - VH_ent.min()) / (VH_ent.max() - VH_ent.min())\n",
    "    \n",
    "        with rasterio.open(set[1]) as src:\n",
    "            vv = src.read(1).astype(np.float32)\n",
    "            vh = src.read(2).astype(np.float32)\n",
    "            rvi = src.read(3).astype(np.float32)\n",
    "            sdwi = src.read(4).astype(np.float32)\n",
    "\n",
    "            # Convert from dB to linear scale\n",
    "            vv_linear = 10 ** (vv / 10)\n",
    "            vh_linear = 10 ** (vh / 10)\n",
    "\n",
    "            vv_lin_norm = (vv_linear - vv_linear.min()) / (vv_linear.max() - vv_linear.min())\n",
    "            vh_lin_norm = (vh_linear - vh_linear.min()) / (vh_linear.max() - vh_linear.min())\n",
    "            rvi_norm = (rvi - rvi.min()) / (rvi.max() - rvi.min())\n",
    "            sdwi_norm = (sdwi - sdwi.min()) / (sdwi.max() - sdwi.min())\n",
    "\n",
    "        s1_data = np.stack([vv_lin_norm, vh_lin_norm, rvi_norm, sdwi_norm, VV_contrast, VV_asm,VV_diss, VV_idm, VV_corr, VV_var, VV_ent, VH_contrast, VH_asm,VH_diss, VH_idm, VH_corr, VH_var, VH_ent], axis=-1)\n",
    "\n",
    "        with rasterio.open(set[0]) as src:\n",
    "            s2_labels = src.read(1).astype(np.int32)\n",
    "    \n",
    "        X_train.append(s1_data)\n",
    "        y_train.append(s2_labels)\n",
    "\n",
    "\n",
    "    X_unlabeled = []\n",
    "\n",
    "    for i, im in enumerate(unlab_s1):\n",
    "        with rasterio.open(im) as src:\n",
    "            vv = src.read(1).astype(np.float32)\n",
    "            vh = src.read(2).astype(np.float32)\n",
    "            rvi = src.read(3).astype(np.float32)\n",
    "            sdwi = src.read(4).astype(np.float32)\n",
    "\n",
    "            # Convert from dB to linear scale\n",
    "            vv_linear = 10 ** (vv / 10)\n",
    "            vh_linear = 10 ** (vh / 10)\n",
    "\n",
    "            vv_lin_norm = (vv_linear - vv_linear.min()) / (vv_linear.max() - vv_linear.min())\n",
    "            vh_lin_norm = (vh_linear - vh_linear.min()) / (vh_linear.max() - vh_linear.min())\n",
    "            rvi_norm = (rvi - rvi.min()) / (rvi.max() - rvi.min())\n",
    "            sdwi_norm = (sdwi - sdwi.min()) / (sdwi.max() - sdwi.min())\n",
    "\n",
    "        with rasterio.open(unlab_glcm[i]) as glcm_src:\n",
    "            VV_contrast = glcm_src.read(1).astype(np.float32)\n",
    "            VV_asm = glcm_src.read(2).astype(np.float32)\n",
    "            VV_diss = glcm_src.read(3).astype(np.float32)\n",
    "            VV_idm = glcm_src.read(4).astype(np.float32)\n",
    "            VV_corr = glcm_src.read(5).astype(np.float32)\n",
    "            VV_var = glcm_src.read(6).astype(np.float32)\n",
    "            VV_ent = glcm_src.read(7).astype(np.float32)\n",
    "            VH_contrast = glcm_src.read(8).astype(np.float32)\n",
    "            VH_asm = glcm_src.read(9).astype(np.float32)\n",
    "            VH_diss = glcm_src.read(10).astype(np.float32)\n",
    "            VH_idm = glcm_src.read(11).astype(np.float32)\n",
    "            VH_corr = glcm_src.read(12).astype(np.float32)\n",
    "            VH_var = glcm_src.read(13).astype(np.float32)\n",
    "            VH_ent = glcm_src.read(14).astype(np.float32)\n",
    "\n",
    "\n",
    "            VV_contrast = (VV_contrast - VV_contrast.min()) / (VV_contrast.max() - VV_contrast.min())\n",
    "            VV_asm = (VV_asm- VV_asm.min()) / (VV_asm.max() - VV_asm.min())\n",
    "            VV_diss = (VV_diss - VV_diss.min()) / (VV_diss.max() - VV_diss.min())\n",
    "            VV_idm = (VV_idm - VV_idm.min()) / (VV_idm.max() - VV_idm.min())\n",
    "            VV_corr = (VV_corr - VV_corr.min()) / (VV_corr.max() - VV_corr.min())\n",
    "            VV_var = (VV_var - VV_var.min()) / (VV_var.max() - VV_var.min())\n",
    "            VV_ent = (VV_ent - VV_ent.min()) / (VV_ent.max() - VV_ent.min())\n",
    "            VH_contrast = (VH_contrast - VH_contrast.min()) / (VH_contrast.max() - VH_contrast.min())\n",
    "            VH_asm = (VH_asm- VH_asm.min()) / (VH_asm.max() - VH_asm.min())\n",
    "            VH_diss = (VH_diss - VH_diss.min()) / (VH_diss.max() - VH_diss.min())\n",
    "            VH_idm = (VH_idm - VH_idm.min()) / (VH_idm.max() - VH_idm.min())\n",
    "            VH_corr = (VH_corr - VH_corr.min()) / (VH_corr.max() - VH_corr.min())\n",
    "            VH_var = (VH_var - VH_var.min()) / (VH_var.max() - VH_var.min())\n",
    "            VH_ent = (VH_ent - VH_ent.min()) / (VH_ent.max() - VH_ent.min())\n",
    "\n",
    "        s1_unlab_data = np.stack([vv_lin_norm, vh_lin_norm, rvi_norm, sdwi_norm, VV_contrast, VV_asm,VV_diss, VV_idm, VV_corr, VV_var, VV_ent, VH_contrast, VH_asm,VH_diss, VH_idm, VH_corr, VH_var, VH_ent], axis=-1)\n",
    "\n",
    "        X_unlabeled.append(s1_unlab_data)\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Imagery for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### WSL #########################\n",
    "# labels = get_labels('/mnt/d/SabineRS/s2classifications')\n",
    "# backscatter_ims = get_grd('/mnt/d/SabineRS/GRD/3_ratio')\n",
    "# glcm_ims = get_glcm('/mnt/d/SabineRS/GRD/2_registered/glcm')\n",
    "\n",
    "###################### Linux #########################\n",
    "otsu_ims, kmeans_ims, gmm_ims, majority_ims = get_labels('/home/wcc/Desktop/SabineRS/MSI/s2classifications')\n",
    "backscatter_ims = get_grd('/home/wcc/Desktop/SabineRS/GRD/3_ratio')\n",
    "glcm_ims = get_glcm('/home/wcc/Desktop/SabineRS/GRD/2_registered/glcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the Sentinel-1 backscatter and glcm  with labels according to date\n",
    "labeledPairs = find_closest_dates(majority_ims, backscatter_ims, glcm_ims)\n",
    "\n",
    "# Filter out tuples that contain any None entries\n",
    "# no close matches between S2 labels and S1 images\n",
    "filtered_data = [entry for entry in labeledPairs if None not in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1matches = [set[1] for set in filtered_data]\n",
    "glcmmatches = [set[2] for set in filtered_data]\n",
    "s1_X = [i for i in backscatter_ims if i not in s1matches]   # unlabeled S1 data for model training\n",
    "glcm_X = [i for i in glcm_ims if i not in glcmmatches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "\n",
    "labelpaths = []\n",
    "grdpaths = []\n",
    "glcmpaths = []\n",
    "\n",
    "unlabs1 = []\n",
    "unlabglcm = []\n",
    "\n",
    "for i, pair in enumerate(filtered_data):\n",
    "    labelpaths.append(filtered_data[0])\n",
    "    grdpaths.append(filtered_data[1])\n",
    "    glcmpaths.append(filtered_data[2])\n",
    "\n",
    "for j, im in enumerate(s1_X):\n",
    "    unlabs1.append(im)\n",
    "    unlabglcm.append(glcm_X[j])\n",
    "    \n",
    "labelseries=pd.Series(labelpaths, name='labelpaths')\n",
    "grdseries = pd.Series(grdpaths, name = 'grdpaths')\n",
    "glcmseries = pd.Series(glcmpaths, name='glcmpaths')\n",
    "train_df=pd.concat([labelseries, grdseries, glcmseries], axis=1)\n",
    "\n",
    "ugrdseries = pd.Series(unlabs1, name = 'grdpaths')\n",
    "uglcmseries = pd.Series(unlabglcm, name='glcmpaths')\n",
    "unlabel_df = pd.concat([ugrdseries, uglcmseries], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_unlabeled = stack_data(filtered_data, s1_X, glcm_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 18 # 4 bands from GRD and 14 from GLCM \n",
    "num_classes = 3\n",
    "img_height, img_width = X_train[0].shape[:2]  # Assuming all images have the same dimensions\n",
    "\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NN\n",
    "- name could be BUNet -- Beneficial Use Network given the focus on sediment enrichment in wetlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved model with batch normalization, dropout, and skip connections\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(798, 693, 18)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(3, (1, 1), activation='softmax', padding='same')  # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "hist = model.fit(\n",
    "    np.array(X_train_split),\n",
    "    np.array(y_train_split),\n",
    "    epochs=20,\n",
    "    batch_size=5,\n",
    "    validation_split=0.2,\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir = '/home/wcc/Desktop/SabineRS/modellogs')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color = 'red', label = 'loss')\n",
    "plt.plot(hist.history['val_loss'], color = 'blue', label = 'val_loss')\n",
    "plt.suptitle('CNN Training Loss', fontsize = 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color = 'red', label = 'accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color = 'blue', label = 'val_accuracy')\n",
    "plt.suptitle('CNN Training accuracy', fontsize = 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet\n",
    "- transfer learning from MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcc/tools/miniforge/envs/gis/lib/python3.11/site-packages/keras/src/applications/mobilenet.py:142: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 18 input channels.\n",
      "  input_shape = imagenet_utils.obtain_input_shape(\n"
     ]
    }
   ],
   "source": [
    "mobile = tf.keras.applications.mobilenet.MobileNet(\n",
    "    input_shape=np.array(X_train).shape[1:],\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    input_tensor=None,\n",
    "    weights=None,\n",
    "    pooling=None,\n",
    "    classes=3,\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcc/tools/miniforge/envs/gis/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (32, 798, 693, 18) (18 channels).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x=mobile.output\n",
    "# x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(3,activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "transfermodel = tf.keras.Model(inputs = mobile.input, outputs = preds)\n",
    "\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "train_generator=train_datagen.flow(\n",
    "    np.array(X_train_split),\n",
    "    np.array(y_train_split),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    sample_weight=None,\n",
    "    seed=None,\n",
    "    save_to_dir='/home/wcc/Desktop/SabineRS/modellogs',\n",
    "    ignore_class_split=False,\n",
    "    subset=None\n",
    ")\n",
    "\n",
    "# train_generator=train_datagen.flow('C:/Users/Ferhat/Python Code/Workshop/Tensoorflow transfer learning/downloads',\n",
    "#                                                  target_size=(224,224),\n",
    "#                                                  color_mode='rgb',\n",
    "#                                                  batch_size=32,\n",
    "#                                                  class_mode='categorical',\n",
    "#                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'fit_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Adam optimizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# loss function will be categorical cross entropy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# evaluation metric will be accuracy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m step_size_train\u001b[38;5;241m=\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtransfermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m(generator\u001b[38;5;241m=\u001b[39mtrain_generator,\n\u001b[1;32m      8\u001b[0m                    steps_per_epoch\u001b[38;5;241m=\u001b[39mstep_size_train,\n\u001b[1;32m      9\u001b[0m                    epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'fit_generator'"
     ]
    }
   ],
   "source": [
    "\n",
    "transfermodel.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "transfermodel.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet\n",
    "- transfer learngin from EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check accuracy, precision, recall, F1 scores\n",
    "- compare shallow NN with ResNet and UNet results, prove mine is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation data\n",
    "y_pred = model.predict(np.array(X_val_split))  # shape: (num_samples, height, width, num_classes)\n",
    "\n",
    "# Convert predictions to class labels by taking the argmax along the class dimension\n",
    "y_pred_labels = np.argmax(y_pred, axis=-1)  # shape: (num_samples, height, width)\n",
    "\n",
    "# Flatten the arrays for metric calculations\n",
    "y_val_flat = np.array(y_val_split).flatten()  # True labels\n",
    "y_pred_flat = y_pred_labels.flatten()  # Predicted labels\n",
    "\n",
    "# Calculate metrics for each class\n",
    "print(\"Accuracy:\", accuracy_score(y_val_flat, y_pred_flat))\n",
    "print(\"Precision, Recall, F1 Score per class:\\n\", classification_report(y_val_flat, y_pred_flat, target_names=['open water', 'subaqueous land', 'subaerial land']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample index to display (for example, the first sample in validation data)\n",
    "i = -1\n",
    "true_labels = y_val_split[i]          # True labels for the sample\n",
    "predicted_labels = y_pred_labels[i]  # Predicted labels for the sample\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Display the ground truth\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(true_labels, cmap='viridis')\n",
    "plt.title('Ground Truth')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the model predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_labels, cmap='viridis')\n",
    "plt.title('Predicted Labels')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Morphological Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphological operators if needed\n",
    "\n",
    "cleaned_ims = {\"otsu\": [],\n",
    "               \"kmeans\": [], \n",
    "               \"gmm\": []\n",
    "               }\n",
    "\n",
    "for i, (method, entry) in enumerate(zip(classification_methods, [relabeled_images['otsu'], relabeled_images[\"kmeans\"], relabeled_images['gmm']])):\n",
    "    for j, im in enumerate(entry):\n",
    "        # Define a square kernel; adjust the size as needed\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "\n",
    "        # apply morphological functions to eliminate isolated pixels from each class\n",
    "        subaqueous = (im == 0).astype(np.uint8)\n",
    "        subaerial = (im == 1).astype(np.uint8)\n",
    "\n",
    "        ######## KMeans\n",
    "        # Apply opening to remove small isolated pixels\n",
    "        subaerial_cleaned = cv2.morphologyEx(subaerial, cv2.MORPH_OPEN, kernel)\n",
    "        subaqueous_cleaned = cv2.morphologyEx(subaqueous, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Apply closing to fill small holes\n",
    "        subaerial_cleaned = cv2.morphologyEx(subaerial_cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "        subaqueous_cleaned = cv2.morphologyEx(subaqueous_cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Reconstruct the classified image\n",
    "        cleaned_classified_image = (subaqueous_cleaned * i +\n",
    "                                    subaqueous_cleaned * 1)      \n",
    "\n",
    "        # Add the processed relabeled image to the dictionary\n",
    "        cleaned_ims[method].append(cleaned_classified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check resulting classes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truthing\n",
    "- get water extent maps from various sources to serve as ground truth data for confirming the classification results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://global-surface-water.appspot.com/download\n",
    "2. USGS LandCover\n",
    "3. Copernicus Water and Wetness Product?\n",
    "4. Chesapeake Conservancy High-Resolution Land Cover Dataset\n",
    "5. RAMSAR Wetlands Sites\n",
    "6. MODIS Land Cover Type Product (MCD12Q1)\n",
    "7. Sentinel-2 Labeled Datasets for Wetland Classification\n",
    "8. OSM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
