{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will use the ESA SNAP GPT tool to create GRD products from the already downloaded SLC imagery. With the GRD data, the Normalized Difference Polarization Index, VV/VH, VH/VV, Normalized VH index, and Normalized VV index will be used to create water masks. Otsu thresholding will be done to label pixels as subaqeuous or subaerial.\n",
    "\n",
    "These labeled pixels will be used for two things:\n",
    "1. Generating a water mask for the InSAR time series (maybe the lowest water extent)\n",
    "2. Generating a time series of subaerial change for individual creation sites\n",
    "\n",
    "Plan is to couple the InSAR and Area time series to estimate volumetric changes of wetland BUDM sites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from osgeo import gdal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_help():\n",
    "\n",
    "    cmd = f'{GPT_PATH} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def help_cmd(cmdname):\n",
    "\n",
    "    cmd = f'{GPT_PATH} {cmdname} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def project_dir(slcpath):\n",
    "    \"\"\"\n",
    "    This function reads in a string that you wish to make your working directory \n",
    "    for the InSAR project, and creates a data directory to store the data for ISCE2 and mintpy\n",
    "    work-dir = str\n",
    "        path to the directory created in 01_get_slc.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    # path for GRD data storage\n",
    "    grd_dir = slcpath.replace('SLC', 'GRD')\n",
    "    os.makedirs(grd_dir, exist_ok=True)\n",
    "\n",
    "    # list of grd directories\n",
    "    grdpaths = []\n",
    "    for step in ['0_tnr', '1_cal', '2_deburst', '3_multilook', '4_speckle', '5_spgr', '6_grd', '7_subset']:\n",
    "        grdpaths.append(os.path.join(grd_dir, step))\n",
    "        os.makedirs(os.path.join(grd_dir, step), exist_ok=True)\n",
    "\n",
    "\n",
    "    return grd_dir, grdpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GPT path, number of processors, and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_PATH = '/home/wcc/esa-snap/bin/gpt'\n",
    "NUM_PROCESSORS = 24\n",
    "MEMORY_SIZE = '96G'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GRD images that were downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have downloaded .zip files covering your AOI from ASF Vertex\n",
    "# enter the file directory below\n",
    "slc_zips = '/home/wcc/Desktop/SabineRS/Sentinel-1/SLC/ASCENDING/136/93'\n",
    "\n",
    "slc_zips_list = sorted(os.listdir(slc_zips), key=lambda x: datetime.strptime(x[17:25], '%Y%m%d'))\n",
    "slc_zips_dirs = [os.path.join(slc_zips, slc) for slc in slc_zips_list]\n",
    "slc_zips_dates = [slc[17:25] for slc in slc_zips_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GRD working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_dir, grdpaths = project_dir(slc_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing steps for SLC to GRD:\n",
    "1. Thermal Noise Removal\n",
    "2. Calibration\n",
    "3. TOPSAR-Deburst\n",
    "4. Multilook\n",
    "5. Speckle-Filter\n",
    "6. Range-Doppler Terrain Correciton to 20m resolution\n",
    "7. Subset\n",
    "8. Write to tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARISATIONS = 'VV,VH' #can define this here for the rest of the notebook, if need to change will do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ThermalNoiseRemoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_NOISE = False\n",
    "REINTRODUCE_TN = False\n",
    "REMOVE_TN = True\n",
    "\n",
    "tnr_outpath = os.path.join(grdpaths[0], f'{slc_zips_dates[0]}.dim')\n",
    "tnr_cmd = f'{GPT_PATH} ThermalNoiseRemoval -SsourceProduct={os.path.join(slc_zips,slc_zips_list[0])} -PoutputNoise={OUTPUT_NOISE} -PreIntroduceThermalNoise={REINTRODUCE_TN} -PremoveThermalNoise={REMOVE_TN} -PselectedPolarisations={POLARISATIONS} -t {tnr_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(tnr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "\n",
    "# tnr_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     tnr_outpaths.append(os.path.join(grdpaths[0], f'{slc_zips_dates[i]}.dim'))\n",
    "#     tnr_cmd = f'{GPT_PATH} ThermalNoiseRemoval -SsourceProduct={os.path.join(slc_zips,file)} -PoutputNoise={OUTPUT_NOISE} -PreIntroduceThermalNoise={REINTRODUCE_TN} -PremoveThermalNoise={REMOVE_TN} -PselectedPolarisations={POLARISATIONS} -t {tnr_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(tnr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA_BAND = False\n",
    "OUT_BETA = False\n",
    "GAMMA_BAND = False\n",
    "OUT_GAMMA = False\n",
    "COMPLEX_IM = False\n",
    "DB_IM = False\n",
    "SIGMA_BAND = True\n",
    "\n",
    "cal_outpath = os.path.join(grdpaths[1], f'{slc_zips_dates[0]}.dim')\n",
    "\n",
    "tnr_cmd = f'{GPT_PATH} Calibration -Ssource={os.path.join(slc_zips,tnr_outpath)} -PcreateBetaBand={BETA_BAND} -PcreateGammaBand={GAMMA_BAND} -PoutputBetaBand={OUT_BETA} -PoutputGammaBand={OUT_GAMMA} -PoutputImageInComplex={COMPLEX_IM} -PoutputImageScaleInDb={DB_IM} -PoutputSigmaBand={SIGMA_BAND} -PselectedPolarisations={POLARISATIONS} -t {cal_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(tnr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# cal_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     cal_outpaths.append(os.path.join(grdpaths[1], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     tnr_cmd = f'{GPT_PATH} Calibration -Ssource={os.path.join(slc_zips,file)} -PcreateBetaBand={BETA_BAND} -PcreateGammaBand={GAMMA_BAND} -PoutputBetaBand={OUT_BETA} -PoutputGammaBand={OUT_GAMMA} -PoutputImageInComplex={COMPLEX_IM} -PoutputImageScaleInDb={DB_IM} -PoutputSigmaBand={SIGMA_BAND} -PselectedPolarisations={POLARISATIONS} -t {cal_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(tnr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TOPSAR-Deburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deburst_outpath = os.path.join(grdpaths[2], f'{slc_zips_dates[0]}.dim')\n",
    "\n",
    "deburst_cmd = f'{GPT_PATH} TOPSAR-Deburst -Ssource={os.path.join(slc_zips,cal_outpath)} -PselectedPolarisations={POLARISATIONS} -t {deburst_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(deburst_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# deburst_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     deburst_outpaths.append(os.path.join(grdpaths[2], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     deburst_cmd = f'{GPT_PATH} TOPSAR-Deburst -Ssource={os.path.join(slc_zips,file)} -PselectedPolarisations={POLARISATIONS} -t {deburst_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(deburst_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Multilook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_PIXEL = False\n",
    "AZI_LOOKS = '1'         # sentinel-1 is 22m in azimuth\n",
    "RANGE_LOOKS = '5'       # sentinel-1 is between 2.7 and 3.5m in range, depending on topography\n",
    "\n",
    "mlook_outpath = os.path.join(grdpaths[3], f'{slc_zips_dates[i]}.dim')\n",
    "\n",
    "mlook_cmd = f'{GPT_PATH} Multilook -Ssource={os.path.join(slc_zips,deburst_outpath)} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {mlook_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(mlook_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "\n",
    "# mlook_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     mlook_outpaths.append(os.path.join(grdpaths[3], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     mlook_cmd = f'{GPT_PATH} Multilook -Ssource={os.path.join(slc_zips,file)} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {mlook_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(mlook_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Speckle-Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBORHOOD_SIZE = 100 # integer, (1, 200]\n",
    "DAMPING_FACTOR = 2      # integer, for the Frost filter only\n",
    "NUM_LOOKS = 1.0         # integer, (0, *)\n",
    "ESTIMATE_ENL = False\n",
    "FILTER = 'Lee Sigma'  # str, otptions include None, Boxcar, Median, Frost, Gamma Map, Lee, Refined Lee, Lee Sigma, or IDAN\n",
    "FILTER_XSIZE = 3        # integer, (1, 100]\n",
    "FITLER_YSIZE = 3        # integer, (1, 100]\n",
    "NUM_LOOKS_STR = '1'         # str of integer, options are '1', '2', '3', '4'\n",
    "SIGMA_STR = '0.9'       # str, options are '0.5', '0.6', '0.7', '0.8', '0.9'\n",
    "TARGET_WINDOW = '3x3'   # str, can be '3x3' or '5x5'\n",
    "WINDOW_SIZE = '5x5'     # str, can be '5x5', '7x7', '9x9', '11x11', '13x13', or '15x15'  \n",
    "\n",
    "filter_outpath = os.path.join(grdpaths[4], f'{slc_zips_dates[0]}.dim')\n",
    "\n",
    "filter_cmd = f'{GPT_PATH} Speckle-Filter -Ssource={os.path.join(slc_zips,mlook_outpath)} -PanSize={NEIGHBORHOOD_SIZE} -PdampingFactor={DAMPING_FACTOR} -Penl={NUM_LOOKS} -PestimateENL={ESTIMATE_ENL} -Pfilter=\"{FILTER}\" -PfilterSizeX={FILTER_XSIZE} -PfilterSizeY={FITLER_YSIZE} -PnumLooksStr={NUM_LOOKS_STR} -PsigmaStr={SIGMA_STR} -PtargetWindowSizeStr={TARGET_WINDOW} -PwindowSize={WINDOW_SIZE} -t {filter_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(filter_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "\n",
    "# filter_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     filter_outpaths.append(os.path.join(grdpaths[4], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     filter_cmd = f'{GPT_PATH} Speckle-Filter -Ssource={os.path.join(slc_zips,file)} -PanSize={NEIGHBORHOOD_SIZE} -PdampingFactor={DAMPING_FACTOR} -Penl={NUM_LOOKS} -PestimateENL={ESTIMATE_ENL} -Pfilter={FILTER} -PfilterSizeX={FILTER_XSIZE} -PfilterSizeY={FITLER_YSIZE} -PnumLooksStr={NUM_LOOKS_STR} -PsigmaStr={SIGMA_STR} -PtargetWindowSizeStr={TARGET_WINDOW} -PwindowSize={WINDOW_SIZE} -t {filter_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(filter_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. SRGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERP_METHOD = 'Sinc'      # str, options are 'Linear', 'Cubic', 'Cubic2', and 'Sinc'\n",
    "WARP_POLY_PRDER = 4         # integer, options include 1, 2, 3, 4\n",
    "\n",
    "srgr_outpath = os.path.join(grdpaths[5], f'{slc_zips_dates[0]}.dim')\n",
    "\n",
    "srgr_cmd = f'{GPT_PATH} SRGR -Ssource={os.path.join(slc_zips,filter_outpath)} -PinterpolationMethod=\"{INTERP_METHOD} interpolation\" -PwarpPolynomialOrder={WARP_POLY_PRDER} -t {srgr_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(srgr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# srgr_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     srgr_outpaths.append(os.path.join(grdpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     srgr_cmd = f'{GPT_PATH} SRGR -Ssource={os.path.join(slc_zips,file)} -PinterploationMethod={INTERP_METHOD} -PwarpPolynomialOrder={WARP_POLY_PRDER} -t {srgr_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(srgr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. GRD-Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_outpath = os.path.join(grdpaths[6], f'{slc_zips_dates[0]}.dim')\n",
    "\n",
    "grd_cmd = f'{GPT_PATH} GRD-Post -Ssource={os.path.join(slc_zips,srgr_outpath)} -t {grd_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(grd_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# grd_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     grd_outpaths.append(os.path.join(grdpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     grd_cmd = f'{GPT_PATH} GRD-Post -Ssource={os.path.join(slc_zips,file)} -PinterploationMethod={INTERP_METHOD} -PwarpPolynomialOrder={WARP_POLY_PRDER} -t {grd_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(grd_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Subset? (using the aoi from the initial wrapped ifg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the wrapped ifg from 02a_isce2_processing.ipynb\n",
    "\n",
    "ds = gdal.Open('/home/wcc/Desktop/SabineRS/Sentinel-1/InSAR/interferometry/work/merged/filt_topophase.flat.geo.vrt', gdal.GA_ReadOnly)\n",
    "slc = ds.GetRasterBand(1).ReadAsArray()\n",
    "transform = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "# getting the min max of the axes\n",
    "firstx = transform[0]\n",
    "firsty = transform[3]\n",
    "deltay = transform[5]\n",
    "deltax = transform[1]\n",
    "lastx = firstx+slc.shape[1]*deltax\n",
    "lasty = firsty+slc.shape[0]*deltay\n",
    "ymin = np.min([lasty,firsty])\n",
    "ymax = np.max([lasty,firsty])\n",
    "xmin = np.min([lastx,firstx])\n",
    "xmax = np.max([lastx,firstx])\n",
    "\n",
    "aoi = f'POLYGON(({round(xmax,2)} {round(ymin,2)}, {round(xmax,2)} {round(ymax,2)}, {round(xmin,2)} {round(ymax,2)}, {round(xmin,2)} {round(ymin,2)}, {round(xmax,2)} {round(ymin,2)}))'\n",
    "print(aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_METADATA = True\n",
    "FULL_SWATH = False\n",
    "REFERNCE_BAND = 'Sigma0_VV'\n",
    "\n",
    "subset_outpath = os.path.join(grdpaths[7], f'{slc_zips_dates[0]}.dim')\n",
    "\n",
    "subset_cmd = f'{GPT_PATH} Subset -Ssource={os.path.join(slc_zips,grd_outpath)} -PcopyMetadata={COPY_METADATA} -PfullSwath={FULL_SWATH} -PgeoRegion=\"{aoi}\" -PreferenceBand={REFERNCE_BAND} -t {subset_outpath} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# subset_outpaths = []\n",
    "# for i, file in enumerate(slc_zips_list):\n",
    "#     subset_outpaths.append(os.path.join(subsetpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "#     subset_cmd = f'{GPT_PATH} subset-Post -Ssource={os.path.join(slc_zips,file)} -PinterploationMethod={INTERP_METHOD} -PwarpPolynomialOrder={WARP_POLY_PRDER} -t {subset_outpaths} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "#     try:\n",
    "#         result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         print(result.stdout.decode())\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(grdpaths[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_cmd('BandSelect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_CACHE = False\n",
    "DELETE_ON_FAILURE = True\n",
    "OUTFILE = os.path.join(grd_dir, f'{slc_zips_dates[0]}.tif')\n",
    "FILE_FORMAT = 'GeoTIFF / BigTIFF'         # options include BEAM-DIMAP, HDF5, and more\n",
    "WRITE_ENTIRE_ROW = False\n",
    "\n",
    "tiff_cmd = f'{GPT_PATH} Write -Ssource={os.path.join(slc_zips,subset_outpath)} -PclearCacheAfterRowWrite={CLEAR_CACHE} -PdeleteOutputOnFailure={DELETE_ON_FAILURE} -PwriteEntireTileRows={WRITE_ENTIRE_ROW} -t {OUTFILE} -f \"{FILE_FORMAT}\" -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "try:\n",
    "    result = subprocess.run(tiff_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "# shutil.rmtree(grdpaths[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(grd_dir, f'{slc_zips_dates[0]}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cmd = f'{GPT_PATH} /home/wcc/Desktop/SabineRS/Sentinel-1/GRD/export_geotiff.xml -Pinput={subset_outpath} -Poutput={os.path.join(grd_dir, f\"{slc_zips_dates[0]}.tif\")}'\n",
    "try:\n",
    "    result = subprocess.run(test_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Command failed: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
