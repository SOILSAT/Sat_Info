{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will use the ESA SNAP GPT tool to create GRD products from the already downloaded SLC imagery. With the GRD data, the Normalized Difference Polarization Index, VV/VH, VH/VV, Normalized VH index, and Normalized VV index will be used to create water masks. Otsu thresholding will be done to label pixels as subaqeuous or subaerial.\n",
    "\n",
    "These labeled pixels will be used for two things:\n",
    "1. Generating a water mask for the InSAR time series (maybe the lowest water extent)\n",
    "2. Generating a time series of subaerial change for individual creation sites\n",
    "\n",
    "Plan is to couple the InSAR and Area time series to estimate volumetric changes of wetland BUDM sites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from osgeo import gdal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_help():\n",
    "\n",
    "    cmd = f'{GPT_PATH} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def help_cmd(cmdname):\n",
    "\n",
    "    cmd = f'{GPT_PATH} {cmdname} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def project_dir(slcpath):\n",
    "    \"\"\"\n",
    "    This function reads in a string that you wish to make your working directory \n",
    "    for the InSAR project, and creates a data directory to store the data for ISCE2 and mintpy\n",
    "    work-dir = str\n",
    "        path to the directory created in 01_get_slc.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    grd_dir=slcpath.replace('SLC','GRD')\n",
    "    os.makedirs(grd_dir, exist_ok=True)\n",
    "\n",
    "    # list of grd directories\n",
    "    grdpaths = []\n",
    "    for step in ['1_orbit', '2_tnr', '3_cal', '4_deburst', '5_multilook', '6_subset', '7_speckle', '8_tf', '9_tc', 'tifs']:\n",
    "        grdpaths.append(os.path.join(grd_dir, step))\n",
    "        os.makedirs(os.path.join(grd_dir, step), exist_ok=True, mode=0o777)\n",
    "\n",
    "\n",
    "    return grd_dir, grdpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GPT path, number of processors, and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_PATH = '/home/clay/esa-snap/bin/gpt'\n",
    "NUM_PROCESSORS = 24\n",
    "MEMORY_SIZE = '96G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GRD images that were downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have downloaded .zip files covering your AOI from ASF Vertex\n",
    "# enter the file directory below\n",
    "slc_zips = '/home/clay/Documents/SabineRS/Sentinel-1/SLC/ASCENDING/136/93'\n",
    "\n",
    "slc_zips_list = sorted(os.listdir(slc_zips), key=lambda x: datetime.strptime(x[17:25], '%Y%m%d'))\n",
    "slc_zips_dirs = [os.path.join(slc_zips, slc) for slc in slc_zips_list]\n",
    "slc_zips_dates = [slc[17:25] for slc in slc_zips_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GRD working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_dir, grdpaths = project_dir(slc_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which polarizations you want to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARISATIONS = 'VV,VH' #can define this here for the rest of the notebook, if need to change will do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply-Orbit-File (this is the longest one, took me like 2 hours for 130+ images with 24 processors and 96G of ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORBIT_TYPE = 'Sentinel Precise (Auto Download)'     # str, options include'Sentinel Precise (Auto Download)', 'Sentinel Restituted (Auto Download)', 'DORIS Preliminary POR (ENVISAT)', 'DORIS Precise VOR (ENVISAT) (Auto Download)', 'DELFT Precise (ENVISAT, ERS1&2) (Auto Download)', 'PRARE Precise (ERS1&2) (Auto Download)', 'Kompsat5 Precise'\n",
    "POLY_DEGREE = 3                                     # int\n",
    "CONTINUE_ON_FAIL = False\n",
    "\n",
    "orbit_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    orbit_outpaths.append(os.path.join(grdpaths[0], f'{slc_zips_dates[i]}.dim'))\n",
    "    orbit_cmd = f'{GPT_PATH} Apply-Orbit-File -Ssource={os.path.join(slc_zips,file)} -PcontinueOnFail={CONTINUE_ON_FAIL} -PorbitType=\"{ORBIT_TYPE}\" -PpolyDegree={POLY_DEGREE} -t {orbit_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(orbit_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ThermalNoiseRemoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_NOISE = False\n",
    "REINTRODUCE_TN = False\n",
    "REMOVE_TN = True\n",
    "\n",
    "tnr_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tnr_outpaths.append(os.path.join(grdpaths[1], f'{slc_zips_dates[i]}.dim'))\n",
    "    tnr_cmd = f'{GPT_PATH} ThermalNoiseRemoval -SsourceProduct={orbit_outpaths[i]} -PoutputNoise={OUTPUT_NOISE} -PreIntroduceThermalNoise={REINTRODUCE_TN} -PremoveThermalNoise={REMOVE_TN} -PselectedPolarisations={POLARISATIONS} -t {tnr_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(tnr_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(orbit_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(orbit_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[0])  # removes the previous step folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA_BAND = True\n",
    "OUT_BETA = True\n",
    "GAMMA_BAND = False\n",
    "OUT_GAMMA = False\n",
    "COMPLEX_IM = False\n",
    "DB_IM = False\n",
    "SIGMA_BAND = True\n",
    "\n",
    "cal_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    cal_outpaths.append(os.path.join(grdpaths[2], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    cal_cmd = f'{GPT_PATH} Calibration -Ssource={tnr_outpaths[i]} -PcreateBetaBand={BETA_BAND} -PcreateGammaBand={GAMMA_BAND} -PoutputBetaBand={OUT_BETA} -PoutputGammaBand={OUT_GAMMA} -PoutputImageInComplex={COMPLEX_IM} -PoutputImageScaleInDb={DB_IM} -PoutputSigmaBand={SIGMA_BAND} -PselectedPolarisations={POLARISATIONS} -t {cal_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(cal_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(tnr_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(tnr_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. TOPSAR-Deburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deburst_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    deburst_outpaths.append(os.path.join(grdpaths[3], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    deburst_cmd = f'{GPT_PATH} TOPSAR-Deburst -Ssource={cal_outpaths[i]} -PselectedPolarisations={POLARISATIONS} -t {deburst_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(deburst_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(cal_outpaths[i]) # removes the .dim just used source product\n",
    "    shutil.rmtree(cal_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Multilook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_PIXEL = False\n",
    "AZI_LOOKS = '1'         # sentinel-1 is 22m in azimuth\n",
    "RANGE_LOOKS = '5'       # sentinel-1 is between 2.7 and 3.5m in range, depending on topography\n",
    "\n",
    "mlook_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    mlook_outpaths.append(os.path.join(grdpaths[4], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    mlook_cmd = f'{GPT_PATH} Multilook -Ssource={deburst_outpaths[i]} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {mlook_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(mlook_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(deburst_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(deburst_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Subset? (using the aoi from the initial wrapped ifg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the wrapped ifg from 02a_isce2_processing.ipynb\n",
    "\n",
    "ds = gdal.Open('/home/clay/Documents/SabineRS/Sentinel-1/InSAR/interferometry/work/merged/filt_topophase.flat.geo.vrt', gdal.GA_ReadOnly)\n",
    "slc = ds.GetRasterBand(1).ReadAsArray()\n",
    "transform = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "# getting the min max of the axes\n",
    "firstx = transform[0]\n",
    "firsty = transform[3]\n",
    "deltay = transform[5]\n",
    "deltax = transform[1]\n",
    "lastx = firstx+slc.shape[1]*deltax\n",
    "lasty = firsty+slc.shape[0]*deltay\n",
    "ymin = np.min([lasty,firsty])\n",
    "ymax = np.max([lasty,firsty])\n",
    "xmin = np.min([lastx,firstx])\n",
    "xmax = np.max([lastx,firstx])\n",
    "\n",
    "aoi = f'POLYGON(({round(xmax,2)} {round(ymin,2)}, {round(xmax,2)} {round(ymax,2)}, {round(xmin,2)} {round(ymax,2)}, {round(xmin,2)} {round(ymin,2)}, {round(xmax,2)} {round(ymin,2)}))'\n",
    "print(aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_METADATA = True\n",
    "FULL_SWATH = False\n",
    "REFERNCE_BAND = 'Beta0_VV'\n",
    "\n",
    "subset_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    subset_outpaths.append(os.path.join(grdpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    subset_cmd = f'{GPT_PATH} Subset -Ssource={mlook_outpaths[i]} -PcopyMetadata={COPY_METADATA} -PfullSwath={FULL_SWATH} -PgeoRegion=\"{aoi}\" -PreferenceBand={REFERNCE_BAND} -t {subset_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(mlook_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(mlook_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Speckle-Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBORHOOD_SIZE = 100 # integer, (1, 200]\n",
    "DAMPING_FACTOR = 2      # integer, for the Frost filter only\n",
    "NUM_LOOKS = 1.0         # integer, (0, *)\n",
    "ESTIMATE_ENL = False\n",
    "FILTER = 'Lee Sigma'  # str, otptions include None, Boxcar, Median, Frost, Gamma Map, Lee, Refined Lee, Lee Sigma, or IDAN\n",
    "FILTER_XSIZE = 3        # integer, (1, 100]\n",
    "FITLER_YSIZE = 3        # integer, (1, 100]\n",
    "NUM_LOOKS_STR = '1'         # str of integer, options are '1', '2', '3', '4'\n",
    "SIGMA_STR = '0.9'       # str, options are '0.5', '0.6', '0.7', '0.8', '0.9'\n",
    "TARGET_WINDOW = '3x3'   # str, can be '3x3' or '5x5'\n",
    "WINDOW_SIZE = '5x5'     # str, can be '5x5', '7x7', '9x9', '11x11', '13x13', or '15x15'  \n",
    "\n",
    "\n",
    "filter_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    filter_outpaths.append(os.path.join(grdpaths[6], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    filter_cmd = f'{GPT_PATH} Speckle-Filter -Ssource={subset_outpaths[i]} -PanSize={NEIGHBORHOOD_SIZE} -PdampingFactor={DAMPING_FACTOR} -Penl={NUM_LOOKS} -PestimateENL={ESTIMATE_ENL} -Pfilter=\"{FILTER}\" -PfilterSizeX={FILTER_XSIZE} -PfilterSizeY={FITLER_YSIZE} -PnumLooksStr={NUM_LOOKS_STR} -PsigmaStr={SIGMA_STR} -PtargetWindowSizeStr={TARGET_WINDOW} -PwindowSize={WINDOW_SIZE} -t {filter_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(filter_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(subset_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(subset_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Terrain-Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_OVERLAP = 0.2                            # double, [0,1]\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'BICUBIC_INTERPOLATION'      # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "OVERSAMPLE_MULTIPLE = 2.0                           # double, [1.0, 4.0]\n",
    "SOURCE_BANDS = 'Beta0_VV,Beta0_VH'\n",
    "\n",
    "tf_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tf_outpaths.append(os.path.join(grdpaths[7], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    tf_cmd = f'{GPT_PATH} Terrain-Flattening -Ssource={filter_outpaths[i]} -PadditionalOverlap={ADDITIONAL_OVERLAP} -PdemName=\"{DEM_NAME}\" -PdemResamplingMethod={DEM_RESAMPLE_METHOD} -PoversamplingMultiple={OVERSAMPLE_MULTIPLE} -PsourceBands={SOURCE_BANDS} -t {tf_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(tf_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(filter_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(filter_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Terrain Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGN_STANDARD_GRID = False\n",
    "RADIOMETRIC_NORMALIZATION = True\n",
    "AUX_FILE = 'Product Auxiliary File'   # str, options include 'Latest Auxiliary File', 'Product Auxiliary File', 'External Auxiliary File'\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'DELAUNAY_INTERPOLATION'   # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION', 'DELAUNAY_INTERPOLATION'\n",
    "IMG_RESAMPLE_METHOD = 'CUBIC_CONVOLUTION' # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "NODATA_SEA = False\n",
    "OUTPUT_COMPLEX = False\n",
    "PIXEL_SPACING_METERS = 20.0   # double\n",
    "SAVE_DEM = False\n",
    "SAVE_ANGLE_ELLIPSOID = False\n",
    "SAVE_LAT_LON = False\n",
    "SAVE_LAYOVER_SHADOW = True\n",
    "SAVE_LOCAL_ANGLE = False\n",
    "SAVE_PROJ_LOCAL_ANGLE = True\n",
    "SAVE_SOURCE_BAND = True\n",
    "SOURCE_BANDS = 'Gamma0_VV,Gamma0_VH'\n",
    "GRID_ORIGIN_X = 0   # double\n",
    "GRID_ORIGIN_Y = 0   # double\n",
    "\n",
    "# only needed if you you are using external dem\n",
    "# uncomment these and adjust your gpt command below accordingly\n",
    "#  EXTERNAL_AUX = \n",
    "# EXTERNAL_DEMFILE =\n",
    "# EXTERNAL_DEMFILE_NODATA =\n",
    "# ETERNAL_DEM_EGM = False\n",
    "\n",
    "tc_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tc_outpaths.append(os.path.join(grdpaths[8], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    tc_cmd = f'{GPT_PATH} Terrain-Correction -Ssource={tf_outpaths[i]} -PalignToStandardGrid={ALIGN_STANDARD_GRID} -PapplyRadiometricNormalization={RADIOMETRIC_NORMALIZATION} -PauxFile=\"{AUX_FILE}\" -PdemName=\"{DEM_NAME}\" -PdemResamplingMethod=\"{DEM_RESAMPLE_METHOD}\" -PimgResamplingMethod=\"{IMG_RESAMPLE_METHOD}\" -PnodataValueAtSea={NODATA_SEA} -PoutputComplex={OUTPUT_COMPLEX} -PpixelSpacingInMeter={PIXEL_SPACING_METERS} -PsaveDEM={SAVE_DEM} -PsaveIncidenceAngleFromEllipsoid={SAVE_ANGLE_ELLIPSOID} -PsaveLatLon={SAVE_LAT_LON} -PsaveLayoverShadowMask={SAVE_LAYOVER_SHADOW} -PsaveLocalIncidenceAngle={SAVE_LOCAL_ANGLE} -PsaveProjectedLocalIncidenceAngle={SAVE_PROJ_LOCAL_ANGLE} -PsaveSelectedSourceBand={SAVE_SOURCE_BAND} -PsourceBands={SOURCE_BANDS} -PstandardGridOriginX={GRID_ORIGIN_X} -PstandardGridOriginY={GRID_ORIGIN_Y} -t {tc_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(tc_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(tf_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(tf_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write\n",
    "- writes VV and VH gamma to .tif, NoData = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiff doesn't contain metadata. I am going to try export as a NetCDF file instead of GEotiff-Bigtiff\n",
    "\n",
    "write_xml = '/home/clay/Documents/geotiff_export.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_BANDS = 'Gamma0_VV,Gamma0_VH'\n",
    "\n",
    "tif_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tif_outpaths.append(os.path.join(grdpaths[9], f'{slc_zips_dates[i]}'))\n",
    "\n",
    "    rdtc_cmd = f'{GPT_PATH} {write_xml} -Pinput={tc_outpaths[i]} -Poutput={tif_outpaths[i]} -Ppolarisations=\"{POLARISATIONS}\" -Pbands=\"{OUT_BANDS}\" -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(rdtc_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(tc_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(tc_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
