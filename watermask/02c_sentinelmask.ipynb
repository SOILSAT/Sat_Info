{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will be used to read in the two .nc files containing the Sentinel-1 Gamma0 backscatter, as well as the composite bands created from the backscatter. Using these .nc files, cloud-masked Sentinel-2 images will be retrieved from the GEE. The NDVI and NDWI will be used at 10-meter resolution. Otsu thresholding will be applied on all 9 bands, where the NDVI and NDWI will serve as refinement given the finer spatial resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import cv2\n",
    "import ee\n",
    "import geemap\n",
    "import geemap.colormaps as cm\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "from skimage.filters import threshold_otsu\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2_sr_cld_col(aoi, start_date, end_date, cloud_filter):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', cloud_filter)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    combined_coll = ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))\n",
    "\n",
    "    return combined_coll.map(lambda img: img.clip(aoi))\n",
    "\n",
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "\n",
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "\n",
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img.addBands(is_cld_shdw)\n",
    "\n",
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)\n",
    "\n",
    "def export_image_to_drive(image, description, aoi):\n",
    "    \"\"\"\n",
    "    Export a single image to Google Drive.\n",
    "\n",
    "    Args:\n",
    "        image: ee.Image, the image to be exported.\n",
    "        description: str, unique description for the export task.\n",
    "        aoi: ee.Geometry, the area of interest for the export.\n",
    "    \"\"\"\n",
    "\n",
    "    image = image.select(['B2', 'B3', 'B4', 'B8'])\n",
    "    # Setup the export task\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=image,\n",
    "        description=description,\n",
    "        region=aoi,  # Make sure the geometry (aoi) is defined earlier\n",
    "        fileFormat='GeoTIFF',\n",
    "        scale=10  # Adjust the scale as needed\n",
    "    )\n",
    "    task.start()\n",
    "    print(f'Exporting {description} to Drive...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedley_glint_correction(image, SWIR_band='B11', bands=['B2', 'B3', 'B4', 'B8']):\n",
    "    # Select bands\n",
    "    swir = image.select(SWIR_band)\n",
    "    coefficients = image.select(bands).divide(swir).reduce(ee.Reducer.mean())\n",
    "    \n",
    "    # Apply correction\n",
    "    corrected = image.select(bands).subtract(swir.multiply(coefficients))\n",
    "    return image.addBands(corrected, overwrite=True).set('glint_corrected', True)\n",
    "\n",
    "def s2_10m_target_indices(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "\n",
    "    return image.addBands([ndvi, ndwi])\n",
    "\n",
    "def otsu_gee(histogram):\n",
    "    counts = ee.Array(ee.Dictionary(histogram).get('histogram'))\n",
    "    means = ee.Array(ee.Dictionary(histogram).get('bucketMeans'))  # Correct key\n",
    "    size = means.length().get([0])\n",
    "    total = counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "    sum_ = means.multiply(counts).reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "    mean = sum_.divide(total)\n",
    "\n",
    "    indices = ee.List.sequence(1, size)\n",
    "\n",
    "    # Compute between sum of squares (BSS)\n",
    "    def compute_bss(i):\n",
    "        i = ee.Number(i)\n",
    "        a_counts = counts.slice(0, 0, i)\n",
    "        a_count = a_counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "        a_means = means.slice(0, 0, i)\n",
    "        a_mean = a_means.multiply(a_counts).reduce(ee.Reducer.sum(), [0]).get([0]).divide(a_count)\n",
    "\n",
    "        b_count = total.subtract(a_count)\n",
    "        b_mean = sum_.subtract(a_count.multiply(a_mean)).divide(b_count)\n",
    "\n",
    "        return a_count.multiply(a_mean.subtract(mean).pow(2)).add(\n",
    "            b_count.multiply(b_mean.subtract(mean).pow(2))\n",
    "        )\n",
    "\n",
    "    bss = indices.map(compute_bss)\n",
    "\n",
    "    # Return the mean value corresponding to the maximum BSS\n",
    "    return means.sort(bss).get([-1])\n",
    "\n",
    "def extract_valid_bounds_nc_to_epsg4326(nc_file, band_name):\n",
    "    \"\"\"Extracts the bounding box of valid (non-NaN) data from a NetCDF file and converts it to EPSG:4326.\"\"\"\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "    # Ensure the band exists\n",
    "    if band_name not in dataset.variables:\n",
    "        raise ValueError(f\"Band '{band_name}' not found in the NetCDF file.\")\n",
    "\n",
    "    # Read latitude, longitude, and band data\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    lon = dataset.variables['lon'][:]\n",
    "    data = dataset.variables[band_name][:]\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "\n",
    "    # Create a mask for valid (non-NaN) pixels\n",
    "    valid_mask = ~np.isnan(data)\n",
    "\n",
    "    # Find the row and column indices of valid pixels\n",
    "    valid_rows, valid_cols = np.where(valid_mask)\n",
    "\n",
    "    if valid_rows.size == 0 or valid_cols.size == 0:\n",
    "        raise ValueError(\"No valid data in the NetCDF file.\")\n",
    "\n",
    "    # Get the lat/lon bounds based on valid data\n",
    "    min_lat, max_lat = lat[valid_rows.min()], lat[valid_rows.max()]\n",
    "    min_lon, max_lon = lon[valid_cols.min()], lon[valid_cols.max()]\n",
    "\n",
    "    # NetCDF usually stores lat/lon as 1D vectors, assuming they are regularly spaced.\n",
    "    bounds_src_crs = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "    # Assuming the NetCDF data is in EPSG:4326, if not, transform to EPSG:4326\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:4326\", always_xy=True)\n",
    "    min_lon_4326, min_lat_4326 = transformer.transform(min_lon, min_lat)\n",
    "    max_lon_4326, max_lat_4326 = transformer.transform(max_lon, max_lat)\n",
    "\n",
    "    # Create bounding box in EPSG:4326 format\n",
    "    bounds_epsg4326 = (min_lon_4326, min_lat_4326, max_lon_4326, max_lat_4326)\n",
    "\n",
    "    # Convert to Earth Engine Bounding Box\n",
    "    bbox = ee.Geometry.BBox(bounds_epsg4326[0], bounds_epsg4326[1], bounds_epsg4326[2], bounds_epsg4326[3])\n",
    "\n",
    "    return bbox\n",
    "\n",
    "def get_date(image):\n",
    "    return ee.Feature(None, {'date': image.date().format('YYYY-MM-dd')})\n",
    "\n",
    "def mosaic_images_for_date(date, sentinel2_col):\n",
    "    date_filter = ee.Filter.date(date, ee.Date(date).advance(1, 'day'))\n",
    "\n",
    "    # Try to get Sentinel-2 mosaic; return None if no images available\n",
    "    s2_filtered = sentinel2_col.filter(date_filter)\n",
    "    s2_mosaic = ee.Algorithms.If(s2_filtered.size().gt(0), s2_filtered.mosaic(), None)\n",
    "    \n",
    "    return ee.Dictionary({\n",
    "        'date': date,\n",
    "        'S2': s2_mosaic\n",
    "    })\n",
    "\n",
    "## Function to add spectral indices images to the map.\n",
    "def add_ind_to_map(image, map_object, band, date):\n",
    "\n",
    "    if band =='NDWI':\n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.ndwi}, f'{date}_{band}')\n",
    "    elif band =='NDVI': \n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.ndvi}, f'{date}_{band}')\n",
    "\n",
    "def filter_images_by_valid_pixels(mosaic_dict, subset_aoi, percentage, im_scale):\n",
    "    good_ims = {}\n",
    "    \n",
    "    for date in list(mosaic_dict.keys()):\n",
    "        image = ee.Image(mosaic_dict[date].get('S2'))\n",
    "        valid_pixels = image.mask().reduceRegion(\n",
    "                        reducer=ee.Reducer.sum(),\n",
    "                        geometry=subset_aoi,\n",
    "                        scale=im_scale,\n",
    "                        maxPixels=1e13\n",
    "                    ).values().get(0)\n",
    "        total_pixels = subset_aoi.area().divide(im_scale ** 2)  # Estimate total pixels at given scale\n",
    "        valid_fraction = ee.Number(valid_pixels).divide(total_pixels)\n",
    "\n",
    "        if valid_fraction.getInfo() > percentage:\n",
    "            good_ims[date] = image\n",
    "\n",
    "    return good_ims\n",
    "\n",
    "def visualize_nc_band(nc_file, band_name):\n",
    "    # Open the NetCDF file\n",
    "    dataset = nc.Dataset(nc_file, mode='r')\n",
    "    \n",
    "    # Check if the band exists\n",
    "    if band_name not in dataset.variables:\n",
    "        print(f\"Error: Band '{band_name}' not found in the NetCDF file.\")\n",
    "        print(\"Available bands:\", list(dataset.variables.keys()))\n",
    "        dataset.close()\n",
    "        return\n",
    "\n",
    "    # Extract latitude, longitude, and the requested band\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    lon = dataset.variables['lon'][:]\n",
    "    band_data = dataset.variables[band_name][:]\n",
    "\n",
    "    # Close the dataset after reading\n",
    "    dataset.close()\n",
    "\n",
    "    # Handle missing values (replace with NaN)\n",
    "    band_data = np.where(np.isnan(band_data), np.nanmin(band_data), band_data)\n",
    "\n",
    "    # Clip values to avoid extreme outliers\n",
    "    vmin, vmax = np.percentile(band_data, [1, 99])\n",
    "\n",
    "    # Plot the specified band with enhanced visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(band_data, extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "               cmap='gray', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label=band_name)\n",
    "    plt.title(f'{band_name} Visualization')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    # plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def check_spatial_resolution(nc_file):\n",
    "    # Open the NetCDF file\n",
    "    dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    lon = dataset.variables['lon'][:]\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "\n",
    "    # Calculate the resolution by taking the mean difference\n",
    "    lat_res = np.abs(lat[1] - lat[0])  # Latitude resolution\n",
    "    lon_res = np.abs(lon[1] - lon[0])  # Longitude resolution\n",
    "\n",
    "    print(f\"Latitude resolution: {lat_res:.6f} degrees\")\n",
    "    print(f\"Longitude resolution: {lon_res:.6f} degrees\")\n",
    "\n",
    "    # Approximate resolution in meters (assuming 1 degree ≈ 111 km)\n",
    "    lat_res_m = lat_res * 111000\n",
    "    lon_res_m = lon_res * 111000 * np.cos(np.deg2rad(lat.mean()))\n",
    "\n",
    "    print(f\"Approximate pixel size: {lat_res_m:.2f} meters (latitude), {lon_res_m:.2f} meters (longitude)\")\n",
    "\n",
    "def otsu_nc(nc_path):\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "    \n",
    "    thresholds = {}\n",
    "    \n",
    "    for band in ds.data_vars.keys():  # Loop through each band\n",
    "        data = ds[band].values  # Extract NumPy array\n",
    "        \n",
    "        # Ensure data is valid (no NaNs)\n",
    "        data = np.nan_to_num(data, nan=0)\n",
    "        \n",
    "        if data.ndim == 2:  # Single-band case\n",
    "            thresholds[band] = threshold_otsu(data)\n",
    "        \n",
    "        elif data.ndim == 3:  # Multi-band case (e.g., [time, height, width])\n",
    "            thresholds[band] = [threshold_otsu(data[i]) for i in range(data.shape[0])]\n",
    "\n",
    "    ds.close()\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in the custom Gamma0 .nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'SabineRS'\n",
    "SYSTEM = 'mac'          # or 'linux'\n",
    "\n",
    "\n",
    "if SYSTEM == 'linux':\n",
    "    grd_dir = f'/home/clay/Documents/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/10_netcdfs'\n",
    "    comp_dir =f'/home/clay/Documents/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/11_composites'\n",
    "else:\n",
    "    grd_dir = f'/Users/clayc/Documents/Work/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/10_netcdfs'\n",
    "    comp_dir =f'/Users/clayc/Documents/Work/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/11_composites'\n",
    "\n",
    "if os.path.exists(grd_dir) == False:\n",
    "    aoi = ee.Geometry.BBox(-93.68183131860044, 30.403070192729114, -92.57492722550838, 29.476368145631415)\n",
    "\n",
    "else:\n",
    "    nc_files = {}\n",
    "    \n",
    "    grd_dates = sorted(file[:-3] for file in os.listdir(grd_dir) if file.endswith('.nc')) \n",
    "    grd_dates = [datetime.strptime(key, \"%Y%m%d\") for key in grd_dates]\n",
    "    grd_files = sorted([os.path.join(grd_dir, f'{file}') for file in os.listdir(grd_dir) if file.endswith('.nc')]) \n",
    "    comp_files = sorted([os.path.join(comp_dir, f'{file}') for file in os.listdir(comp_dir) if file.endswith('.nc')]) \n",
    "    \n",
    "    for i, date in enumerate(grd_dates):\n",
    "        nc_files[date] = [grd_files[i], comp_files[i]]\n",
    "    \n",
    "    aoi = extract_valid_bounds_nc_to_epsg4326(grd_files[0], 'Gamma0_VV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_nc_file(grd_files[-1])\n",
    "# check_spatial_resolution(grd_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Sentinel imagery from GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_cloud_cov = 20\n",
    "START_DATE = '2019-10-01'\n",
    "END_DATE = '2024-10-01'\n",
    "CLD_PRB_THRESH = 35\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 100\n",
    "ORBIT = 'ASCENDING'\n",
    "\n",
    "s2_coll = get_s2_sr_cld_col(aoi, START_DATE, END_DATE, s2_cloud_cov)\n",
    "\n",
    "s2_cm = (s2_coll.map(add_cld_shdw_mask).map(apply_cld_shdw_mask).map(s2_10m_target_indices)).select(['NDVI', 'NDWI'])       # cloud masked sentinel-2 10-meter ndvi and ndwi bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mosaic images with matching dates\n",
    "- Common cells for GEE Sentinel imagery typically smaller than that of ASF/Copernicus\n",
    "- Could result in multiple images for a single day\n",
    "- Iterate through the information or each of the images in the S1 and S2 collections, mosaic matching dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = s2_cm.map(get_date).aggregate_array('date').getInfo()\n",
    "unique_list = sorted(list(set(s2_date_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of mosaicked images for all dates\n",
    "mosaic_dict = {}\n",
    "for date in unique_list:\n",
    "    mosaics = mosaic_images_for_date(date, s2_cm)\n",
    "    strpdate = datetime.strptime(date, '%Y-%m-%d')\n",
    "    mosaic_dict[strpdate] = mosaics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporally register Sentinel-1 .nc files with Sentinel-2 images\n",
    "- As of now, this only gets exact matches. Need to integrate one that allows for matches within a certain threshold of days. Like each S2 epoch within 3 days of a S1 epoch will be registered to said S1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in nc_files.keys():\n",
    "    if date in list(mosaic_dict.keys()):\n",
    "        nc_files[date].append(mosaic_dict[date])\n",
    "\n",
    "filtered_dates = sorted(list(nc_files.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the appropriate mosaics to GEE\n",
    "# will bring back in at the end to do the subpixel refinement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding (Otsu, adaptive, gaussian?)\n",
    "- Otsu on all 20-meter SAR bands\n",
    "- Otsu on NDVI and NDWI 10-meter bands\n",
    "- Majority voting with SAR, refinement with NDVI and NDWI\n",
    "- Final shoul give water mask for each registered pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu on the GEE cloud-masked S2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_ims = {}\n",
    "ndwi_ims = {}\n",
    "\n",
    "for band in ['NDVI', 'NDWI']:\n",
    "    for date in nc_files.keys():\n",
    "        if len(nc_files[date]) == 3:\n",
    "            image = ee.Image(nc_files[date][-1].get('S2'))\n",
    "            histogram = image.select(band).reduceRegion(\n",
    "            reducer=ee.Reducer.histogram(),\n",
    "            geometry=aoi,\n",
    "            scale=10,  # Adjust based on your dataset resolution\n",
    "            maxPixels=1e15\n",
    "            ).get(band)\n",
    "\n",
    "            result = otsu_gee(histogram)\n",
    "            threshold = result.getInfo()\n",
    "\n",
    "            # Create binary mask\n",
    "            binary_mask = image.select(band).gt(threshold)\n",
    "        \n",
    "        if band == 'NDVI':\n",
    "            ndvi_ims[date] = binary_mask\n",
    "        else:\n",
    "            ndwi_ims[date] = binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export the water masks created from the NDVI and NDWI to google drive as a .tif?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map = geemap.Map()\n",
    "# map.centerObject(aoi, 8)\n",
    "\n",
    "# target_band = 'NDWI'\n",
    "\n",
    "# for i in [0, 15, -1]:\n",
    "#     image = ndwi_ims[filtered_dates[i]]\n",
    "#     map.addLayer(image, {'min': 0, 'max': 1, 'bands': target_band}, f'{filtered_dates[i]}')\n",
    "\n",
    "# map.addLayerControl(position='topleft')\n",
    "# map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu on the GRD files. OpenCV or Scikit-image???\n",
    "\n",
    "Logarithmic trsnform + adaptive thresholding on SAR, Otsu + manual on optical??\n",
    "\n",
    "Gamma MAP Filtering + Otsu on SAR, Otsu + manual on optical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {}\n",
    "\n",
    "for entry in list(nc_files.items()):\n",
    "    maskedbands = []\n",
    "    date = entry[0]\n",
    "    files = [entry[1][0], entry[1][1]]\n",
    "    file_bands = [['Gamma0_VV', 'Gamma0_VH'], ['VH_VV', 'NDPI', 'NVVI', 'NVHI', 'RVI']]\n",
    "\n",
    "    for file, bands in zip(files, file_bands):\n",
    "        for band in bands:\n",
    "            dataset = nc.Dataset(file, mode='r')\n",
    "\n",
    "            if band not in dataset.variables:\n",
    "                print(f\"Error: Band '{band}' not found in the NetCDF file.\")\n",
    "                print(\"Available bands:\", list(dataset.variables.keys()))\n",
    "                dataset.close()\n",
    "\n",
    "            # Extract latitude, longitude, and the requested band\n",
    "            lat = dataset.variables['lat'][:]\n",
    "            lon = dataset.variables['lon'][:]\n",
    "            band_data = dataset.variables[band][:]\n",
    "\n",
    "            # Close the dataset after reading\n",
    "            dataset.close()\n",
    "\n",
    "            # Handle missing values (replace with NaN)\n",
    "            band_data = np.where(np.isnan(band_data), np.nanmin(band_data), band_data)\n",
    "\n",
    "            # Clip values to avoid extreme outliers\n",
    "            vmin, vmax = np.percentile(band_data, [1, 99])\n",
    "            clipped_data = np.clip(band_data, vmin, vmax)\n",
    "            threshold = threshold_otsu(clipped_data)\n",
    "\n",
    "            binary_mask = np.zeros_like(band_data, dtype=np.uint8)\n",
    "            binary_mask[~np.isnan(band_data)] = (band_data[~np.isnan(band_data)] > threshold).astype(np.uint8)\n",
    "\n",
    "            maskedbands.append(binary_mask)\n",
    "            binary_masks = np.stack(maskedbands, axis=0)  # Shape: (7, H, W)\n",
    "            final_s1_mask = np.mean(binary_masks, axis=0) >= 0.5  # Majority vote (50% or more)\n",
    "\n",
    "    masks[date] = final_s1_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 subpixel refinement using NDVI and NDWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NDVI AND NDWI REFINEMENT LATER\n",
    "\n",
    "# Compute Otsu’s thresholds\n",
    "ndwi_otsu = threshold_otsu(ndwi)\n",
    "ndvi_otsu = threshold_otsu(ndvi)\n",
    "\n",
    "# Constrain to literature-based values\n",
    "ndwi_thresh = max(ndwi_otsu, 0.1)  # Prevent overly strict thresholds\n",
    "ndvi_thresh = min(ndvi_otsu, 0.3)  # Prevent excessive masking\n",
    "\n",
    "# Apply refined thresholds\n",
    "ndwi_mask = ndwi > ndwi_thresh\n",
    "ndvi_mask = ndvi < ndvi_thresh\n",
    "\n",
    "# Combine with Sentinel-1 water mask\n",
    "final_mask = water_mask & ndwi_mask & ndvi_mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
