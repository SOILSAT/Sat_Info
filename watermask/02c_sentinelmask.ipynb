{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will be used to read in the two .nc files containing the Sentinel-1 Gamma0 backscatter, as well as the composite bands created from the backscatter. Using these .nc files, cloud-masked Sentinel-2 images will be retrieved from the GEE. The NDVI and NDWI will be used at 10-meter resolution. Otsu thresholding will be applied on all 9 bands, where the NDVI and NDWI will serve as refinement given the finer spatial resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import ee\n",
    "import geemap\n",
    "import geemap.colormaps as cm\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2_sr_cld_col(aoi, start_date, end_date, cloud_filter):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', cloud_filter)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    combined_coll = ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))\n",
    "\n",
    "    return combined_coll.map(lambda img: img.clip(aoi))\n",
    "\n",
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "\n",
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "\n",
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img.addBands(is_cld_shdw)\n",
    "\n",
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)\n",
    "\n",
    "def export_image_to_drive(image, description, aoi):\n",
    "    \"\"\"\n",
    "    Export a single image to Google Drive.\n",
    "\n",
    "    Args:\n",
    "        image: ee.Image, the image to be exported.\n",
    "        description: str, unique description for the export task.\n",
    "        aoi: ee.Geometry, the area of interest for the export.\n",
    "    \"\"\"\n",
    "\n",
    "    image = image.select(['B2', 'B3', 'B4', 'B8'])\n",
    "    # Setup the export task\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=image,\n",
    "        description=description,\n",
    "        region=aoi,  # Make sure the geometry (aoi) is defined earlier\n",
    "        fileFormat='GeoTIFF',\n",
    "        scale=10  # Adjust the scale as needed\n",
    "    )\n",
    "    task.start()\n",
    "    print(f'Exporting {description} to Drive...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedley_glint_correction(image, SWIR_band='B11', bands=['B2', 'B3', 'B4', 'B8']):\n",
    "    # Select bands\n",
    "    swir = image.select(SWIR_band)\n",
    "    coefficients = image.select(bands).divide(swir).reduce(ee.Reducer.mean())\n",
    "    \n",
    "    # Apply correction\n",
    "    corrected = image.select(bands).subtract(swir.multiply(coefficients))\n",
    "    return image.addBands(corrected, overwrite=True).set('glint_corrected', True)\n",
    "\n",
    "def s2_10m_target_indices(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "\n",
    "    return image.addBands([ndvi, ndwi])\n",
    "\n",
    "def otsu_gee(histogram):\n",
    "    counts = ee.Array(ee.Dictionary(histogram).get('histogram'))\n",
    "    means = ee.Array(ee.Dictionary(histogram).get('bucketMeans'))  # Correct key\n",
    "    size = means.length().get([0])\n",
    "    total = counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "    sum_ = means.multiply(counts).reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "    mean = sum_.divide(total)\n",
    "\n",
    "    indices = ee.List.sequence(1, size)\n",
    "\n",
    "    # Compute between sum of squares (BSS)\n",
    "    def compute_bss(i):\n",
    "        i = ee.Number(i)\n",
    "        a_counts = counts.slice(0, 0, i)\n",
    "        a_count = a_counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "        a_means = means.slice(0, 0, i)\n",
    "        a_mean = a_means.multiply(a_counts).reduce(ee.Reducer.sum(), [0]).get([0]).divide(a_count)\n",
    "\n",
    "        b_count = total.subtract(a_count)\n",
    "        b_mean = sum_.subtract(a_count.multiply(a_mean)).divide(b_count)\n",
    "\n",
    "        return a_count.multiply(a_mean.subtract(mean).pow(2)).add(\n",
    "            b_count.multiply(b_mean.subtract(mean).pow(2))\n",
    "        )\n",
    "\n",
    "    bss = indices.map(compute_bss)\n",
    "\n",
    "    # Return the mean value corresponding to the maximum BSS\n",
    "    return means.sort(bss).get([-1])\n",
    "\n",
    "def extract_valid_bounds_nc_to_epsg4326(nc_file, band_name):\n",
    "    \"\"\"Extracts the bounding box of valid (non-NaN) data from a NetCDF file and converts it to EPSG:4326.\"\"\"\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "    # Ensure the band exists\n",
    "    if band_name not in dataset.variables:\n",
    "        raise ValueError(f\"Band '{band_name}' not found in the NetCDF file.\")\n",
    "\n",
    "    # Read latitude, longitude, and band data\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    lon = dataset.variables['lon'][:]\n",
    "    data = dataset.variables[band_name][:]\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "\n",
    "    # Create a mask for valid (non-NaN) pixels\n",
    "    valid_mask = ~np.isnan(data)\n",
    "\n",
    "    # Find the row and column indices of valid pixels\n",
    "    valid_rows, valid_cols = np.where(valid_mask)\n",
    "\n",
    "    if valid_rows.size == 0 or valid_cols.size == 0:\n",
    "        raise ValueError(\"No valid data in the NetCDF file.\")\n",
    "\n",
    "    # Get the lat/lon bounds based on valid data\n",
    "    min_lat, max_lat = lat[valid_rows.min()], lat[valid_rows.max()]\n",
    "    min_lon, max_lon = lon[valid_cols.min()], lon[valid_cols.max()]\n",
    "\n",
    "    # NetCDF usually stores lat/lon as 1D vectors, assuming they are regularly spaced.\n",
    "    bounds_src_crs = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "    # Assuming the NetCDF data is in EPSG:4326, if not, transform to EPSG:4326\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:4326\", always_xy=True)\n",
    "    min_lon_4326, min_lat_4326 = transformer.transform(min_lon, min_lat)\n",
    "    max_lon_4326, max_lat_4326 = transformer.transform(max_lon, max_lat)\n",
    "\n",
    "    # Create bounding box in EPSG:4326 format\n",
    "    bounds_epsg4326 = (min_lon_4326, min_lat_4326, max_lon_4326, max_lat_4326)\n",
    "\n",
    "    # Convert to Earth Engine Bounding Box\n",
    "    bbox = ee.Geometry.BBox(bounds_epsg4326[0], bounds_epsg4326[1], bounds_epsg4326[2], bounds_epsg4326[3])\n",
    "\n",
    "    return bbox\n",
    "\n",
    "def get_date(image):\n",
    "    return ee.Feature(None, {'date': image.date().format('YYYY-MM-dd')})\n",
    "\n",
    "def mosaic_images_for_date(date, sentinel2_col):\n",
    "    date_filter = ee.Filter.date(date, ee.Date(date).advance(1, 'day'))\n",
    "\n",
    "    # Try to get Sentinel-2 mosaic; return None if no images available\n",
    "    s2_filtered = sentinel2_col.filter(date_filter)\n",
    "    s2_mosaic = ee.Algorithms.If(s2_filtered.size().gt(0), s2_filtered.mosaic(), None)\n",
    "    \n",
    "    return ee.Dictionary({\n",
    "        'date': date,\n",
    "        'S2': s2_mosaic\n",
    "    })\n",
    "\n",
    "## Function to add spectral indices images to the map.\n",
    "def add_ind_to_map(image, map_object, band, date):\n",
    "\n",
    "    if band =='NDWI':\n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.ndwi}, f'{date}_{band}')\n",
    "    elif band =='NDVI': \n",
    "        map_object.addLayer(image, {'min': -1, 'max': 1, 'bands': band, 'palette': cm.palettes.ndvi}, f'{date}_{band}')\n",
    "\n",
    "def filter_images_by_valid_pixels(mosaic_dict, subset_aoi, percentage, im_scale):\n",
    "    good_ims = {}\n",
    "    \n",
    "    for date in list(mosaic_dict.keys()):\n",
    "        image = ee.Image(mosaic_dict[date].get('S2'))\n",
    "        valid_pixels = image.mask().reduceRegion(\n",
    "                        reducer=ee.Reducer.sum(),\n",
    "                        geometry=subset_aoi,\n",
    "                        scale=im_scale,\n",
    "                        maxPixels=1e13\n",
    "                    ).values().get(0)\n",
    "        total_pixels = subset_aoi.area().divide(im_scale ** 2)  # Estimate total pixels at given scale\n",
    "        valid_fraction = ee.Number(valid_pixels).divide(total_pixels)\n",
    "\n",
    "        if valid_fraction.getInfo() > percentage:\n",
    "            good_ims[date] = image\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return good_ims\n",
    "\n",
    "# Function to visualize the NetCDF bands\n",
    "def visualize_nc_file(nc_file):\n",
    "    # Open the NetCDF file\n",
    "    dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "    # Extract variables\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    lon = dataset.variables['lon'][:]\n",
    "    gamma0_vv = dataset.variables['Gamma0_VV'][:]\n",
    "    gamma0_vh = dataset.variables['Gamma0_VH'][:]\n",
    "\n",
    "    # Close the dataset after reading\n",
    "    dataset.close()\n",
    "\n",
    "    # Determine global min/max for consistent color scaling\n",
    "    vmin = min(gamma0_vv.min(), gamma0_vh.min())\n",
    "    vmax = max(gamma0_vv.max(), gamma0_vh.max())\n",
    "\n",
    "    # Plot Gamma0_VV\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    im1 = plt.imshow(gamma0_vv, extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "                     cmap='gray', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(im1, label='Gamma0_VV')\n",
    "    plt.title('Gamma0_VV')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Plot Gamma0_VH\n",
    "    plt.subplot(1, 2, 2)\n",
    "    im2 = plt.imshow(gamma0_vh, extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "                     cmap='gray', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(im2, label='Gamma0_VH')\n",
    "    plt.title('Gamma0_VH')\n",
    "    plt.xlabel('Longitude')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_spatial_resolution(nc_file):\n",
    "    # Open the NetCDF file\n",
    "    dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    lon = dataset.variables['lon'][:]\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "\n",
    "    # Calculate the resolution by taking the mean difference\n",
    "    lat_res = np.abs(lat[1] - lat[0])  # Latitude resolution\n",
    "    lon_res = np.abs(lon[1] - lon[0])  # Longitude resolution\n",
    "\n",
    "    print(f\"Latitude resolution: {lat_res:.6f} degrees\")\n",
    "    print(f\"Longitude resolution: {lon_res:.6f} degrees\")\n",
    "\n",
    "    # Approximate resolution in meters (assuming 1 degree â‰ˆ 111 km)\n",
    "    lat_res_m = lat_res * 111000\n",
    "    lon_res_m = lon_res * 111000 * np.cos(np.deg2rad(lat.mean()))\n",
    "\n",
    "    print(f\"Approximate pixel size: {lat_res_m:.2f} meters (latitude), {lon_res_m:.2f} meters (longitude)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in the custom Gamma0 .nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'SabineRS'\n",
    "SYSTEM = 'mac'          # or 'linux'\n",
    "\n",
    "\n",
    "if SYSTEM == 'linux':\n",
    "    grd_dir = f'/home/clay/Documents/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/10_netcdfs'\n",
    "    comp_dir =f'/home/clay/Documents/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/11_composites'\n",
    "else:\n",
    "    grd_dir = f'/Volumes/SamsungExt/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/10_netcdfs'\n",
    "    comp_dir =f'/Volumes/SamsungExt/{PROJECT}/Sentinel-1/GRD/ASCENDING/136/93/11_composites'\n",
    "\n",
    "    if isExist = os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(grd_dir) == False:\n",
    "    aoi = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_dates = sorted(file[:-3] for file in os.listdir(grd_dir) if file.endswith('.nc')) # need to test NetCDF export\n",
    "grd_files = sorted([os.path.join(grd_dir, f'{file}') for file in os.listdir(grd_dir) if file.endswith('.nc')]) # need to test NetCDF export\n",
    "comp_files = sorted([os.path.join(comp_dir, f'{file}') for file in os.listdir(comp_dir) if file.endswith('.nc')]) # need to test NetCDF export\n",
    "\n",
    "# visualize_nc_file(grd_files[0])\n",
    "# check_spatial_resolution(grd_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = extract_valid_bounds_nc_to_epsg4326(grd_files[0], 'Gamma0_VV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw a more refined aoi if you wish.\n",
    "- Will use this to filter out the cloud-masked S2 NDVI and NDWI images to only contain images with your desired percentage of remaining pixels within the aoi subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of those interactive map sequences I had\n",
    "## interactive map for you to draw a polygon to signify your aoi\n",
    "\n",
    "## Create a map centered at a specific location\n",
    "m = geemap.Map(center=[20, 0], zoom=2, basemap='HYBRID')\n",
    "m.centerObject(aoi, 8)\n",
    "\n",
    "m.addLayer(aoi)\n",
    "## Add drawing tools\n",
    "m.add_draw_control()\n",
    "## Display the map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the drawn features\n",
    "draw_features = m.draw_features[0]\n",
    "## Establish ee.Polygon from drawn area of interest to collect imagery\n",
    "subset_aoi = ee.Geometry.Polygon(draw_features.getInfo()['geometry']['coordinates'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Sentinel imagery from GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_cloud_cov = 20\n",
    "START_DATE = '2019-10-01'\n",
    "END_DATE = '2024-10-01'\n",
    "CLD_PRB_THRESH = 35\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 100\n",
    "ORBIT = 'ASCENDING'\n",
    "\n",
    "s2_coll = get_s2_sr_cld_col(aoi, START_DATE, END_DATE, s2_cloud_cov)\n",
    "\n",
    "s2_cm = (s2_coll.map(hedley_glint_correction).map(add_cld_shdw_mask).map(apply_cld_shdw_mask).map(s2_10m_target_indices)).select(['NDVI', 'NDWI'])       # cloud masked sentinel-2 10-meter ndvi and ndwi bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mosaic images with matching dates\n",
    "- Common cells for GEE Sentinel imagery typically smaller than that of ASF/Copernicus\n",
    "- Could result in multiple images for a single day\n",
    "- Iterate through the information or each of the images in the S1 and S2 collections, mosaic matching dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_date_list = s2_cm.map(get_date).aggregate_array('date').getInfo()\n",
    "unique_list = sorted(list(set(s2_date_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of mosaicked images for all dates\n",
    "mosaic_dict = {}\n",
    "for date in unique_list:\n",
    "    mosaics = mosaic_images_for_date(date, s2_cm)\n",
    "    mosaic_dict[date] = mosaics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter mosaics by percentage of pixels remaining within the subset aoi\n",
    "- either simplify the storage of the images within the moasaic_dict, or improve the below function to accomodate the storage?\n",
    "- Need to be able to determine the number of remaining pixels within the subset_aoi. If the percentage of pixels within the subset_aoi is above clear_threshold, the image will remain in the collection. If not, it will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = filter_images_by_valid_pixels(mosaic_dict, subset_aoi, 0.50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporally register Sentinel-1 .nc files with Sentinel-2 images\n",
    "- reduces amount of processing needed if Sentinel-2 images don't have a close match\n",
    "- Different relook frequencies lead to epochs on different dates. Need to match up the dates as close as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gee_dates = sorted(list(filtered_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gee_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grd_dates is 'YYYYMMDD' in format, gee_dates is 'YYYY-MM-DD'\n",
    "# reformat the grd_dates\n",
    "# check for matches, create dictionary containing the S1 .nc files and the corresponding S2 ee.Images\n",
    "\n",
    "sorted(list(set(gee_dates + grd_dates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this to visualize the Mosaics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a map\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(aoi, 12)\n",
    "\n",
    "filtered_dates = sorted(list(filtered_dict.keys()))\n",
    "for i in [0, 10, 50, 100]:\n",
    "    image = filtered_dict[filtered_dates[i]]\n",
    "    add_ind_to_map(image, Map, 'NDVI', filtered_dates[i])\n",
    "\n",
    "# Display the map\n",
    "Map.addLayerControl()  # Optional: Add a layer control panel\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otsu threshold\n",
    "- Need to upsample the GRD images to ~20-meter resolution to match with interferograms\n",
    "- Otsu on all 20-meter SAR bands\n",
    "- Otsu on NDVI and NDWI 10-meter bands\n",
    "- Majority voting with SAR, refinement with NDVI and NDWI\n",
    "- Final shoul give water mask for each registered pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_ims = {}\n",
    "ndwi_ims={}\n",
    "\n",
    "for band in ['NDVI', 'NDWI']:\n",
    "    for date in filtered_dates:\n",
    "        image = filtered_dict[filtered_dates[i]]\n",
    "        histogram = image.select(band).reduceRegion(\n",
    "        reducer=ee.Reducer.histogram(),\n",
    "        geometry=aoi,\n",
    "        scale=10,  # Adjust based on your dataset resolution\n",
    "        maxPixels=1e15\n",
    "        ).get(band)\n",
    "\n",
    "        result = otsu_gee(histogram)\n",
    "        threshold = result.getInfo()\n",
    "\n",
    "        # Create binary mask\n",
    "        binary_mask = image.select(band).gt(threshold)\n",
    "        \n",
    "    if band == 'NDVI':\n",
    "        ndvi_ims[date] = binary_mask\n",
    "    else:\n",
    "        ndwi_ims[date] = binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = geemap.Map()\n",
    "map.centerObject(aoi, 8)\n",
    "\n",
    "target_band = 'NDVI'\n",
    "\n",
    "for i in [0, 50, 100]:\n",
    "    image = filtered_dict[filtered_dates[i]]\n",
    "    histogram = image.select(target_band).reduceRegion(\n",
    "    reducer=ee.Reducer.histogram(),\n",
    "    geometry=aoi,\n",
    "    scale=10,  # Adjust based on your dataset resolution\n",
    "    maxPixels=1e15\n",
    "    ).get(target_band)\n",
    "    \n",
    "    result = otsu_gee(histogram)\n",
    "    threshold = result.getInfo()\n",
    "\n",
    "    # Create binary mask\n",
    "    binary_mask = image.select(target_band).gt(threshold)\n",
    "    add_ind_to_map(binary_mask, map, target_band, filtered_dates[i])\n",
    "\n",
    "map.addLayerControl(position='topleft')\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
