{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to form interferograms using ESA SNAP. Shifting to SNAP because of possible NASA funding cuts; might impact the support for both ISCE2 and ISCE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_help():\n",
    "\n",
    "    cmd = f'{GPT_PATH} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def help_cmd(cmdname):\n",
    "\n",
    "    cmd = f'{GPT_PATH} {cmdname} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def project_dir(work_dir):\n",
    "    \"\"\"\n",
    "    This function reads in a string that you wish to make your working directory \n",
    "    for the InSAR project, and creates a data directory to store the data for ISCE2 and mintpy\n",
    "    work-dir = str\n",
    "        path to the directory created in 01_get_slc.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    #creates file on your desktop containing the work of this notebook\n",
    "    os.makedirs(work_dir, exist_ok=True, mode=0o777)\n",
    "    \n",
    "    # file inside work_dir for isce2 interferometry\n",
    "    if_dir = os.path.join(work_dir,'InSAR/interferometry')\n",
    "    os.makedirs(if_dir, exist_ok=True, mode=0o777)\n",
    "    \n",
    "    # file inside work_dir for mintpy time-series\n",
    "    ts_dir = os.path.join(work_dir,'InSAR/time_series')\n",
    "    os.makedirs(ts_dir, exist_ok=True, mode=0o777)\n",
    "\n",
    "    return if_dir, ts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GPT path, number of processors, and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_PATH = '/home/clay/esa-snap/bin/gpt'          #linux\n",
    "GPT_PATH = '/Applications/esa-snap/bin/gpt'         #mac\n",
    "NUM_PROCESSORS = 10                                 # 24 total on my linux, 14 total on mac\n",
    "MEMORY_SIZE = '18G'                                 # 96G total on my linux, 24G total on mac\n",
    "SNAPHU_PATH = '/usr/local/bin/snaphu'               # path to snaphu for unwrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SLC images that were downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have downloaded .zip files covering your AOI from ASF Vertex\n",
    "# enter the file directory below\n",
    "slc_zips = '/Users/clayc/Documents/Dissertation/SabineRS/Sentinel-1/SLC/ASCENDING/136/93'\n",
    "\n",
    "slc_zips_list = sorted(os.listdir(slc_zips), key=lambda x: datetime.strptime(x[17:25], '%Y%m%d'))\n",
    "slc_zips_dirs = [os.path.join(slc_zips, slc) for slc in slc_zips_list]\n",
    "slc_zips_dates = [slc[17:25] for slc in slc_zips_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories for SNAP interferogram and MintPy\n",
    "Needs for MintPy are:\n",
    "1. Wrapped Ifg\n",
    "2. Elevation Band\n",
    "3. Coherence Band\n",
    "4. Unwrapped ifg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory structure much simpler than ISCE:\n",
    "1. Ifg directory\n",
    "    - stores wrapped ifg .dim and .data files (optional) \n",
    "    - stores coherence band .dim and .data files\n",
    "    - stores elevation band .dim and .data files\n",
    "    - stores unwrapped ifg .dim and .data files\n",
    "2. Referance DEM .dim and .data file\n",
    "3. MintPy directory\n",
    "    - MintPy .txt config file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '/Users/clayc/Documents/Dissertation/SabineRS'\n",
    "work_dir = os.path.join(proj_dir, 'Sentinel-1')\n",
    "if_dir, ts_dir= project_dir(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a list of triplets that will be used to form the interferograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all unique triplets from the SLC files list\n",
    "triplets = list(itertools.combinations(slc_zips_dirs, 3))\n",
    "\n",
    "# For each triplet, create the three interferogram pairs\n",
    "for trip in triplets:\n",
    "    # trip is a tuple of three SLC filenames (A, B, C)\n",
    "    A, B, C = trip\n",
    "    ifg_AB = f\"{A}, {B}\"\n",
    "    ifg_AC = f\"{A}, {C}\"\n",
    "    ifg_BC = f\"{B}, {C}\"\n",
    "    \n",
    "    print(\"Triplet:\", trip)\n",
    "    print(\"Interferogram pairs:\")\n",
    "    print(\"  \", ifg_AB)\n",
    "    print(\"  \", ifg_AC)\n",
    "    print(\"  \", ifg_BC)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which polarizations and DEM you want to use for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can define this here for the rest of the notebook\n",
    "# typically VV has highest coherence due to reliance on surface scattering\n",
    "# VH largely impacted by volumetric scattering present in vegetated areas\n",
    "\n",
    "POLARISATIONS = 'VV' \n",
    "\n",
    "# repeated a lot, so leaving this here as well\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TOPSAR-Split\n",
    "- will try to add something in here where can input an aoi and automatically identify the swaths needed\n",
    "- wkt aoi can be passed to get the bursts, but not sure if it also works for subswaths\n",
    "- ABraun recommends splitting before applying orbit file, maybe will save time?\n",
    "- do for all images\n",
    "\n",
    "- May need to just go in manually for the reference file to identify subswaths and bursts :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this, could be interesting to keep both polarisations\n",
    "# would use VV for the interferograms but could include VH for coherence time-series?\n",
    "# this uses all subswaths. Will probably take longer but it make\n",
    "\n",
    "#after manual inspection\n",
    "SUBSWATH = 'IW2'\n",
    "FIRSTBURST = 2\n",
    "LASTBURST = 6\n",
    "\n",
    "with open('/Users/clayc/Documents/Dissertation/SabineRS/wkt_aoi.txt') as f:        \n",
    "    lines = f.readlines()\n",
    "aoi = lines[0]\n",
    "\n",
    "splits_path = os.path.join(if_dir, '01_split')\n",
    "os.makedirs(splits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "split_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    split_outpaths.append(os.path.join(splits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    split_cmd = f'{GPT_PATH} TOPSAR-Split -Ssource={os.path.join(slc_zips,file)} -PselectedPolarisations={POLARISATIONS} -Psubswath={SUBSWATH} -PfirstBurstIndex={FIRSTBURST} -PlastBurstIndex={LASTBURST} -t {split_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(split_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply Orbit File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORBIT_TYPE = 'Sentinel Precise (Auto Download)'     # str, options include'Sentinel Precise (Auto Download)', 'Sentinel Restituted (Auto Download)', 'DORIS Preliminary POR (ENVISAT)', 'DORIS Precise VOR (ENVISAT) (Auto Download)', 'DELFT Precise (ENVISAT, ERS1&2) (Auto Download)', 'PRARE Precise (ERS1&2) (Auto Download)', 'Kompsat5 Precise'\n",
    "POLY_DEGREE = 3                                     # int\n",
    "CONTINUE_ON_FAIL = False\n",
    "\n",
    "orbits_path = os.path.join(if_dir, '02_orbit')\n",
    "os.makedirs(orbits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "orbit_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    orbit_outpaths.append(os.path.join(orbits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    orbit_cmd = f'{GPT_PATH} Apply-Orbit-File -Ssource={split_outpaths[i]} -PcontinueOnFail={CONTINUE_ON_FAIL} -PorbitType=\"{ORBIT_TYPE}\" -PpolyDegree={POLY_DEGREE} -t {orbit_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(orbit_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(splits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Back-geocoding Co-reg\n",
    "- this one is funky. Need to pass the two files for the ifg together?\n",
    "- pass with -SsourceProducts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_RESAMPLE_METHOD = 'BICUBIC_INTERPOLATION'       # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "RESAMPLE_METHOD = 'BISINC_5_POINT_INTERPOLATION'    # str, options inlcude 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "\n",
    "geocodes_path = os.path.join(if_dir, '03_geocode')\n",
    "os.makedirs(geocodes_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "geocode_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    geocode_outpaths.append(os.path.join(geocodes_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    geocode_cmd = f'{GPT_PATH} Back-Geocoding -PdemName={DEM_NAME} -PdemResamplingMethod={DEM_RESAMPLE_METHOD} -PresamplingType={RESAMPLE_METHOD} -t {geocode_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(geocode_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(orbits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. ESD (only if using more than one burst. which is pretty typical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERENCE_THRESHOLD = 0.3           # (0, 1]\n",
    "ESD_ESTIMATOR = 'Periodogram'       # Average or Periodogram\n",
    "\n",
    "#documentation not very good, assuming this is just resampling done before estimating ESD?\n",
    "# will look at STEP forum for more info\n",
    "WIN_ACC_AZI = 16                    # 2, 4, 8, 16, 32, 64\n",
    "WIN_ACC_RANGE = 16                  # 2, 4, 8, 16, 32, 64\n",
    "WIN_HEIGHT = 512                    # 32, 64, 128, 256, 512, 1024, 2048\n",
    "WIN_OVERSAMPLING = 128              # 32, 64, 128, 256\n",
    "WIN_WIDTH = 512                     # 32, 64, 128, 256, 512, 1024, 2048\n",
    "\n",
    "INTEGRATION_METHOD = 'L1 and L2'    # L1, L2, L1 and L2\n",
    "NUM_BLOCKS_PER_OVERLAP = 10         # [1,20]\n",
    "OVERALL_AZI_SHIFT = 0.0             \n",
    "OVERALL_RANGE_SHIFT = 0.0           \n",
    "WEIGHT_FUNCTION = 'Inv Quadratic'   # None, Linear, Quadratic, Inv Quadratic\n",
    "XCOR_THRESHOLD = 0.1                #(0, *)\n",
    "\n",
    "esds_path = os.path.join(if_dir, '04_ESD')\n",
    "os.makedirs(esds_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "esd_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    esd_outpaths.append(os.path.join(esds_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    esd_cmd = f'{GPT_PATH} Enhanced-Spectral-Diversity -Ssource={geocode_outpaths[i]} -PcohThreshold={COHERENCE_THRESHOLD} -PesdEstimator={ESD_ESTIMATOR} -PfineWinAccAzimuth={WIN_ACC_AZI} -PfineWinAccRange={WIN_ACC_RANGE} -PfineWinHeightStr={WIN_HEIGHT} -PfineWinOversampling={WIN_OVERSAMPLING} -PfineWinWidthStr={WIN_WIDTH} -PintegrationMethod=\"{INTEGRATION_METHOD}\" -PnumBlocksPerOverlap={NUM_BLOCKS_PER_OVERLAP} -PoverallAzimuthShift={OVERALL_AZI_SHIFT} PoverallRangeShift={OVERALL_RANGE_SHIFT} -PweightFunc=\"{WEIGHT_FUNCTION}\" -PxCorrThreshold={XCOR_THRESHOLD} -t {esd_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(esd_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(geocodes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Generate Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  gpt Interferogram [options] \n",
      "\n",
      "Description:\n",
      "  Compute interferograms from stack of coregistered S-1 images\n",
      "\n",
      "\n",
      "Source Options:\n",
      "  -SsourceProduct=<file>    Sets source 'sourceProduct' to <filepath>.\n",
      "                            This is a mandatory source.\n",
      "\n",
      "Parameter Options:\n",
      "  -PcohWinAz=<int>                        Size of coherence estimation window in Azimuth direction\n",
      "                                          Default value is '10'.\n",
      "  -PcohWinRg=<int>                        Size of coherence estimation window in Range direction\n",
      "                                          Default value is '10'.\n",
      "  -PdemName=<string>                      The digital elevation model.\n",
      "                                          Default value is 'SRTM 3Sec'.\n",
      "  -PexternalDEMApplyEGM=<boolean>         Sets parameter 'externalDEMApplyEGM' to <boolean>.\n",
      "                                          Default value is 'true'.\n",
      "  -PexternalDEMFile=<file>                Sets parameter 'externalDEMFile' to <file>.\n",
      "  -PexternalDEMNoDataValue=<double>       Sets parameter 'externalDEMNoDataValue' to <double>.\n",
      "                                          Default value is '0'.\n",
      "  -PincludeCoherence=<boolean>            Sets parameter 'includeCoherence' to <boolean>.\n",
      "                                          Default value is 'true'.\n",
      "  -PorbitDegree=<int>                     Degree of orbit (polynomial) interpolator\n",
      "                                          Value must be one of '1', '2', '3', '4', '5'.\n",
      "                                          Default value is '3'.\n",
      "  -PoutputElevation=<boolean>             Sets parameter 'outputElevation' to <boolean>.\n",
      "                                          Default value is 'false'.\n",
      "  -PoutputFlatEarthPhase=<boolean>        Sets parameter 'outputFlatEarthPhase' to <boolean>.\n",
      "                                          Default value is 'false'.\n",
      "  -PoutputLatLon=<boolean>                Sets parameter 'outputLatLon' to <boolean>.\n",
      "                                          Default value is 'false'.\n",
      "  -PoutputTopoPhase=<boolean>             Sets parameter 'outputTopoPhase' to <boolean>.\n",
      "                                          Default value is 'false'.\n",
      "  -PsquarePixel=<boolean>                 Use ground square pixel\n",
      "                                          Default value is 'true'.\n",
      "  -PsrpNumberPoints=<int>                 Number of points for the 'flat earth phase' polynomial estimation\n",
      "                                          Value must be one of '301', '401', '501', '601', '701', '801', '901', '1001'.\n",
      "                                          Default value is '501'.\n",
      "  -PsrpPolynomialDegree=<int>             Order of 'Flat earth phase' polynomial\n",
      "                                          Value must be one of '1', '2', '3', '4', '5', '6', '7', '8'.\n",
      "                                          Default value is '5'.\n",
      "  -PsubtractFlatEarthPhase=<boolean>      Sets parameter 'subtractFlatEarthPhase' to <boolean>.\n",
      "                                          Default value is 'true'.\n",
      "  -PsubtractTopographicPhase=<boolean>    Sets parameter 'subtractTopographicPhase' to <boolean>.\n",
      "                                          Default value is 'false'.\n",
      "  -PtileExtensionPercent=<string>         Define extension of tile for DEM simulation (optimization parameter).\n",
      "                                          Default value is '100'.\n",
      "\n",
      "Graph XML Format:\n",
      "  <graph id=\"someGraphId\">\n",
      "    <version>1.0</version>\n",
      "    <node id=\"someNodeId\">\n",
      "      <operator>Interferogram</operator>\n",
      "      <sources>\n",
      "        <sourceProduct>${sourceProduct}</sourceProduct>\n",
      "      </sources>\n",
      "      <parameters>\n",
      "        <subtractFlatEarthPhase>boolean</subtractFlatEarthPhase>\n",
      "        <srpPolynomialDegree>int</srpPolynomialDegree>\n",
      "        <srpNumberPoints>int</srpNumberPoints>\n",
      "        <orbitDegree>int</orbitDegree>\n",
      "        <includeCoherence>boolean</includeCoherence>\n",
      "        <cohWinAz>int</cohWinAz>\n",
      "        <cohWinRg>int</cohWinRg>\n",
      "        <squarePixel>boolean</squarePixel>\n",
      "        <subtractTopographicPhase>boolean</subtractTopographicPhase>\n",
      "        <demName>string</demName>\n",
      "        <externalDEMFile>file</externalDEMFile>\n",
      "        <externalDEMNoDataValue>double</externalDEMNoDataValue>\n",
      "        <externalDEMApplyEGM>boolean</externalDEMApplyEGM>\n",
      "        <tileExtensionPercent>string</tileExtensionPercent>\n",
      "        <outputFlatEarthPhase>boolean</outputFlatEarthPhase>\n",
      "        <outputTopoPhase>boolean</outputTopoPhase>\n",
      "        <outputElevation>boolean</outputElevation>\n",
      "        <outputLatLon>boolean</outputLatLon>\n",
      "      </parameters>\n",
      "    </node>\n",
      "  </graph>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help_cmd('Interferogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERENCE_AZI_WIN = 10              \n",
    "COHERENCE_RANGE_WIN = 10\n",
    "OUTPUT_ELEVATION = True                 # boolean\n",
    "OUTPUT_FLAT_EARTH_PHASE = False         # boolean\n",
    "OUTPUT_LAT_LON = False                  # boolean\n",
    "OUTPUT_TOPO_PHASE = False               # boolean\n",
    "SQUARE_PIXEL = True                     # boolean\n",
    "\n",
    "# these are for flat earth phase estimation\n",
    "FEP_NUM_POINTS = 501                    # 301, 401, 501, 601, 701, 801, 901, 1001\n",
    "FEP_POLY_DEGREE = 5                     # 1, 2, 3, 4, 5, 6, 7, 8\n",
    "\n",
    "SUBTRACT_FEP = True                     # boolean\n",
    "SUBTRACT_TOPO_PHASE = False             # boolean   \n",
    "TILE_EXTENSION_PERCENT = 100\n",
    "\n",
    "ifg_cmd = f'{GPT_PATH} Interferogram -SsourceProduct={ifg_outpaths[i]} -PcohWinAz={COHERENCE_AZI_WIN} -t {deburst_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TOPSAR-Deburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deburst_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    deburst_outpaths.append(os.path.join(grdpaths[3], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    deburst_cmd = f'{GPT_PATH} TOPSAR-Deburst -Ssource={cal_outpaths[i]} -PselectedPolarisations={POLARISATIONS} -t {deburst_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(deburst_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(cal_outpaths[i]) # removes the .dim just used source product\n",
    "    shutil.rmtree(cal_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multilook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_PIXEL = False\n",
    "AZI_LOOKS = '1'         # sentinel-1 is 22m in azimuth\n",
    "RANGE_LOOKS = '5'       # sentinel-1 is between 2.7 and 3.5m in range, depending on topography\n",
    "\n",
    "mlook_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    mlook_outpaths.append(os.path.join(grdpaths[4], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    mlook_cmd = f'{GPT_PATH} Multilook -Ssource={deburst_outpaths[i]} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {mlook_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(mlook_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(deburst_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(deburst_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Topo-phase removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Goldstein Phase Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9a. Merge (if multiple subswaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9b. Subset and extract wrapped Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Add and extract elevation band only (needed for attribute metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Add and extract coherence band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. SNAPHU Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. SNAPHU unwrap\n",
    "- use subprocess\n",
    "- CLI example: snaphu -f snaphu.conf Phase_ifg_VV_28Mar2010_02May2010.snaphu.img 5191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. SNAPHU Import => Unwrapped Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Terrain Correction for 8b, 9, 10, and 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGN_STANDARD_GRID = False\n",
    "RADIOMETRIC_NORMALIZATION = True\n",
    "AUX_FILE = 'Product Auxiliary File'   # str, options include 'Latest Auxiliary File', 'Product Auxiliary File', 'External Auxiliary File'\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'DELAUNAY_INTERPOLATION'   # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION', 'DELAUNAY_INTERPOLATION'\n",
    "IMG_RESAMPLE_METHOD = 'CUBIC_CONVOLUTION' # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "NODATA_SEA = False\n",
    "OUTPUT_COMPLEX = False\n",
    "PIXEL_SPACING_METERS = 20.0   # double\n",
    "SAVE_DEM = False\n",
    "SAVE_ANGLE_ELLIPSOID = False\n",
    "SAVE_LAT_LON = False\n",
    "SAVE_LAYOVER_SHADOW = True\n",
    "SAVE_LOCAL_ANGLE = False\n",
    "SAVE_PROJ_LOCAL_ANGLE = True\n",
    "SAVE_SOURCE_BAND = True\n",
    "SOURCE_BANDS = 'Gamma0_VV,Gamma0_VH'\n",
    "GRID_ORIGIN_X = 0   # double\n",
    "GRID_ORIGIN_Y = 0   # double\n",
    "\n",
    "# only needed if you you are using external dem\n",
    "# uncomment these and adjust your gpt command below accordingly\n",
    "#  EXTERNAL_AUX = \n",
    "# EXTERNAL_DEMFILE =\n",
    "# EXTERNAL_DEMFILE_NODATA =\n",
    "# ETERNAL_DEM_EGM = False\n",
    "\n",
    "tc_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tc_outpaths.append(os.path.join(grdpaths[8], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    tc_cmd = f'{GPT_PATH} Terrain-Correction -Ssource={tf_outpaths[i]} -PalignToStandardGrid={ALIGN_STANDARD_GRID} -PapplyRadiometricNormalization={RADIOMETRIC_NORMALIZATION} -PauxFile=\"{AUX_FILE}\" -PdemName=\"{DEM_NAME}\" -PdemResamplingMethod=\"{DEM_RESAMPLE_METHOD}\" -PimgResamplingMethod=\"{IMG_RESAMPLE_METHOD}\" -PnodataValueAtSea={NODATA_SEA} -PoutputComplex={OUTPUT_COMPLEX} -PpixelSpacingInMeter={PIXEL_SPACING_METERS} -PsaveDEM={SAVE_DEM} -PsaveIncidenceAngleFromEllipsoid={SAVE_ANGLE_ELLIPSOID} -PsaveLatLon={SAVE_LAT_LON} -PsaveLayoverShadowMask={SAVE_LAYOVER_SHADOW} -PsaveLocalIncidenceAngle={SAVE_LOCAL_ANGLE} -PsaveProjectedLocalIncidenceAngle={SAVE_PROJ_LOCAL_ANGLE} -PsaveSelectedSourceBand={SAVE_SOURCE_BAND} -PsourceBands={SOURCE_BANDS} -PstandardGridOriginX={GRID_ORIGIN_X} -PstandardGridOriginY={GRID_ORIGIN_Y} -t {tc_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(tc_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "#     os.remove(tf_outpaths[i]) # removes the just used source product\n",
    "#     shutil.rmtree(tf_outpaths[i][:-4]+'.data')\n",
    "\n",
    "# shutil.rmtree(grdpaths[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Final subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_METADATA = True\n",
    "FULL_SWATH = False\n",
    "REFERNCE_BAND = 'Beta0_VV'\n",
    "\n",
    "subset_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    subset_outpaths.append(os.path.join(grdpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    subset_cmd = f'{GPT_PATH} Subset -Ssource={mlook_outpaths[i]} -PcopyMetadata={COPY_METADATA} -PfullSwath={FULL_SWATH} -PgeoRegion=\"{aoi}\" -PreferenceBand={REFERNCE_BAND} -t {subset_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(mlook_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(mlook_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
