{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to form interferograms using ESA SNAP. Shifting to SNAP because of possible NASA funding cuts; might impact the support for both ISCE2 and ISCE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_help():\n",
    "\n",
    "    cmd = f'{GPT_PATH} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def help_cmd(cmdname):\n",
    "\n",
    "    cmd = f'{GPT_PATH} {cmdname} -h -c {MEMORY_SIZE}'\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "def project_dir(work_dir):\n",
    "    \"\"\"\n",
    "    This function reads in a string that you wish to make your working directory \n",
    "    for the InSAR project, and creates a data directory to store the data for ISCE2 and mintpy\n",
    "    work-dir = str\n",
    "        path to the directory created in 01_get_slc.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    #creates file on your desktop containing the work of this notebook\n",
    "    os.makedirs(work_dir, exist_ok=True, mode=0o777)\n",
    "    \n",
    "    # file inside work_dir for isce2 interferometry\n",
    "    if_dir = os.path.join(work_dir,'InSAR/interferometry')\n",
    "    os.makedirs(if_dir, exist_ok=True, mode=0o777)\n",
    "    \n",
    "    # file inside work_dir for mintpy time-series\n",
    "    ts_dir = os.path.join(work_dir,'InSAR/time_series')\n",
    "    os.makedirs(ts_dir, exist_ok=True, mode=0o777)\n",
    "\n",
    "    return if_dir, ts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish GPT path, number of processors, and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_PATH = '/home/clay/esa-snap/bin/gpt'          #linux\n",
    "GPT_PATH = '/Applications/esa-snap/bin/gpt'         #mac\n",
    "NUM_PROCESSORS = 10                                 # 24 total on my linux, 14 total on mac\n",
    "MEMORY_SIZE = '18G'                                 # 96G total on my linux, 24G total on mac\n",
    "SNAPHU_PATH = '/usr/local/bin/snaphu'               # path to snaphu for unwrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SLC images that were downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have downloaded .zip files covering your AOI from ASF Vertex\n",
    "# enter the file directory below\n",
    "slc_zips = '/Users/clayc/Documents/Dissertation/SabineRS/Sentinel-1/SLC/ASCENDING/136/93'\n",
    "\n",
    "slc_zips_list = sorted(os.listdir(slc_zips), key=lambda x: datetime.strptime(x[17:25], '%Y%m%d'))\n",
    "slc_zips_dirs = [os.path.join(slc_zips, slc) for slc in slc_zips_list]\n",
    "slc_zips_dates = [slc[17:25] for slc in slc_zips_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories for SNAP interferogram and MintPy\n",
    "Needs for MintPy are:\n",
    "1. Wrapped Ifg\n",
    "2. Elevation Band\n",
    "3. Coherence Band\n",
    "4. Unwrapped ifg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory structure much simpler than ISCE:\n",
    "1. Ifg directory\n",
    "    - stores wrapped ifg .dim and .data files (optional) \n",
    "    - stores coherence band .dim and .data files\n",
    "    - stores elevation band .dim and .data files\n",
    "    - stores unwrapped ifg .dim and .data files\n",
    "2. Referance DEM .dim and .data file\n",
    "3. MintPy directory\n",
    "    - MintPy .txt config file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '/Users/clayc/Documents/Dissertation/SabineRS'\n",
    "work_dir = os.path.join(proj_dir, 'Sentinel-1')\n",
    "if_dir, ts_dir= project_dir(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a list of triplets that will be used to form the interferograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all unique triplets from the SLC files list\n",
    "triplets = list(itertools.combinations(slc_zips_dirs, 3))\n",
    "\n",
    "# For each triplet, create the three interferogram pairs\n",
    "for trip in triplets:\n",
    "    # trip is a tuple of three SLC filenames (A, B, C)\n",
    "    A, B, C = trip\n",
    "    ifg_AB = f\"{A}, {B}\"\n",
    "    ifg_AC = f\"{A}, {C}\"\n",
    "    ifg_BC = f\"{B}, {C}\"\n",
    "    \n",
    "    print(\"Triplet:\", trip)\n",
    "    print(\"Interferogram pairs:\")\n",
    "    print(\"  \", ifg_AB)\n",
    "    print(\"  \", ifg_AC)\n",
    "    print(\"  \", ifg_BC)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which polarizations you want to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can define this here for the rest of the notebook\n",
    "# typically VV has highest coherence due to reliance on surface scattering\n",
    "# VH largely impacted by volumetric scattering present in vegetated areas\n",
    "\n",
    "POLARISATIONS = 'VV' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TOPSAR-Split\n",
    "- will try to add something in here where can input an aoi and automatically identify the swaths needed\n",
    "- wkt aoi can be passed to get the bursts, but not sure if it also works for subswaths\n",
    "- ABraun recommends splitting before applying orbit file, maybe will save time?\n",
    "- do for all images\n",
    "\n",
    "- May need to just go in manually for the reference file to identify subswaths and bursts :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this, could be interesting to keep both polarisations\n",
    "# would use VV for the interferograms but could include VH for coherence time-series?\n",
    "# this uses all subswaths. Will probably take longer but it make\n",
    "\n",
    "#after manual inspection\n",
    "SUBSWATH = 'IW2'\n",
    "FIRSTBURST = 2\n",
    "LASTBURST = 6\n",
    "\n",
    "with open('/Users/clayc/Documents/Dissertation/SabineRS/wkt_aoi.txt') as f:        \n",
    "    lines = f.readlines()\n",
    "aoi = lines[0]\n",
    "\n",
    "splits_path = os.path.join(if_dir, '01_split')\n",
    "os.makedirs(splits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "split_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    split_outpaths.append(os.path.join(splits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    split_cmd = f'{GPT_PATH} TOPSAR-Split -Ssource={os.path.join(slc_zips,file)} -PselectedPolarisations={POLARISATIONS} -Psubswath={SUBSWATH} -PfirstBurstIndex={FIRSTBURST} -PlastBurstIndex={LASTBURST} -t {split_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(split_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply Orbit File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORBIT_TYPE = 'Sentinel Precise (Auto Download)'     # str, options include'Sentinel Precise (Auto Download)', 'Sentinel Restituted (Auto Download)', 'DORIS Preliminary POR (ENVISAT)', 'DORIS Precise VOR (ENVISAT) (Auto Download)', 'DELFT Precise (ENVISAT, ERS1&2) (Auto Download)', 'PRARE Precise (ERS1&2) (Auto Download)', 'Kompsat5 Precise'\n",
    "POLY_DEGREE = 3                                     # int\n",
    "CONTINUE_ON_FAIL = False\n",
    "\n",
    "orbits_path = os.path.join(if_dir, '02_orbit')\n",
    "os.makedirs(orbits_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "orbit_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    orbit_outpaths.append(os.path.join(orbits_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    orbit_cmd = f'{GPT_PATH} Apply-Orbit-File -Ssource={split_outpaths[i]} -PcontinueOnFail={CONTINUE_ON_FAIL} -PorbitType=\"{ORBIT_TYPE}\" -PpolyDegree={POLY_DEGREE} -t {orbit_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(orbit_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(splits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Back-geocoding Co-reg\n",
    "- this one is funky. Need to pass the two files for the ifg together?\n",
    "- pass with -SsourceProducts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'BICUBIC_INTERPOLATION'       # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "RESAMPLE_METHOD = 'BISINC_5_POINT_INTERPOLATION'    # str, options inlcude 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "\n",
    "geocodes_path = os.path.join(if_dir, '03_geocode')\n",
    "os.makedirs(geocodes_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "geocode_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    geocode_outpaths.append(os.path.join(geocodes_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    geocode_cmd = f'{GPT_PATH} Back-Geocoding -PdemName={DEM_NAME} -PdemResamplingMethod={DEM_RESAMPLE_METHOD} -PresamplingType={RESAMPLE_METHOD} -t {geocode_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(geocode_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(orbits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. ESD (only if using more than one burst. which is pretty typical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_cmd('Back-Geocoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'BICUBIC_INTERPOLATION'       # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "RESAMPLE_METHOD = 'BISINC_5_POINT_INTERPOLATION'    # str, options inlcude 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "\n",
    "esds_path = os.path.join(if_dir, '04_ESD')\n",
    "os.makedirs(esds_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "esd_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    esd_outpaths.append(os.path.join(esds_path, f'{slc_zips_dates[i]}.dim'))\n",
    "    esd_cmd = f'{GPT_PATH} Enhanced-Spectral-Diversity -PdemName={DEM_NAME} -PdemResamplingMethod={DEM_RESAMPLE_METHOD} -PresamplingType={RESAMPLE_METHOD} -t {esd_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(esd_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "shutil.rmtree(orbits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Generate Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TOPSAR-Deburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deburst_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    deburst_outpaths.append(os.path.join(grdpaths[3], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    deburst_cmd = f'{GPT_PATH} TOPSAR-Deburst -Ssource={cal_outpaths[i]} -PselectedPolarisations={POLARISATIONS} -t {deburst_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(deburst_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(cal_outpaths[i]) # removes the .dim just used source product\n",
    "    shutil.rmtree(cal_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multilook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_PIXEL = False\n",
    "AZI_LOOKS = '1'         # sentinel-1 is 22m in azimuth\n",
    "RANGE_LOOKS = '5'       # sentinel-1 is between 2.7 and 3.5m in range, depending on topography\n",
    "\n",
    "mlook_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    mlook_outpaths.append(os.path.join(grdpaths[4], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    mlook_cmd = f'{GPT_PATH} Multilook -Ssource={deburst_outpaths[i]} -PgrSquarePixel={SQUARE_PIXEL} -PnAzLooks={AZI_LOOKS} -PnRgLooks={RANGE_LOOKS} -t {mlook_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(mlook_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(deburst_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(deburst_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Topo-phase removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Goldstein Phase Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9a. Merge (if multiple subswaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9b. Subset and extract wrapped Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Add and extract elevation band only (needed for attribute metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Add and extract coherence band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. SNAPHU Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. SNAPHU unwrap\n",
    "- use subprocess\n",
    "- CLI example: snaphu -f snaphu.conf Phase_ifg_VV_28Mar2010_02May2010.snaphu.img 5191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. SNAPHU Import => Unwrapped Ifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Terrain Correction for 8b, 9, 10, and 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGN_STANDARD_GRID = False\n",
    "RADIOMETRIC_NORMALIZATION = True\n",
    "AUX_FILE = 'Product Auxiliary File'   # str, options include 'Latest Auxiliary File', 'Product Auxiliary File', 'External Auxiliary File'\n",
    "DEM_NAME = 'Copernicus 30m Global DEM'\n",
    "DEM_RESAMPLE_METHOD = 'DELAUNAY_INTERPOLATION'   # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION', 'DELAUNAY_INTERPOLATION'\n",
    "IMG_RESAMPLE_METHOD = 'CUBIC_CONVOLUTION' # str, options include 'NEAREST_NEIGHBOUR', 'BILINEAR_INTERPOLATION', 'CUBIC_CONVOLUTION', 'BISINC_5_POINT_INTERPOLATION', 'BISINC_11_POINT_INTERPOLATION', 'BISINC_21_POINT_INTERPOLATION', 'BICUBIC_INTERPOLATION'\n",
    "NODATA_SEA = False\n",
    "OUTPUT_COMPLEX = False\n",
    "PIXEL_SPACING_METERS = 20.0   # double\n",
    "SAVE_DEM = False\n",
    "SAVE_ANGLE_ELLIPSOID = False\n",
    "SAVE_LAT_LON = False\n",
    "SAVE_LAYOVER_SHADOW = True\n",
    "SAVE_LOCAL_ANGLE = False\n",
    "SAVE_PROJ_LOCAL_ANGLE = True\n",
    "SAVE_SOURCE_BAND = True\n",
    "SOURCE_BANDS = 'Gamma0_VV,Gamma0_VH'\n",
    "GRID_ORIGIN_X = 0   # double\n",
    "GRID_ORIGIN_Y = 0   # double\n",
    "\n",
    "# only needed if you you are using external dem\n",
    "# uncomment these and adjust your gpt command below accordingly\n",
    "#  EXTERNAL_AUX = \n",
    "# EXTERNAL_DEMFILE =\n",
    "# EXTERNAL_DEMFILE_NODATA =\n",
    "# ETERNAL_DEM_EGM = False\n",
    "\n",
    "tc_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    tc_outpaths.append(os.path.join(grdpaths[8], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    tc_cmd = f'{GPT_PATH} Terrain-Correction -Ssource={tf_outpaths[i]} -PalignToStandardGrid={ALIGN_STANDARD_GRID} -PapplyRadiometricNormalization={RADIOMETRIC_NORMALIZATION} -PauxFile=\"{AUX_FILE}\" -PdemName=\"{DEM_NAME}\" -PdemResamplingMethod=\"{DEM_RESAMPLE_METHOD}\" -PimgResamplingMethod=\"{IMG_RESAMPLE_METHOD}\" -PnodataValueAtSea={NODATA_SEA} -PoutputComplex={OUTPUT_COMPLEX} -PpixelSpacingInMeter={PIXEL_SPACING_METERS} -PsaveDEM={SAVE_DEM} -PsaveIncidenceAngleFromEllipsoid={SAVE_ANGLE_ELLIPSOID} -PsaveLatLon={SAVE_LAT_LON} -PsaveLayoverShadowMask={SAVE_LAYOVER_SHADOW} -PsaveLocalIncidenceAngle={SAVE_LOCAL_ANGLE} -PsaveProjectedLocalIncidenceAngle={SAVE_PROJ_LOCAL_ANGLE} -PsaveSelectedSourceBand={SAVE_SOURCE_BAND} -PsourceBands={SOURCE_BANDS} -PstandardGridOriginX={GRID_ORIGIN_X} -PstandardGridOriginY={GRID_ORIGIN_Y} -t {tc_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(tc_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "#     os.remove(tf_outpaths[i]) # removes the just used source product\n",
    "#     shutil.rmtree(tf_outpaths[i][:-4]+'.data')\n",
    "\n",
    "# shutil.rmtree(grdpaths[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Final subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_METADATA = True\n",
    "FULL_SWATH = False\n",
    "REFERNCE_BAND = 'Beta0_VV'\n",
    "\n",
    "subset_outpaths = []\n",
    "for i, file in enumerate(slc_zips_list):\n",
    "    subset_outpaths.append(os.path.join(grdpaths[5], f'{slc_zips_dates[i]}.dim'))\n",
    "\n",
    "    subset_cmd = f'{GPT_PATH} Subset -Ssource={mlook_outpaths[i]} -PcopyMetadata={COPY_METADATA} -PfullSwath={FULL_SWATH} -PgeoRegion=\"{aoi}\" -PreferenceBand={REFERNCE_BAND} -t {subset_outpaths[i]} -c {MEMORY_SIZE} -q {NUM_PROCESSORS}'\n",
    "    try:\n",
    "        result = subprocess.run(subset_cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command failed: {e.stderr.decode()}\")\n",
    "\n",
    "    os.remove(mlook_outpaths[i]) # removes the just used source product\n",
    "    shutil.rmtree(mlook_outpaths[i][:-4]+'.data')\n",
    "\n",
    "shutil.rmtree(grdpaths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
