{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New notebook. The following steps will be taken:\n",
    "1. Manually download eHydro time cubes for the target USACE district from the CIRP website. https://cirp.usace.army.mil/products/csat_districts.php\n",
    "2. Filter all the time cubes to dates after first light of S2A (2015-06-27).\n",
    "3. Resample the surveys to 10-meter spatial resolution, matching that of the S2 L2W.nc products from ACOLITE.\n",
    "4. Use ESA OData and OpenSearch to retrieve the needed .SAFE files.\n",
    "5. Use ACOLITE to process the acquired .SAFE files, output the merged L2W.nc files\n",
    "6. Create similar time cubes for the S2 data, making sure to clip to only non-cloudy pixels lying within the survey extents. Will need to reproject this data to the appropriate crs.\n",
    "7. Save all data to an appropriate directory for use when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "# Data handling and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import Polygon\n",
    "import asf_search as asf\n",
    "import ee\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# Sentinel Hub\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "        \"scope\": \"openid\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "        )    \n",
    "    \n",
    "    return r.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file continaing username and password for copernicus browser, as well as the client id and secret for sentinelhub\n",
    "# you gotta make your own, too lazy to keep typing in my info\n",
    "\n",
    "with open('/home/clay/Desktop/s2_login_stuff.txt') as f:        \n",
    "    lines = f.readlines()\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = lines[0][:-1]\n",
    "config.sh_client_secret = lines[1][:-1]\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usace_code = 'CESWG'\n",
    "BATHY_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy'        # directory where the bathymetry data was downloaded\n",
    "S2_PATH = f'/home/clay/Documents/SDB/{usace_code}/s2_SAFE'\n",
    "os.makedirs(S2_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the downloaded bathy time cubes to only contain surveys after S2A first light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going to filter by years instead of S2Date, hopefully will save some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2A_DATE = '2015-06-27'\n",
    "FILTERED_BATHY_PATH = os.path.join(os.path.dirname(BATHY_PATH), 'bathy_filtered')\n",
    "os.makedirs(FILTERED_BATHY_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split .nc files by year, makes storage a bit easier.\n",
    "# will also allow for training models at different dates\n",
    "# can potentially use some landsat for 15 to 30m historical studies as well\n",
    "\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    if year == '2015':\n",
    "        start_dt = datetime.strptime(f'{year}-06-27', '%Y-%m-%d')\n",
    "    else:\n",
    "        start_dt = datetime.strptime(f'{year}-01-01', '%Y-%m-%d')\n",
    "\n",
    "    if year == '2025':\n",
    "        end_dt = datetime.strptime(datetime.now().strftime('%Y-%m-%d'), '%Y-%m-%d')\n",
    "    else:\n",
    "        end_dt = datetime.strptime(f'{year}-12-31', '%Y-%m-%d')\n",
    "\n",
    "    start_num = float(start_dt.strftime('%Y%m%d'))\n",
    "    end_num = float(end_dt.strftime('%Y%m%d'))\n",
    "\n",
    "    os.makedirs(os.path.join(FILTERED_BATHY_PATH, year), exist_ok=True)\n",
    "    \n",
    "    for path in [os.path.join(BATHY_PATH, f) for f in os.listdir(BATHY_PATH) if f.endswith('.nc')]:\n",
    "        ds = xr.open_dataset(path, chunks='auto')\n",
    "\n",
    "        # filtered_ds = ds.sel(time=slice(start_num, end_num))\n",
    "        filtered_ds = ds.sel(time=(ds.time > start_num) & (ds.time < end_num))\n",
    "\n",
    "        ds.close()\n",
    "        filtered_ds.to_netcdf(path.replace('bathy', f'bathy_filtered/{year}'))\n",
    "        filtered_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
