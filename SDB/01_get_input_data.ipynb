{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New notebook. The following steps will be taken:\n",
    "1. Manually download eHydro time cubes for the target USACE district from the CIRP website. https://cirp.usace.army.mil/products/csat_districts.php\n",
    "2. Filter all the time cubes to dates after first light of S2A (2015-06-27).\n",
    "3. Resample the surveys to 10-meter spatial resolution, matching that of the S2 L2W.nc products from ACOLITE.\n",
    "4. Use ESA OData and OpenSearch to retrieve the needed .SAFE files.\n",
    "5. Use ACOLITE to process the acquired .SAFE files, output the merged L2W.nc files\n",
    "6. Create similar time cubes for the S2 data, making sure to clip to only non-cloudy pixels lying within the survey extents. Will need to reproject this data to the appropriate crs.\n",
    "7. Save all data to an appropriate directory for use when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from scipy import interpolate\n",
    "\n",
    "# Data handling and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import nctoolkit as nc\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import Polygon\n",
    "import asf_search as asf\n",
    "import ee\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# Sentinel Hub\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usace_code = 'CESWG'\n",
    "BATHY_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy'        # directory where the bathymetry data was downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the downloaded bathy time cubes to only contain surveys after S2A first light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going to filter by years instead of S2Date, hopefully will save some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2A_DATE = '2015-06-27'\n",
    "FILTERED_BATHY_PATH = os.path.join(os.path.dirname(BATHY_PATH), 'bathy_filtered')\n",
    "os.makedirs(FILTERED_BATHY_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split .nc files by year, makes storage a bit easier.\n",
    "# will also allow for training models at different dates\n",
    "# can potentially use some landsat for 15 to 30m historical studies as well\n",
    "# probably add something in here to update the metadata attributes\n",
    "\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    if year == '2015':\n",
    "        start_dt = datetime.strptime(f'{year}-06-27', '%Y-%m-%d')\n",
    "    else:\n",
    "        start_dt = datetime.strptime(f'{year}-01-01', '%Y-%m-%d')\n",
    "\n",
    "    if year == '2025':\n",
    "        end_dt = datetime.strptime(datetime.now().strftime('%Y-%m-%d'), '%Y-%m-%d')\n",
    "    else:\n",
    "        end_dt = datetime.strptime(f'{year}-12-31', '%Y-%m-%d')\n",
    "\n",
    "    start_num = float(start_dt.strftime('%Y%m%d'))\n",
    "    end_num = float(end_dt.strftime('%Y%m%d'))\n",
    "\n",
    "    os.makedirs(os.path.join(FILTERED_BATHY_PATH, year), exist_ok=True)\n",
    "    \n",
    "    for path in [os.path.join(BATHY_PATH, f) for f in os.listdir(BATHY_PATH) if f.endswith('.nc')]:\n",
    "        ds = xr.open_dataset(path, chunks='auto')\n",
    "\n",
    "        # filtered_ds = ds.sel(time=slice(start_num, end_num))\n",
    "        filtered_ds = ds.sel(time=(ds.time > start_num) & (ds.time < end_num))\n",
    "\n",
    "        ds.close()\n",
    "        filtered_ds.to_netcdf(path.replace('bathy', f'bathy_filtered/{year}'))\n",
    "        filtered_ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample the filtered time cubes from 10ft spatial resolution to 10meter to match S2 files\n",
    "- nctoolkit seems useful\n",
    "- I think the setup for this .nc files are messy and can't be loaded by nctoolkit. Will check the CSAT code to see how they extract the data. If that's not promising, just gonna go back to my original workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESAMPLED_BATHY_PATH = os.path.join(os.path.dirname(FILTERED_BATHY_PATH), 'bathy_resampled')\n",
    "os.makedirs(RESAMPLED_BATHY_PATH, exist_ok=True)\n",
    "\n",
    "TARGETYEAR = '2024'\n",
    "os.makedirs(os.path.join(RESAMPLED_BATHY_PATH, TARGETYEAR), exist_ok=True)\n",
    "\n",
    "test = [os.path.join(FILTERED_BATHY_PATH, TARGETYEAR, f) for f in os.listdir(os.path.join(FILTERED_BATHY_PATH, TARGETYEAR)) if f.endswith('.nc')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset\n",
    "ds = xr.open_dataset(test)  # Replace with actual filename\n",
    "\n",
    "# Resample dataset using coarsen\n",
    "scale_factor = int(32.8084 / 10)  # Convert 10ft to 10m resolution\n",
    "ds_resampled = ds.coarsen(points=scale_factor, boundary=\"trim\").mean()\n",
    "\n",
    "# Extract first time entry\n",
    "time_index = 0  # First time step\n",
    "\n",
    "# Get original and resampled data\n",
    "original_elevations = ds.elevations.isel(time=time_index)\n",
    "resampled_elevations = ds_resampled.elevations.isel(time=time_index)\n",
    "\n",
    "# Get corresponding lat/lon\n",
    "lat_original = ds.latitudes\n",
    "lon_original = ds.longitudes\n",
    "\n",
    "lat_resampled = ds_resampled.latitudes\n",
    "lon_resampled = ds_resampled.longitudes\n",
    "\n",
    "# Plot the original data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(lon_original, lat_original, c=original_elevations, cmap=\"viridis\", s=1)\n",
    "plt.colorbar(label=\"Elevation (m)\")\n",
    "plt.title(\"Original Data (10ft resolution)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Plot the resampled data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(lon_resampled, lat_resampled, c=resampled_elevations, cmap=\"viridis\", s=5)\n",
    "plt.colorbar(label=\"Elevation (m)\")\n",
    "plt.title(\"Resampled Data (10m resolution)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 3 surveys from the ds\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Extract survey IDs and dates for titles\n",
    "# survey_ids = ds.variables['surveyId'][:3]\n",
    "survey_dates = ds.variables['time'][:3]\n",
    "\n",
    "# Plot each of the first 3 surveys\n",
    "for i in range(3):\n",
    "    # Get elevation data for this time step\n",
    "    elevation_data = ds.variables['elevations'][i]\n",
    "    \n",
    "    # Create scatter plot using points\n",
    "    scatter = axes[i].scatter(\n",
    "        ds.variables['longitudes'][:],\n",
    "        ds.variables['latitudes'][:],\n",
    "        c=elevation_data,\n",
    "        cmap='viridis',\n",
    "        s=1\n",
    "    )\n",
    "    \n",
    "    # Add colorbar and title\n",
    "    plt.colorbar(scatter, ax=axes[i], label='Elevation')\n",
    "    axes[i].set_title(f'Survey Date: {survey_dates[i]}')\n",
    "    axes[i].set_xlabel('Longitude')\n",
    "    axes[i].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
