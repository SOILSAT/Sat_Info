{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to leverage asf_search to retrieve Sentinel-2 .SAFE images that correspond to the eHydro hydrographic surveys. These .SAFE files will then be fed into ACOLITE for the needed preprocessing. Once preprocessed, the images for the hydrographic surveys and the Sentinel-2 images will be fed into 02_data_prep.ipynb to ensure the same area coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import asf_search as asf\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file continaing username and password for copernicus browser, as well as the client id and secret for sentinelhub\n",
    "# you gotta make your own, too lazy to keep typing in my info\n",
    "\n",
    "with open('/home/clay/Desktop/s2_login_stuff.txt') as f:        \n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "        \"scope\": \"openid\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "        )\n",
    "    return r.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig()\n",
    "config.sh_client_id = lines[0][:-1]\n",
    "config.sh_client_secret = lines[1][:-1]\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bathy_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        bathy = src.read(1)\n",
    "        xmin, ymin, xmax, ymax = src.bounds\n",
    "    \n",
    "    plt.imshow(\n",
    "        bathy,\n",
    "        extent=(xmin, xmax, ymin, ymax),\n",
    "        origin=\"lower\",\n",
    "        cmap=\"viridis\"\n",
    "    )\n",
    "    plt.colorbar(label=\"Depth (Feet)\")\n",
    "    plt.title(\"Rasterized Bathymetry\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "\n",
    "def extract_date(filepath):\n",
    "    \"\"\"extract search date window from the eHydro data\"\"\"\n",
    "    match = re.search(r'\\d{4}\\d{2}\\d{2}', filepath)\n",
    "    date = datetime.strptime(match.group(), '%Y%m%d')\n",
    "    return (date - timedelta(days=1)).strftime('%Y-%m-%d'), (date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "def extract_valid_bounds_to_epsg4326(raster_path):\n",
    "    \"\"\"Extracts the bounding box of valid (non-NaN) data from a raster and converts it to EPSG:4326.\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read the raster data\n",
    "        data = src.read(1)  # Assuming a single band\n",
    "        transform = src.transform  # Affine transform of the raster\n",
    "        src_crs = src.crs  # Source CRS of the raster\n",
    "\n",
    "        # Create a mask for valid (non-NaN) pixels\n",
    "        valid_mask = ~np.isnan(data)\n",
    "\n",
    "        # Find the row and column indices of valid pixels\n",
    "        valid_rows, valid_cols = np.where(valid_mask)\n",
    "\n",
    "        if valid_rows.size == 0 or valid_cols.size == 0:\n",
    "            raise ValueError(\"No valid data in the raster.\")\n",
    "\n",
    "        # Calculate the geographic coordinates of the valid bounds\n",
    "        min_row, max_row = valid_rows.min(), valid_rows.max()\n",
    "        min_col, max_col = valid_cols.min(), valid_cols.max()\n",
    "\n",
    "        # Use the transform to convert row/col to geographic bounds\n",
    "        min_x, min_y = rasterio.transform.xy(transform, min_row, min_col, offset=\"ul\")\n",
    "        max_x, max_y = rasterio.transform.xy(transform, max_row, max_col, offset=\"ul\")\n",
    "\n",
    "        # Bounds in the source CRS\n",
    "        bounds_src_crs = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "        # Transform bounds to EPSG:4326\n",
    "        transformer = Transformer.from_crs(src_crs, \"EPSG:4326\", always_xy=True)\n",
    "        min_x_4326, min_y_4326 = transformer.transform(min_x, min_y)\n",
    "        max_x_4326, max_y_4326 = transformer.transform(max_x, max_y)\n",
    "\n",
    "        bounds_epsg4326 = (min_x_4326, min_y_4326, max_x_4326, max_y_4326)\n",
    "\n",
    "        # Create polygon coordinates in clockwise order starting from top-left\n",
    "        coords = [\n",
    "            (min_x_4326, max_y_4326),  # top-left\n",
    "            (max_x_4326, max_y_4326),  # top-right\n",
    "            (max_x_4326, min_y_4326),  # bottom-right\n",
    "            (min_x_4326, min_y_4326),  # bottom-left\n",
    "            (min_x_4326, max_y_4326)   # back to top-left to close the polygon\n",
    "        ]\n",
    "    \n",
    "        # Format coordinates into WKT string\n",
    "        coord_str = ','.join([f'{x} {y}' for x, y in coords])\n",
    "        wkt = f'POLYGON(({coord_str}))'\n",
    "    \n",
    "        # bbox = ee.Geometry.BBox(bounds_epsg4326[0], bounds_epsg4326[1], bounds_epsg4326[2], bounds_epsg4326[3])\n",
    "\n",
    "    return bounds_epsg4326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usace_code = 'CESWG'\n",
    "BATHY_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy_rasters'        # STORAGE_DIR from 01a_get_ehydro.ipynb\n",
    "S2_PATH = f'/home/clay/Documents/SDB/{usace_code}/s2_SAFE'\n",
    "os.makedirs(S2_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for appropriate Sentinel-2 L1C .SAFE files\n",
    "- .SAFE needed for input into ACOLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = get_access_token(lines[-2][:-1], lines[-1][:-1])\n",
    "data_collection = 'SENTINEL-2'\n",
    "\n",
    "survey_info = {}\n",
    "for survey_name in surveynames:\n",
    "\n",
    "    raster = os.path.join(BATHY_PATH, f\"{survey_name}.tif\")\n",
    "    date = extract_date(raster)\n",
    "    bounds = extract_valid_bounds_to_epsg4326(raster)\n",
    "\n",
    "    json = requests.get(f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{bounds}') and ContentDate/Start gt {date[0]}T00:00:00.000Z and ContentDate/Start lt {date[1]}T00:00:00.000Z\").json()\n",
    "    results=pd.DataFrame.from_dict(json['value'])\n",
    "\n",
    "    if len(results) != 0:\n",
    "        urls = []\n",
    "        s2_names = []\n",
    "        for s2_name in list(results.Name):\n",
    "            if 'L1C' in s2_name:\n",
    "                urls.append(f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({(results[results.Name == s2_name]['Id'].values[0])})/$value\")\n",
    "                s2_names.append(s2_name)\n",
    "        survey_info[survey_name] = (urls, s2_names)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Copernicus Hub for Sentinel-2 L1C .SAFE files\n",
    "- iterate by eHydro name. Create a folder for each survey\n",
    "- store all .SAFE files in designated survey folder\n",
    "- .SAFE files named appropriately as stored in .items\n",
    "- Mosaic together during ACOLITE processing, or in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no worky ones:\n",
    "- S2A_MSIL1C_20221025T165401_N0510_R026_T15RTN_20240728T042150.SAFE\n",
    "- S2A_MSIL1C_20230413T164841_N0509_R026_T15RUN_20230413T220740.SAFE\n",
    "- S2A_MSIL1C_20230413T164841_N0510_R026_T15RUN_20240824T151650.SAFE\n",
    "- S2B_MSIL1C_20230518T164849_N0509_R026_T15RVP_20230518T220853.SAFE\n",
    "- S2B_MSIL1C_20230518T164849_N0509_R026_T15RUP_20230518T220853.SAFE\n",
    "- S2A_MSIL1C_20230304T165201_N0510_R026_T15RUP_20240819T071321.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RUP_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RTN_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RUN_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20221013T170239_N0400_R069_T15RTN_20221013T215905.SAFE\n",
    "- S2A_MSIL1C_20200501T165901_N0500_R069_T14RQT_20230330T120915.SAFE\n",
    "- S2A_MSIL1C_20200501T165901_N0500_R069_T15RTM_20230330T120915.SAFE\n",
    "- S2A_MSIL1C_20230722T164901_N0509_R026_T15RUN_20230722T215211.SAFE\n",
    "- S2B_MSIL1C_20191218T170719_N0500_R069_T14RQS_20230607T033311.SAFE\n",
    "- S2B_MSIL1C_20191218T170719_N0500_R069_T15RTM_20230607T033311.SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Main progress bar for surveys\n",
    "for key, items in tqdm(survey_info.items(), desc=\"Processing surveys\"):\n",
    "    if len(items[0]) == 0:\n",
    "        continue\n",
    "        \n",
    "    os.makedirs(os.path.join(S2_PATH, key), exist_ok=True)\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    access_token = get_access_token(lines[-2][:-1], lines[-1][:-1])\n",
    "    \n",
    "    # Process each file within the survey\n",
    "    for url, s2_name in zip(items[0], items[1]):\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            file_path = os.path.join(S2_PATH, f\"{key}/{s2_name[:-5]}.zip\")\n",
    "            \n",
    "            # Progress bar for individual file download\n",
    "            with tqdm(\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                desc=f\"Downloading {s2_name}\",\n",
    "                leave=True  # Keep the progress bar after completion\n",
    "            ) as pbar:\n",
    "                with open(file_path, \"wb\") as file:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {s2_name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the files\n",
    "# you may need to use sudo apt install parallel in a bash terminal\n",
    "# !find . -type f -name \"*.zip\" | parallel unzip -o {} -d {//}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed to ACOLITE\n",
    "- mosaic images if multiple corresponding to a single eHydro survey, possible due to survey covering multiple S2 tiles\n",
    "- will do ACOLITE processing in this notebook once all .SAFE files are downloaded\n",
    "- will reproject Bathy and S2 rasters to common CRS in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(S2_PATH, 'HS_03_BMP_20240226_CS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [os.path.join(S2_PATH, f'HS_03_BMP_20240226_CS/{f}') for f in os.listdir(os.path.join(S2_PATH, 'HS_03_BMP_20240226_CS')) if f.endswith('.SAFE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add acolite clone to Python path and import acolite\n",
    "import sys, os\n",
    "user_home = os.path.expanduser(\"~\")\n",
    "sys.path.append(user_home+'/tools/acolite')\n",
    "import acolite as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images from a single survey for ACOLITE testing\n",
    "# make it a survey with multiple images, will test mosaicking capabilities\n",
    "# mosaciking may not be needed, since the model will be trained pixelwise\n",
    "\n",
    "os.listdir(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in ~/.netrc file\n",
    "# # add EARTHDATA_u and EARTHDATA_p\n",
    "# os.environ['EARTHDATA_u'] = ''\n",
    "# os.environ['EARTHDATA_p'] = ''\n",
    "\n",
    "# scenes to process\n",
    "bundles = test_set\n",
    "# alternatively use glob\n",
    "# import glob\n",
    "# bundles = glob.glob('/path/to/scene*')\n",
    "\n",
    "# output directory\n",
    "out_dir = os.path.join(S2_PATH, f'HS_03_BMP_20240226_CS/acolite_test')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# optional 4 element limit list [S, W, N, E] \n",
    "bounds = extract_valid_bounds_to_epsg4326(os.path.join(BATHY_PATH, 'HS_03_BMP_20240226_CS.tif'))\n",
    "\n",
    "\n",
    "# optional file with processing settings\n",
    "# if set to None defaults will be used\n",
    "settings_file = None\n",
    "\n",
    "# run through bundles\n",
    "for bundle in bundles:\n",
    "    # import settings\n",
    "    settings = ac.acolite.settings.parse(settings_file)\n",
    "    # set settings provided above\n",
    "    # settings['limit'] = boundsx\n",
    "    settings['inputfile'] = bundle\n",
    "    settings['output'] = out_dir\n",
    "    # other settings can also be provided here, e.g.\n",
    "    settings['s2_target_res'] = 10\n",
    "    settings['dsf_aot_estimate'] = 'fixed'\n",
    "    # settings['l2w_parameters'] = ['t_nechad', 't_dogliotti']\n",
    "\n",
    "    # process the current bundle\n",
    "    ac.acolite.acolite_run(settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above cell processes the L1C images (still need to mess with settings a bit), but does not clip to the boundaries of the survey. Clipping to only survey pixels will probably be done in 02_data_prep.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
