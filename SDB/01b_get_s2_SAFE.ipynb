{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to leverage asf_search to retrieve Sentinel-2 .SAFE images that correspond to the eHydro hydrographic surveys. These .SAFE files will then be fed into ACOLITE for the needed preprocessing. Once preprocessed, the images for the hydrographic surveys and the Sentinel-2 images will be fed into 02_data_prep.ipynb to ensure the same area coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am big dumb. Instead of querying from eHydro I can just manually get the already created time cubes that are used for csat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include:\n",
    "- OSM for identifying bridges\n",
    "- Use the TUR and SPM bands for identifying adequate imagery. E.g., if the images has not clear enough\n",
    "- Maybe try to get bathy survey bounds and dates directly from API search? Not sure how much more efficient it would actually be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import asf_search as asf\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bathy_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        bathy = src.read(1)\n",
    "        xmin, ymin, xmax, ymax = src.bounds\n",
    "    \n",
    "    plt.imshow(\n",
    "        bathy,\n",
    "        extent=(xmin, xmax, ymin, ymax),\n",
    "        origin=\"lower\",\n",
    "        cmap=\"viridis\"\n",
    "    )\n",
    "    plt.colorbar(label=\"Depth (Feet)\")\n",
    "    plt.title(\"Rasterized Bathymetry\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "\n",
    "def extract_date(filepath):\n",
    "    \"\"\"extract search date window from the eHydro data\"\"\"\n",
    "    match = re.search(r'\\d{4}\\d{2}\\d{2}', filepath)\n",
    "    date = datetime.strptime(match.group(), '%Y%m%d')\n",
    "    return (date - timedelta(days=1)).strftime('%Y-%m-%d'), (date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "def get_reach_date(surveyname):\n",
    "    pattern = re.compile(r'([A-Za-z]{2}_[A-Za-z0-9]{2}_[A-Za-z]{3,4}_\\d{8})')\n",
    "    match = pattern.search(surveyname)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:  \n",
    "        return None\n",
    "\n",
    "def extract_valid_bounds_to_epsg4326(raster_path):\n",
    "    \"\"\"Extracts the bounding box of valid (non-NaN) data from a raster and converts it to EPSG:4326.\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read the raster data\n",
    "        data = src.read(1)  # Assuming a single band\n",
    "        transform = src.transform  # Affine transform of the raster\n",
    "        src_crs = src.crs  # Source CRS of the raster\n",
    "\n",
    "        # Create a mask for valid (non-NaN) pixels\n",
    "        valid_mask = ~np.isnan(data)\n",
    "\n",
    "        # Find the row and column indices of valid pixels\n",
    "        valid_rows, valid_cols = np.where(valid_mask)\n",
    "\n",
    "        if valid_rows.size == 0 or valid_cols.size == 0:\n",
    "            raise ValueError(\"No valid data in the raster.\")\n",
    "\n",
    "        # Calculate the geographic coordinates of the valid bounds\n",
    "        min_row, max_row = valid_rows.min(), valid_rows.max()\n",
    "        min_col, max_col = valid_cols.min(), valid_cols.max()\n",
    "\n",
    "        # Use the transform to convert row/col to geographic bounds\n",
    "        min_x, min_y = rasterio.transform.xy(transform, min_row, min_col, offset=\"ul\")\n",
    "        max_x, max_y = rasterio.transform.xy(transform, max_row, max_col, offset=\"ul\")\n",
    "\n",
    "        # Bounds in the source CRS\n",
    "        bounds_src_crs = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "        # Transform bounds to EPSG:4326\n",
    "        transformer = Transformer.from_crs(src_crs, \"EPSG:4326\", always_xy=True)\n",
    "        min_x_4326, min_y_4326 = transformer.transform(min_x, min_y)\n",
    "        max_x_4326, max_y_4326 = transformer.transform(max_x, max_y)\n",
    "\n",
    "        bounds_epsg4326 = (min_x_4326, min_y_4326, max_x_4326, max_y_4326)\n",
    "\n",
    "        # Create polygon coordinates in clockwise order starting from top-left\n",
    "        coords = [\n",
    "            (min_x_4326, max_y_4326),  # top-left\n",
    "            (max_x_4326, max_y_4326),  # top-right\n",
    "            (max_x_4326, min_y_4326),  # bottom-right\n",
    "            (min_x_4326, min_y_4326),  # bottom-left\n",
    "            (min_x_4326, max_y_4326)   # back to top-left to close the polygon\n",
    "        ]\n",
    "    \n",
    "        # Format coordinates into WKT string\n",
    "        coord_str = ','.join([f'{x} {y}' for x, y in coords])\n",
    "        wkt = f'POLYGON(({coord_str}))'\n",
    "    \n",
    "        # bbox = ee.Geometry.BBox(bounds_epsg4326[0], bounds_epsg4326[1], bounds_epsg4326[2], bounds_epsg4326[3])\n",
    "\n",
    "    return wkt\n",
    "\n",
    "def run_acolite_processing(surveyname, processed_acolite_path):\n",
    "    import os, sys\n",
    "    user_home = os.path.expanduser(\"~\")\n",
    "    sys.path.append(os.path.join(user_home, 'tools/acolite'))\n",
    "    import acolite as ac\n",
    "\n",
    "    # Create the ACOLITE output directory\n",
    "    acolite_survey = os.path.join(processed_acolite_path, get_reach_date(surveyname))\n",
    "    os.makedirs(acolite_survey, exist_ok=True)\n",
    "\n",
    "    # Find all .SAFE bundles for the survey\n",
    "    safe_sets = [\n",
    "        os.path.join(S2_PATH, surveyname, f)\n",
    "        for f in os.listdir(os.path.join(S2_PATH, surveyname))\n",
    "        if f.endswith('.SAFE')\n",
    "    ]\n",
    "\n",
    "    # Extract bounding polygon (ignore returned EPSG since we won't reproject)\n",
    "    bounds = extract_valid_bounds_to_epsg4326(\n",
    "        os.path.join(BATHY_RASTER_PATH, f'{get_reach_date(surveyname)}.tif')\n",
    "    )\n",
    "\n",
    "    settings_file = None\n",
    "\n",
    "    # Process each SAFE bundle\n",
    "    for bundle in safe_sets:\n",
    "        # Parse default/empty settings\n",
    "        settings = ac.acolite.settings.parse(settings_file)\n",
    "\n",
    "        # General options\n",
    "        settings['inputfile'] = bundle\n",
    "        settings['output'] = acolite_survey\n",
    "        settings['polygon'] = bounds\n",
    "        settings['s2_target_res'] = 10  # (10, 20, or 60 for Sentinel-2)\n",
    "        settings['atmospheric_correction_option'] = 'dark_spectrum'\n",
    "        settings['merge_tiles'] = True\n",
    "\n",
    "        # L2W parameters\n",
    "        settings['l2w_parameters'] = ['rhow_*', 'Rrs_*', 'tur_novoa2017', 'spm_novoa2017']\n",
    "        settings['dsf_aot_estimate'] = 'fixed'\n",
    "        settings['dsf_residual_glint_correction'] = True\n",
    "\n",
    "        # Gains\n",
    "        settings['gains'] = True\n",
    "\n",
    "        # Disable any image (PNG) outputs\n",
    "        settings['rgb_rhot'] = False\n",
    "        settings['rgb_rhos'] = False\n",
    "\n",
    "        # Keep XY in NetCDF (optional)\n",
    "        settings['output_xy'] = True\n",
    "\n",
    "        # GeoTIFFs\n",
    "        settings['l2w_export_geotiff'] = True\n",
    "        settings['rgb_rhow'] = True\n",
    "        settings['l1r_export_geotiff'] = False\n",
    "        settings['l2r_export_geotiff'] = False\n",
    "        settings['export_geotiff_coordinates'] = True\n",
    "        settings['export_geotiff_match_file'] = False\n",
    "        settings['export_cloud_optimized_geotiff'] = False\n",
    "\n",
    "        # Delete intermediate NetCDF files, but keep L2W\n",
    "        settings['l1r_delete_netcdf'] = True\n",
    "        settings['l2r_delete_netcdf'] = True\n",
    "        # (Do NOT set l2w_delete_netcdf=True or you'll lose the L2W file!)\n",
    "\n",
    "        # Run ACOLITE processing\n",
    "        ac.acolite.acolite_run(settings=settings)\n",
    "\n",
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "        \"scope\": \"openid\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "        )\n",
    "    return r.json()[\"access_token\"]\n",
    "\n",
    "def stack_tifs_to_multiband(tif_files, output_file):\n",
    "    if not tif_files:\n",
    "        raise ValueError(\"No input files provided.\")\n",
    "    \n",
    "    # Open the first file to retrieve spatial metadata and data type\n",
    "    with rasterio.open(tif_files[0]) as src:\n",
    "        meta = src.meta.copy()\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "        crs = src.crs\n",
    "        transform = src.transform\n",
    "        dtype = src.dtypes[0]\n",
    "    \n",
    "    # Update metadata for multi-band output (number of bands equals number of files)\n",
    "    meta.update({\n",
    "        'count': len(tif_files),\n",
    "    })\n",
    "    \n",
    "    # Create an empty numpy array to hold all bands\n",
    "    stacked_data = np.empty((len(tif_files), height, width), dtype=dtype)\n",
    "    \n",
    "    # Read each file and add it to the array\n",
    "    for idx, tif in enumerate(tif_files):\n",
    "        with rasterio.open(tif) as src:\n",
    "            # Check dimensions for consistency\n",
    "            if src.width != width or src.height != height:\n",
    "                raise ValueError(f\"File {tif} dimensions do not match the first file.\")\n",
    "            stacked_data[idx, :, :] = src.read(1)\n",
    "    \n",
    "    # Write the stacked array to a new multi-band GeoTIFF file\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        dst.write(stacked_data)\n",
    "    \n",
    "    print(f\"Multi-band file written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file continaing username and password for copernicus browser, as well as the client id and secret for sentinelhub\n",
    "# you gotta make your own, too lazy to keep typing in my info\n",
    "\n",
    "with open('/home/clay/Desktop/s2_login_stuff.txt') as f:        \n",
    "    lines = f.readlines()\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = lines[0][:-1]\n",
    "config.sh_client_secret = lines[1][:-1]\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usace_code = 'CESWG'\n",
    "BATHY_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy'        \n",
    "BATHY_RASTER_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy_rasters'\n",
    "S2_PATH = f'/home/clay/Documents/SDB/{usace_code}/s2_SAFE'\n",
    "os.makedirs(S2_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = [f for f in os.listdir(BATHY_PATH)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for appropriate Sentinel-2 L1C .SAFE files\n",
    "- .SAFE needed for input into ACOLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = get_access_token(lines[-2][:-1], lines[-1][:-1])\n",
    "data_collection = 'SENTINEL-2'\n",
    "\n",
    "survey_info = {}\n",
    "for survey_name in surveynames:\n",
    "\n",
    "    raster = os.path.join(BATHY_PATH, f\"{survey_name}.tif\")\n",
    "    date = extract_date(raster)\n",
    "    bounds = extract_valid_bounds_to_epsg4326(raster) \n",
    "\n",
    "    json = requests.get(f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{bounds}') and ContentDate/Start gt {date[0]}T00:00:00.000Z and ContentDate/Start lt {date[1]}T00:00:00.000Z\").json()\n",
    "    results=pd.DataFrame.from_dict(json['value'])\n",
    "\n",
    "    if len(results) != 0:\n",
    "        urls = []\n",
    "        s2_names = []\n",
    "        for s2_name in list(results.Name):\n",
    "            if 'L1C' in s2_name:\n",
    "                urls.append(f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({(results[results.Name == s2_name]['Id'].values[0])})/$value\")\n",
    "                s2_names.append(s2_name)\n",
    "        survey_info[survey_name] = (urls, s2_names)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Copernicus Hub for Sentinel-2 L1C .SAFE files\n",
    "- iterate by eHydro name. Create a folder for each survey\n",
    "- store all .SAFE files in designated survey folder\n",
    "- .SAFE files named appropriately as stored in .items\n",
    "- Mosaic together during ACOLITE processing, or in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no worky ones:\n",
    "- S2A_MSIL1C_20221025T165401_N0510_R026_T15RTN_20240728T042150.SAFE\n",
    "- S2A_MSIL1C_20230413T164841_N0509_R026_T15RUN_20230413T220740.SAFE\n",
    "- S2A_MSIL1C_20230413T164841_N0510_R026_T15RUN_20240824T151650.SAFE\n",
    "- S2B_MSIL1C_20230518T164849_N0509_R026_T15RVP_20230518T220853.SAFE\n",
    "- S2B_MSIL1C_20230518T164849_N0509_R026_T15RUP_20230518T220853.SAFE\n",
    "- S2A_MSIL1C_20230304T165201_N0510_R026_T15RUP_20240819T071321.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RUP_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RTN_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RUN_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20221013T170239_N0400_R069_T15RTN_20221013T215905.SAFE\n",
    "- S2A_MSIL1C_20200501T165901_N0500_R069_T14RQT_20230330T120915.SAFE\n",
    "- S2A_MSIL1C_20200501T165901_N0500_R069_T15RTM_20230330T120915.SAFE\n",
    "- S2A_MSIL1C_20230722T164901_N0509_R026_T15RUN_20230722T215211.SAFE\n",
    "- S2B_MSIL1C_20191218T170719_N0500_R069_T14RQS_20230607T033311.SAFE\n",
    "- S2B_MSIL1C_20191218T170719_N0500_R069_T15RTM_20230607T033311.SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, items in tqdm(survey_info.items(), desc=\"Processing surveys\"):\n",
    "    if len(items[0]) == 0:\n",
    "        continue\n",
    "        \n",
    "    os.makedirs(os.path.join(S2_PATH, key), exist_ok=True)\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    access_token = get_access_token(lines[-2][:-1], lines[-1][:-1])\n",
    "    \n",
    "    # Process each file within the survey\n",
    "    for url, s2_name in zip(items[0], items[1]):\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            file_path = os.path.join(S2_PATH, f\"{key}/{s2_name[:-5]}.zip\")\n",
    "            \n",
    "            # Progress bar for individual file download\n",
    "            with tqdm(\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                desc=f\"Downloading {s2_name}\",\n",
    "                leave=True  # Keep the progress bar after completion\n",
    "            ) as pbar:\n",
    "                with open(file_path, \"wb\") as file:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {s2_name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the files\n",
    "# you may need to use sudo apt install parallel in a bash terminal\n",
    "# !find . -type f -name \"*.zip\" | parallel unzip -o {} -d {//}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed to ACOLITE\n",
    "- mosaic images if multiple corresponding to a single eHydro survey, possible due to survey covering multiple S2 tiles\n",
    "- will do ACOLITE processing in this notebook once all .SAFE files are downloaded\n",
    "- will reproject Bathy and S2 rasters to common CRS in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = f'/home/clay/Documents/SDB/{usace_code}/processed_acolite'\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "\n",
    "verbose_acolite = False      # change this if you want to see what's going on in ACOLITE processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: for band 1, nodata value has been rounded to -2147483648, Int32 being an integer datatype.\n",
      "Warning 1: for band 1, nodata value has been rounded to -2147483648, Int32 being an integer datatype.\n",
      "Warning 1: for band 1, nodata value has been rounded to -2147483648, Int32 being an integer datatype.\n",
      "Warning 1: for band 1, nodata value has been rounded to -2147483648, Int32 being an integer datatype.\n",
      "Warning 1: for band 1, nodata value has been rounded to -2147483648, Int32 being an integer datatype.\n"
     ]
    }
   ],
   "source": [
    "import contextlib       # using this to supress the verbosity since it isn't possible within ACOLITE itself\n",
    "\n",
    "if verbose_acolite == False:\n",
    "    for name in surveynames:\n",
    "        if os.path.exists(os.path.join(S2_PATH, name)):\n",
    "            if os.path.exists(os.path.join(processed_path, name)) and len(os.listdir(os.path.join(processed_path, name))) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "                    run_acolite_processing(name, processed_path)\n",
    "else:\n",
    "    for name in surveynames:\n",
    "        if os.path.exists(os.path.join(S2_PATH, name)):\n",
    "            if os.path.exists(os.path.join(processed_path, name)) and len(os.listdir(os.path.join(processed_path, name))) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                run_acolite_processing(name, processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_tifs = [os.path.join('/media/clay/Crucial/test_acolite/VT_03_MME_20230814', f) for f in os.listdir('/media/clay/Crucial/test_acolite/VT_03_MME_20230814') if f.endswith('.tif') and 'rhow_' in f]\n",
    "stack_tifs_to_multiband(test_tifs, '/media/clay/Crucial/test_acolite/VT_03_MME_20230814_rhow.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some notes after first test run for ACOLITE\n",
    "- will want to get the L2W.nc file, this is the only one I really need\n",
    "- output only the rhos values, rhot is pointless for my bathy model\n",
    "- need to figure out something to mask the land, if not done by default.\n",
    "- make sure glint correction is applied, it should be by default\n",
    "- DSF vs RAdCor? In manual it seems like RAdCor is best for inland waterways like the NCF\n",
    "- Should look into developing some code that can split the survey aoi between scenes if they overlap. Would let only certain aois be processed reducing the run time and storage sizes\n",
    "- DOCKER FOR PARALLEL PROCESSING (doing this last after I have figured out all the processing settings I want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "target_survey = surveynames[10]\n",
    "\n",
    "nc_file = [os.path.join(processed_path, target_survey, f) for f in os.listdir(os.path.join(processed_path, target_survey)) if f.endswith('L2W.nc')][0]\n",
    "bathy_file = os.path.join(BATHY_PATH, f'{target_survey}.tif')  # Assuming bathy is a GeoTIFF\n",
    "\n",
    "# Open the NetCDF file\n",
    "dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "# List all available variables\n",
    "print(\"Available Variables in L2W File:\")\n",
    "print(dataset.variables.keys())\n",
    "\n",
    "# Select bands to visualize\n",
    "bands_to_plot = ['Rrs_665', 'rhow_665', 'TUR_Novoa2017', 'SPM_Novoa2017']  # Modify based on dataset\n",
    "\n",
    "# Create figure and axes (1 row, 5 columns: bathy-overlay + 4 bands)\n",
    "fig, axes = plt.subplots(1, len(bands_to_plot) + 1, figsize=(5 * (len(bands_to_plot) + 1), 5))\n",
    "\n",
    "### **Load rhow_665 (Base Layer)**\n",
    "if 'rhow_665' in dataset.variables:\n",
    "    rhow_data = dataset.variables['rhow_665'][:]\n",
    "    rhow_data = np.ma.masked_invalid(rhow_data)  # Mask invalid values\n",
    "else:\n",
    "    print(\"rhow_665 not found in dataset.\")\n",
    "\n",
    "### **Load and Overlay Bathymetry**\n",
    "with rasterio.open(bathy_file) as bathy:\n",
    "    bathy_data = bathy.read(1)  # Read first band\n",
    "    bathy_data = np.ma.masked_invalid(bathy_data)  # Mask invalid values\n",
    "    \n",
    "    # Ensure bathymetry data matches rhow dimensions (optional resizing)\n",
    "    if bathy_data.shape != rhow_data.shape:\n",
    "        print(\"Resizing bathymetry raster to match rhow_665 dimensions...\")\n",
    "        from skimage.transform import resize\n",
    "        bathy_data = resize(bathy_data, rhow_data.shape, mode='constant', preserve_range=True)\n",
    "\n",
    "    # Plot rhow_665 first\n",
    "    im1 = axes[0].imshow(rhow_data, cmap='viridis', interpolation='nearest')\n",
    "    \n",
    "    # Overlay bathymetry with transparency\n",
    "    im2 = axes[0].imshow(bathy_data, cmap='terrain', alpha=0.5, interpolation='nearest')\n",
    "\n",
    "    axes[0].set_title(\"rhow_665 + Bathymetry Overlay\")\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04, label=\"rhow_665 Reflectance\")\n",
    "    fig.colorbar(im2, ax=axes[0], fraction=0.046, pad=0.04, label=\"Bathymetry Depth\")\n",
    "\n",
    "### **Plot Remaining ACOLITE Bands**\n",
    "for i, band in enumerate(bands_to_plot, start=1):  # Start at index 1 after overlay\n",
    "    if band in dataset.variables:\n",
    "        data = dataset.variables[band][:]\n",
    "        data = np.ma.masked_invalid(data)  # Mask invalid values\n",
    "        \n",
    "        # Plot reflectance or water quality parameters\n",
    "        im = axes[i].imshow(data, cmap='viridis', interpolation='nearest')\n",
    "        axes[i].set_title(band)\n",
    "        axes[i].axis('off')\n",
    "        fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axes[i].set_title(f\"{band} Not Found\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above cell processes the L1C images (still need to mess with settings a bit), but does not clip to the boundaries of the survey. Clipping to only survey pixels will probably be done in 02_data_prep.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
