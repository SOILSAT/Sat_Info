{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to leverage asf_search to retrieve Sentinel-2 .SAFE images that correspond to the eHydro hydrographic surveys. These .SAFE files will then be fed into ACOLITE for the needed preprocessing. Once preprocessed, the images for the hydrographic surveys and the Sentinel-2 images will be fed into 02_data_prep.ipynb to ensure the same area coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am big dumb. Instead of querying from eHydro I can just manually get the already created time cubes that are used for csat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include:\n",
    "- OSM for identifying bridges\n",
    "- Use the TUR and SPM bands for identifying adequate imagery. E.g., if the images has not clear enough\n",
    "- Maybe try to get bathy survey bounds and dates directly from API search? Not sure how much more efficient it would actually be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import asf_search as asf\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bathy_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        bathy = src.read(1)\n",
    "        xmin, ymin, xmax, ymax = src.bounds\n",
    "    \n",
    "    plt.imshow(\n",
    "        bathy,\n",
    "        extent=(xmin, xmax, ymin, ymax),\n",
    "        origin=\"lower\",\n",
    "        cmap=\"viridis\"\n",
    "    )\n",
    "    plt.colorbar(label=\"Depth (Feet)\")\n",
    "    plt.title(\"Rasterized Bathymetry\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "\n",
    "def extract_date(filepath):\n",
    "    \"\"\"extract search date window from the eHydro data\"\"\"\n",
    "    match = re.search(r'\\d{4}\\d{2}\\d{2}', filepath)\n",
    "    date = datetime.strptime(match.group(), '%Y%m%d')\n",
    "    return (date - timedelta(days=1)).strftime('%Y-%m-%d'), (date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "def extract_valid_bounds_to_epsg4326(raster_path):\n",
    "    \"\"\"Extracts the bounding box of valid (non-NaN) data from a raster and converts it to EPSG:4326.\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read the raster data\n",
    "        data = src.read(1)  # Assuming a single band\n",
    "        transform = src.transform  # Affine transform of the raster\n",
    "        src_crs = src.crs  # Source CRS of the raster\n",
    "\n",
    "        # Create a mask for valid (non-NaN) pixels\n",
    "        valid_mask = ~np.isnan(data)\n",
    "\n",
    "        # Find the row and column indices of valid pixels\n",
    "        valid_rows, valid_cols = np.where(valid_mask)\n",
    "\n",
    "        if valid_rows.size == 0 or valid_cols.size == 0:\n",
    "            raise ValueError(\"No valid data in the raster.\")\n",
    "\n",
    "        # Calculate the geographic coordinates of the valid bounds\n",
    "        min_row, max_row = valid_rows.min(), valid_rows.max()\n",
    "        min_col, max_col = valid_cols.min(), valid_cols.max()\n",
    "\n",
    "        # Use the transform to convert row/col to geographic bounds\n",
    "        min_x, min_y = rasterio.transform.xy(transform, min_row, min_col, offset=\"ul\")\n",
    "        max_x, max_y = rasterio.transform.xy(transform, max_row, max_col, offset=\"ul\")\n",
    "\n",
    "        # Bounds in the source CRS\n",
    "        bounds_src_crs = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "        # Transform bounds to EPSG:4326\n",
    "        transformer = Transformer.from_crs(src_crs, \"EPSG:4326\", always_xy=True)\n",
    "        min_x_4326, min_y_4326 = transformer.transform(min_x, min_y)\n",
    "        max_x_4326, max_y_4326 = transformer.transform(max_x, max_y)\n",
    "\n",
    "        bounds_epsg4326 = (min_x_4326, min_y_4326, max_x_4326, max_y_4326)\n",
    "\n",
    "        # Create polygon coordinates in clockwise order starting from top-left\n",
    "        coords = [\n",
    "            (min_x_4326, max_y_4326),  # top-left\n",
    "            (max_x_4326, max_y_4326),  # top-right\n",
    "            (max_x_4326, min_y_4326),  # bottom-right\n",
    "            (min_x_4326, min_y_4326),  # bottom-left\n",
    "            (min_x_4326, max_y_4326)   # back to top-left to close the polygon\n",
    "        ]\n",
    "    \n",
    "        # Format coordinates into WKT string\n",
    "        coord_str = ','.join([f'{x} {y}' for x, y in coords])\n",
    "        wkt = f'POLYGON(({coord_str}))'\n",
    "    \n",
    "        # bbox = ee.Geometry.BBox(bounds_epsg4326[0], bounds_epsg4326[1], bounds_epsg4326[2], bounds_epsg4326[3])\n",
    "\n",
    "    return wkt\n",
    "\n",
    "def run_acolite_processing(surveyname, processed_acolite_path):\n",
    "    user_home = os.path.expanduser(\"~\")\n",
    "    sys.path.append(user_home+'/tools/acolite')\n",
    "    import acolite as ac\n",
    "\n",
    "    acolite_survey = os.path.join(processed_acolite_path, surveyname)            # out_dir for ACOLITE\n",
    "    os.makedirs(acolite_survey, exist_ok=True)\n",
    "\n",
    "    safe_sets = [os.path.join(S2_PATH, surveyname, f) for f in os.listdir(os.path.join(S2_PATH, surveyname)) if f.endswith('.SAFE')]\n",
    "    bounds = extract_valid_bounds_to_epsg4326(os.path.join(BATHY_PATH, f'{surveyname}.tif'))\n",
    "\n",
    "    settings_file = None\n",
    "\n",
    "    # run through bundles\n",
    "    for bundle in safe_sets:\n",
    "        # import settings\n",
    "        settings = ac.acolite.settings.parse(settings_file)\n",
    "\n",
    "        ## General Options\n",
    "        settings['inputfile'] = bundle\n",
    "        settings['output'] = acolite_survey\n",
    "        settings['polygon'] = bounds\n",
    "        settings['s2_target_res'] = 10                                  # can be 10, 20, 60 for S2\n",
    "        settings['atmospheric_correction_option'] = 'dark_spectrum'            # can be dark_spectrum, radcor, or exponential\n",
    "        settings['merge_tiles'] = True\n",
    "\n",
    "        #l2w_parameters is important\n",
    "        # rhow_* and Rrs_* are the important surface reflectance bands\n",
    "        # 'tur_novoa2017', 'spm_novoa2017' give turbidity and suspended sediment estimates, could be useful\n",
    "        settings['l2w_parameters'] = ['rhow_*', 'Rrs_*','tur_novoa2017', 'spm_novoa2017']\n",
    "\n",
    "        settings['dsf_aot_estimate'] = 'fixed'                  # maybe try 'auto' as well??\n",
    "        settings['dsf_residual_glint_correction'] = True\n",
    "\n",
    "\n",
    "        ## Gains options\n",
    "        settings['gains'] = True\n",
    "        # settings['gains_toa] = None                                   # need to look more into TOA gains for PS and S2\n",
    "\n",
    "        ## Masking options, seems like all the defaults are good imo\n",
    "\n",
    "        ## Ancillary Data options\n",
    "        # THESE OPTIONS ARE VERY INTERESTING, WILL PLAY WITH THEM\n",
    "\n",
    "        ## Output options\n",
    "        settings['rgb_rhot'] = False        # not outputting png\n",
    "        settings['rgb_rho2'] = False\n",
    "        settings['output_xy'] = True\n",
    "        settings['netcdf_compression'] = True   # nc compression is lossless\n",
    "\n",
    "        # process the current bundle\n",
    "        ac.acolite.acolite_run(settings=settings)\n",
    "\n",
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "        \"scope\": \"openid\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "        )\n",
    "    return r.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file continaing username and password for copernicus browser, as well as the client id and secret for sentinelhub\n",
    "# you gotta make your own, too lazy to keep typing in my info\n",
    "\n",
    "with open('/home/clay/Desktop/s2_login_stuff.txt') as f:        \n",
    "    lines = f.readlines()\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = lines[0][:-1]\n",
    "config.sh_client_secret = lines[1][:-1]\n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usace_code = 'CESWG'\n",
    "BATHY_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy_rasters'        # STORAGE_DIR from 01a_get_ehydro.ipynb\n",
    "S2_PATH = f'/home/clay/Documents/SDB/{usace_code}/s2_SAFE'\n",
    "os.makedirs(S2_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for appropriate Sentinel-2 L1C .SAFE files\n",
    "- .SAFE needed for input into ACOLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = get_access_token(lines[-2][:-1], lines[-1][:-1])\n",
    "data_collection = 'SENTINEL-2'\n",
    "\n",
    "survey_info = {}\n",
    "for survey_name in surveynames:\n",
    "\n",
    "    raster = os.path.join(BATHY_PATH, f\"{survey_name}.tif\")\n",
    "    date = extract_date(raster)\n",
    "    bounds = extract_valid_bounds_to_epsg4326(raster) \n",
    "\n",
    "    json = requests.get(f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{bounds}') and ContentDate/Start gt {date[0]}T00:00:00.000Z and ContentDate/Start lt {date[1]}T00:00:00.000Z\").json()\n",
    "    results=pd.DataFrame.from_dict(json['value'])\n",
    "\n",
    "    if len(results) != 0:\n",
    "        urls = []\n",
    "        s2_names = []\n",
    "        for s2_name in list(results.Name):\n",
    "            if 'L1C' in s2_name:\n",
    "                urls.append(f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({(results[results.Name == s2_name]['Id'].values[0])})/$value\")\n",
    "                s2_names.append(s2_name)\n",
    "        survey_info[survey_name] = (urls, s2_names)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Copernicus Hub for Sentinel-2 L1C .SAFE files\n",
    "- iterate by eHydro name. Create a folder for each survey\n",
    "- store all .SAFE files in designated survey folder\n",
    "- .SAFE files named appropriately as stored in .items\n",
    "- Mosaic together during ACOLITE processing, or in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no worky ones:\n",
    "- S2A_MSIL1C_20221025T165401_N0510_R026_T15RTN_20240728T042150.SAFE\n",
    "- S2A_MSIL1C_20230413T164841_N0509_R026_T15RUN_20230413T220740.SAFE\n",
    "- S2A_MSIL1C_20230413T164841_N0510_R026_T15RUN_20240824T151650.SAFE\n",
    "- S2B_MSIL1C_20230518T164849_N0509_R026_T15RVP_20230518T220853.SAFE\n",
    "- S2B_MSIL1C_20230518T164849_N0509_R026_T15RUP_20230518T220853.SAFE\n",
    "- S2A_MSIL1C_20230304T165201_N0510_R026_T15RUP_20240819T071321.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RUP_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RTN_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20200811T164849_N0500_R026_T15RUN_20230510T185335.SAFE\n",
    "- S2B_MSIL1C_20221013T170239_N0400_R069_T15RTN_20221013T215905.SAFE\n",
    "- S2A_MSIL1C_20200501T165901_N0500_R069_T14RQT_20230330T120915.SAFE\n",
    "- S2A_MSIL1C_20200501T165901_N0500_R069_T15RTM_20230330T120915.SAFE\n",
    "- S2A_MSIL1C_20230722T164901_N0509_R026_T15RUN_20230722T215211.SAFE\n",
    "- S2B_MSIL1C_20191218T170719_N0500_R069_T14RQS_20230607T033311.SAFE\n",
    "- S2B_MSIL1C_20191218T170719_N0500_R069_T15RTM_20230607T033311.SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, items in tqdm(survey_info.items(), desc=\"Processing surveys\"):\n",
    "    if len(items[0]) == 0:\n",
    "        continue\n",
    "        \n",
    "    os.makedirs(os.path.join(S2_PATH, key), exist_ok=True)\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    access_token = get_access_token(lines[-2][:-1], lines[-1][:-1])\n",
    "    \n",
    "    # Process each file within the survey\n",
    "    for url, s2_name in zip(items[0], items[1]):\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            file_path = os.path.join(S2_PATH, f\"{key}/{s2_name[:-5]}.zip\")\n",
    "            \n",
    "            # Progress bar for individual file download\n",
    "            with tqdm(\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                desc=f\"Downloading {s2_name}\",\n",
    "                leave=True  # Keep the progress bar after completion\n",
    "            ) as pbar:\n",
    "                with open(file_path, \"wb\") as file:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {s2_name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the files\n",
    "# you may need to use sudo apt install parallel in a bash terminal\n",
    "# !find . -type f -name \"*.zip\" | parallel unzip -o {} -d {//}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed to ACOLITE\n",
    "- mosaic images if multiple corresponding to a single eHydro survey, possible due to survey covering multiple S2 tiles\n",
    "- will do ACOLITE processing in this notebook once all .SAFE files are downloaded\n",
    "- will reproject Bathy and S2 rasters to common CRS in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = f'/home/clay/Documents/SDB/{usace_code}/processed_acolite'\n",
    "os.makedirs(processed_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in surveynames:\n",
    "    if os.path.exists(os.path.join(S2_PATH, name)):\n",
    "        if os.path.exists(os.path.join(processed_path, name)) and len(os.listdir(os.path.join(processed_path, name))) > 0:\n",
    "            continue\n",
    "        else:\n",
    "            run_acolite_processing(name, processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some notes after first test run for ACOLITE\n",
    "- will want to get the L2W.nc file, this is the only one I really need\n",
    "- output only the rhos values, rhot is pointless for my bathy model\n",
    "- need to figure out something to mask the land, if not done by default.\n",
    "- make sure glint correction is applied, it should be by default\n",
    "- DSF vs RAdCor? In manual it seems like RAdCor is best for inland waterways like the NCF\n",
    "- Should look into developing some code that can split the survey aoi between scenes if they overlap. Would let only certain aois be processed reducing the run time and storage sizes\n",
    "- DOCKER FOR PARALLEL PROCESSING (doing this last after I have figured out all the processing settings I want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nc.Dataset('/media/clay/Crucial/CESWG/HS_03_BMP_3.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.variables['crsSTPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_date(dataset, start_date='2015-06-27'):\n",
    "    # Convert start_date to integer format (YYYYMMDD)\n",
    "    start_date_int = int(start_date.replace('-', ''))\n",
    "    \n",
    "    # Get time array and convert to mask\n",
    "    time_array = dataset.variables['time'][:]\n",
    "    mask = time_array > start_date_int\n",
    "    \n",
    "    # Print summary of filtering\n",
    "    total_records = len(time_array)\n",
    "    kept_records = mask.sum()\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Records after {start_date}: {kept_records}\")\n",
    "    \n",
    "    # Return filtered data\n",
    "    filtered_data = {\n",
    "        'time': dataset.variables['time'][:][mask],\n",
    "        'surveyId': dataset.variables['surveyId'][:][mask],\n",
    "        'elevations': dataset.variables['elevations'][:][mask],\n",
    "        'latitudes': dataset.variables['latitudes'][:],  # These don't change with time\n",
    "        'longitudes': dataset.variables['longitudes'][:],  # These don't change with time\n",
    "        'crsSTPL': dataset.variables['crsSTPL'][:]\n",
    "    }\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filter = filter_dataset_by_date(dataset, start_date='2015-06-27')\n",
    "test_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 3 surveys from the dataset\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Extract survey IDs and dates for titles\n",
    "survey_ids = dataset.variables['surveyId'][:3]\n",
    "survey_dates = dataset.variables['time'][:3]\n",
    "\n",
    "# Plot each of the first 3 surveys\n",
    "for i in range(3):\n",
    "    # Get elevation data for this time step\n",
    "    elevation_data = dataset.variables['elevations'][i]\n",
    "    \n",
    "    # Create scatter plot using points\n",
    "    scatter = axes[i].scatter(\n",
    "        dataset.variables['longitudes'][:],\n",
    "        dataset.variables['latitudes'][:],\n",
    "        c=elevation_data,\n",
    "        cmap='viridis',\n",
    "        s=1\n",
    "    )\n",
    "    \n",
    "    # Add colorbar and title\n",
    "    plt.colorbar(scatter, ax=axes[i], label='Elevation')\n",
    "    axes[i].set_title(f'Survey {survey_ids[i]}\\nDate: {survey_dates[i]}')\n",
    "    axes[i].set_xlabel('Longitude')\n",
    "    axes[i].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 3 surveys from the dataset\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Extract survey IDs and dates for titles\n",
    "survey_ids = dataset.variables['surveyId'][:3]\n",
    "survey_dates = dataset.variables['time'][:3]\n",
    "\n",
    "# Plot each of the first 3 surveys\n",
    "for i in range(3):\n",
    "    # Get elevation data for this time step\n",
    "    elevation_data = dataset.variables['elevations'][i]\n",
    "    \n",
    "    # Handle masked array properly when converting survey ID to string\n",
    "    if hasattr(survey_ids[i], 'mask'):\n",
    "        # For masked arrays, only join unmasked values\n",
    "        survey_id = \"\".join([str(x) for x in survey_ids[i].data if not survey_ids[i].mask[x]])\n",
    "    else:\n",
    "        # For regular arrays\n",
    "        survey_id = \"\".join([str(x) for x in survey_ids[i] if str(x).strip()])\n",
    "    \n",
    "    # Create scatter plot using points\n",
    "    scatter = axes[i].scatter(\n",
    "        dataset.variables['longitudes'][:],\n",
    "        dataset.variables['latitudes'][:],\n",
    "        c=elevation_data,\n",
    "        cmap='viridis',\n",
    "        s=1\n",
    "    )\n",
    "    \n",
    "    # Add colorbar and title with both survey ID and date\n",
    "    plt.colorbar(scatter, ax=axes[i], label='Elevation')\n",
    "    axes[i].set_title(f'Survey: {survey_id}\\nDate: {survey_dates[i]}')\n",
    "    axes[i].set_xlabel('Longitude')\n",
    "    axes[i].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "nc_file = os.path.join(processed_path, 'S2B_MSI_2024_02_25_17_15_35_merged_L2W.nc')\n",
    "bathy_file = os.path.join(BATHY_PATH, 'HS_03_BMP_20240226_CS.tif')  # Assuming bathy is a GeoTIFF\n",
    "\n",
    "# Open the NetCDF file\n",
    "dataset = nc.Dataset(nc_file, mode='r')\n",
    "\n",
    "# List all available variables\n",
    "print(\"Available Variables in L2W File:\")\n",
    "print(dataset.variables.keys())\n",
    "\n",
    "# Select bands to visualize\n",
    "bands_to_plot = ['Rrs_665', 'rhow_665', 'TUR_Novoa2017', 'SPM_Novoa2017']  # Modify based on dataset\n",
    "\n",
    "# Create figure and axes (1 row, 5 columns: bathy-overlay + 4 bands)\n",
    "fig, axes = plt.subplots(1, len(bands_to_plot) + 1, figsize=(5 * (len(bands_to_plot) + 1), 5))\n",
    "\n",
    "### **Load rhow_665 (Base Layer)**\n",
    "if 'rhow_665' in dataset.variables:\n",
    "    rhow_data = dataset.variables['rhow_665'][:]\n",
    "    rhow_data = np.ma.masked_invalid(rhow_data)  # Mask invalid values\n",
    "else:\n",
    "    print(\"rhow_665 not found in dataset.\")\n",
    "\n",
    "### **Load and Overlay Bathymetry**\n",
    "with rasterio.open(bathy_file) as bathy:\n",
    "    bathy_data = bathy.read(1)  # Read first band\n",
    "    bathy_data = np.ma.masked_invalid(bathy_data)  # Mask invalid values\n",
    "    \n",
    "    # Ensure bathymetry data matches rhow dimensions (optional resizing)\n",
    "    if bathy_data.shape != rhow_data.shape:\n",
    "        print(\"Resizing bathymetry raster to match rhow_665 dimensions...\")\n",
    "        from skimage.transform import resize\n",
    "        bathy_data = resize(bathy_data, rhow_data.shape, mode='constant', preserve_range=True)\n",
    "\n",
    "    # Plot rhow_665 first\n",
    "    im1 = axes[0].imshow(rhow_data, cmap='viridis', interpolation='nearest')\n",
    "    \n",
    "    # Overlay bathymetry with transparency\n",
    "    im2 = axes[0].imshow(bathy_data, cmap='terrain', alpha=0.5, interpolation='nearest')\n",
    "\n",
    "    axes[0].set_title(\"rhow_665 + Bathymetry Overlay\")\n",
    "    axes[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04, label=\"rhow_665 Reflectance\")\n",
    "    fig.colorbar(im2, ax=axes[0], fraction=0.046, pad=0.04, label=\"Bathymetry Depth\")\n",
    "\n",
    "### **Plot Remaining ACOLITE Bands**\n",
    "for i, band in enumerate(bands_to_plot, start=1):  # Start at index 1 after overlay\n",
    "    if band in dataset.variables:\n",
    "        data = dataset.variables[band][:]\n",
    "        data = np.ma.masked_invalid(data)  # Mask invalid values\n",
    "        \n",
    "        # Plot reflectance or water quality parameters\n",
    "        im = axes[i].imshow(data, cmap='viridis', interpolation='nearest')\n",
    "        axes[i].set_title(band)\n",
    "        axes[i].axis('off')\n",
    "        fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axes[i].set_title(f\"{band} Not Found\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above cell processes the L1C images (still need to mess with settings a bit), but does not clip to the boundaries of the survey. Clipping to only survey pixels will probably be done in 02_data_prep.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
