{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to leverage asf_search to retrieve Sentinel-2 .SAFE images that correspond to the eHydro hydrographic surveys. These .SAFE files will then be fed into ACOLITE for the needed preprocessing. Once preprocessed, the images for the hydrographic surveys and the Sentinel-2 images will be fed into 02_data_prep.ipynb to ensure the same area coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import asf_search as asf\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sentinelhub import (\n",
    "    SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubDownloadClient,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import credentials\n",
    "# from creds import *\n",
    "\n",
    "\n",
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "        )\n",
    "    return r.json()[\"access_token\"]\n",
    "\n",
    "access_token = get_access_token()\n",
    "\n",
    "# access_token = get_access_token(\n",
    "#     getpass.getpass(\"Enter your Copernicus username\"),\n",
    "#     getpass.getpass(\"Enter your Copernicus password\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig()\n",
    "config.sh_client_id = \n",
    "config.sh_client_secret = \n",
    "config.sh_base_url = 'https://sh.dataspace.copernicus.eu'\n",
    "config.sh_token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asf_search_results(startdate, enddate, wkt_aoi, pform, proclevel, bmode, flightD, pol):\n",
    "    results = asf.search(\n",
    "        platform= pform,\n",
    "        processingLevel=[proclevel],\n",
    "        start = startdate,\n",
    "        end = enddate,\n",
    "        intersectsWith = wkt_aoi,\n",
    "        beamMode= bmode,\n",
    "        flightDirection= flightD,\n",
    "        polarization= pol\n",
    "    )\n",
    "    \n",
    "    return results.geojson(), results\n",
    "\n",
    "def asf_download(earthdatausr, earthdatapass, directory, metadata, results, direction):\n",
    "    #establish download session\n",
    "    session = asf.ASFSession().auth_with_creds(earthdatausr, earthdatapass)\n",
    "\n",
    "    #create empty dictionary to store epoch information\n",
    "    epoch_info ={}\n",
    "\n",
    "    # paths, frames, filenames, and asf search products into list\n",
    "    paths = [feature[\"properties\"][\"pathNumber\"] for feature in metadata['features']]\n",
    "    frames = [feature[\"properties\"][\"frameNumber\"] for feature in metadata['features']]\n",
    "    filenames = [feature[\"properties\"][\"fileName\"] for feature in metadata['features']]\n",
    "    asfprods = list(results)\n",
    "\n",
    "    # create directories for each path-frame combination, used to store like epochs (easy processing later)\n",
    "    for path in list(Counter(paths)):\n",
    "        path_dir = os.path.join(directory, f\"{direction}/{path}\")\n",
    "        os.makedirs(path_dir, exist_ok=True)\n",
    "        for frame in list(Counter(frames)):\n",
    "            frame_dir = os.path.join(path_dir, f\"{frame}\")\n",
    "            os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "    # popualate epoch_info\n",
    "    for name, path, frame, asfprod in zip(filenames, paths, frames, asfprods):\n",
    "        epoch_info[name] = [path, frame, asfprod]\n",
    "\n",
    "    # download SLC epochs from ASF vertex to apprroprate directories\n",
    "    for name in epoch_info:\n",
    "        path = epoch_info[name][0]\n",
    "        frame = epoch_info[name][1]\n",
    "        asfprod = epoch_info[name][2]\n",
    "\n",
    "        asfprod.download(\n",
    "            path = os.path.join(directory, f'{direction}/{path}/{frame}'),\n",
    "            session = session\n",
    "        )\n",
    "\n",
    "def visualize_bathy_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        bathy = src.read(1)\n",
    "        xmin, ymin, xmax, ymax = src.bounds\n",
    "    \n",
    "    plt.imshow(\n",
    "        bathy,\n",
    "        extent=(xmin, xmax, ymin, ymax),\n",
    "        origin=\"lower\",\n",
    "        cmap=\"viridis\"\n",
    "    )\n",
    "    plt.colorbar(label=\"Depth (Feet)\")\n",
    "    plt.title(\"Rasterized Bathymetry\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "\n",
    "def extract_date(filepath):\n",
    "    \"\"\"extract search date window from the eHydro data\"\"\"\n",
    "    match = re.search(r'\\d{4}\\d{2}\\d{2}', filepath)\n",
    "    date = datetime.strptime(match.group(), '%Y%m%d')\n",
    "    return (date - timedelta(days=1)).strftime('%Y-%m-%d'), (date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "def extract_valid_bounds_to_epsg4326(raster_path):\n",
    "    \"\"\"Extracts the bounding box of valid (non-NaN) data from a raster and converts it to EPSG:4326.\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read the raster data\n",
    "        data = src.read(1)  # Assuming a single band\n",
    "        transform = src.transform  # Affine transform of the raster\n",
    "        src_crs = src.crs  # Source CRS of the raster\n",
    "\n",
    "        # Create a mask for valid (non-NaN) pixels\n",
    "        valid_mask = ~np.isnan(data)\n",
    "\n",
    "        # Find the row and column indices of valid pixels\n",
    "        valid_rows, valid_cols = np.where(valid_mask)\n",
    "\n",
    "        if valid_rows.size == 0 or valid_cols.size == 0:\n",
    "            raise ValueError(\"No valid data in the raster.\")\n",
    "\n",
    "        # Calculate the geographic coordinates of the valid bounds\n",
    "        min_row, max_row = valid_rows.min(), valid_rows.max()\n",
    "        min_col, max_col = valid_cols.min(), valid_cols.max()\n",
    "\n",
    "        # Use the transform to convert row/col to geographic bounds\n",
    "        min_x, min_y = rasterio.transform.xy(transform, min_row, min_col, offset=\"ul\")\n",
    "        max_x, max_y = rasterio.transform.xy(transform, max_row, max_col, offset=\"ul\")\n",
    "\n",
    "        # Bounds in the source CRS\n",
    "        bounds_src_crs = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "        # Transform bounds to EPSG:4326\n",
    "        transformer = Transformer.from_crs(src_crs, \"EPSG:4326\", always_xy=True)\n",
    "        min_x_4326, min_y_4326 = transformer.transform(min_x, min_y)\n",
    "        max_x_4326, max_y_4326 = transformer.transform(max_x, max_y)\n",
    "\n",
    "        bounds_epsg4326 = (min_x_4326, min_y_4326, max_x_4326, max_y_4326)\n",
    "\n",
    "        # Create polygon coordinates in clockwise order starting from top-left\n",
    "        coords = [\n",
    "            (min_x_4326, max_y_4326),  # top-left\n",
    "            (max_x_4326, max_y_4326),  # top-right\n",
    "            (max_x_4326, min_y_4326),  # bottom-right\n",
    "            (min_x_4326, min_y_4326),  # bottom-left\n",
    "            (min_x_4326, max_y_4326)   # back to top-left to close the polygon\n",
    "        ]\n",
    "    \n",
    "        # Format coordinates into WKT string\n",
    "        coord_str = ','.join([f'{x} {y}' for x, y in coords])\n",
    "        wkt = f'POLYGON(({coord_str}))'\n",
    "    \n",
    "        # bbox = ee.Geometry.BBox(bounds_epsg4326[0], bounds_epsg4326[1], bounds_epsg4326[2], bounds_epsg4326[3])\n",
    "\n",
    "    return wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usace_code = 'CESWG'\n",
    "BATHY_PATH = f'/home/clay/Documents/SDB/{usace_code}/bathy_rasters'        # STORAGE_DIR from 01a_get_ehydro.ipynb\n",
    "S2_PATH = f'/home/clay/Documents/SDB/{usace_code}/s2_SAFE'\n",
    "os.makedirs(S2_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_info = {}\n",
    "# for name in surveynames:\n",
    "\n",
    "#     raster = os.path.join(BATHY_PATH, f\"{name}.tif\")\n",
    "#     date = extract_date(raster)\n",
    "#     bounds = extract_valid_bounds_to_epsg4326(raster)\n",
    "#     time_interval = date[0], date[1]  \n",
    "\n",
    "#     survey_info[name] = [bounds, date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for appropriate Sentinel-2 L1C .SAFE files\n",
    "- .SAFE needed for input into ACOLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection = 'SENTINEL-2'\n",
    "\n",
    "survey_info = {}\n",
    "for name in surveynames:\n",
    "\n",
    "    raster = os.path.join(BATHY_PATH, f\"{name}.tif\")\n",
    "    date = extract_date(raster)\n",
    "    bounds = extract_valid_bounds_to_epsg4326(raster)\n",
    "\n",
    "    json = requests.get(f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{bounds}') and ContentDate/Start gt {date[0]}T00:00:00.000Z and ContentDate/Start lt {date[1]}T00:00:00.000Z\").json()\n",
    "    results=pd.DataFrame.from_dict(json['value'])\n",
    "\n",
    "    if len(results) != 0:\n",
    "        for name in list(results.Name):\n",
    "            if 'L1C' in name:\n",
    "                test_id = list(results[results.Name == name]['Id'])[0]\n",
    "                url = f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({(test_id)})/$value\"\n",
    "                survey_info[name] = url\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Copernicus Hub for Sentinel-2 L1C .SAFE files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, url in survey_info.items():\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    response = session.get(url, headers=headers, stream=True)\n",
    "\n",
    "    with open(os.path.join(S2_PATH, f\"{name[:-5]}.zip\"), \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed to ACOLITE\n",
    "- will do ACOLITE processing in this notebook once all .SAFE files are downloaded\n",
    "- will reproject Bathy and S2 rasters to common CRS in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add acolite clone to Python path and import acolite\n",
    "import sys, os\n",
    "user_home = os.path.expanduser(\"~\")\n",
    "sys.path.append(user_home+'/git/acolite')\n",
    "import acolite as ac\n",
    "\n",
    "# add EARTHDATA_u and EARTHDATA_p\n",
    "os.environ['EARTHDATA_u'] = ''\n",
    "os.environ['EARTHDATA_p'] = ''\n",
    "\n",
    "# scenes to process\n",
    "bundles = ['/path/to/scene1', '/path/to/scene2']\n",
    "# alternatively use glob\n",
    "# import glob\n",
    "# bundles = glob.glob('/path/to/scene*')\n",
    "\n",
    "# output directory\n",
    "odir = '/path/to/output/directory'\n",
    "\n",
    "# optional 4 element limit list [S, W, N, E] \n",
    "limit = None\n",
    "\n",
    "# optional file with processing settings\n",
    "# if set to None defaults will be used\n",
    "settings_file = None\n",
    "\n",
    "# run through bundles\n",
    "for bundle in bundles:\n",
    "    # import settings\n",
    "    settings = ac.acolite.settings.parse(settings_file)\n",
    "    # set settings provided above\n",
    "    settings['limit'] = limit\n",
    "    settings['inputfile'] = bundle\n",
    "    settings['output'] = odir\n",
    "    # other settings can also be provided here, e.g.\n",
    "    # settings['s2_target_res'] = 60\n",
    "    # settings['dsf_aot_estimate'] = 'fixed'\n",
    "    # settings['l2w_parameters'] = ['t_nechad', 't_dogliotti']\n",
    "\n",
    "    # process the current bundle\n",
    "    ac.acolite.acolite_run(settings=settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
