{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to develop and compare regression models to correlate the eHydro bathymetric surveys with cloud-masked Sentinel-2 surface refelctances. These models will hopefully provide USACE and the eHydro program with a new, robust, accurate tool for unmanned bathymetric estiamtes. This will be possible at 10-meter resolution at a frequency of up to 5 days.\n",
    "- First starting with XGBoost, RF, and SVM-RBF regressors in the SWG. May try some NN as well\n",
    "- band maths here with the green and blue bands (short wavelengths penetrate water columns more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY OUT THE ACOLITE LIBRARY FOR CORRECTING IMAGES NEAR THE COASTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import rasterio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize an array\n",
    "def normalize(array):\n",
    "    return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
    "\n",
    "# Function to read .tif files\n",
    "def read_tif(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()  # Read all bands\n",
    "        profile = src.profile  # Get metadata (optional, for reference)\n",
    "    return array, profile\n",
    "\n",
    "def extract_raster_data(pair_tuple):\n",
    "    images_data = []\n",
    "\n",
    "    bathy_path = pair_tuple[1]\n",
    "    s2_path = pair_tuple[0]\n",
    "\n",
    "        # --- Step 1: Open Bathymetry Raster ---\n",
    "    with rasterio.open(bathy_path) as bathy:\n",
    "        bathy_data = bathy.read(1)  # Bathymetry data (band 1)\n",
    "        bathy_nodata = bathy.nodata  # NoData value\n",
    "        bathy_transform = bathy.transform\n",
    "        bathy_shape = bathy.shape\n",
    "\n",
    "        # --- Step 2: Open Sentinel-2 Raster ---\n",
    "    with rasterio.open(s2_path) as s2:\n",
    "        if s2.shape != bathy_shape or s2.transform != bathy_transform:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent shapes or transforms:\\n\"\n",
    "                f\"Bathymetry Shape: {bathy_shape}, Sentinel-2 Shape: {s2.shape}.\\n\"\n",
    "                f\"Bathymetry Transform: {bathy_transform}, Sentinel-2 Transform: {s2.transform}.\\n\"\n",
    "                f\"Ensure rasters have identical extents and resolutions.\"\n",
    "            )\n",
    "        \n",
    "        # Read Sentinel-2 bands\n",
    "        bands = {\n",
    "            \"red\": normalize(s2.read(3)),\n",
    "            \"green\": normalize(s2.read(2)),\n",
    "            \"blue\": normalize(s2.read(1)),\n",
    "            \"nir\": normalize(s2.read(4)),\n",
    "            \"blue/green\": normalize(s2.read(1) / s2.read(2)),\n",
    "            \"green/blue\": normalize(s2.read(2) / s2.read(1)),\n",
    "            \"stumpf\": normalize(np.log(s2.read(1) / np.log(s2.read(2)))),\n",
    "            \"nsmi\": normalize(s2.read(3) / s2.read(2)),\n",
    "            \"ti\": normalize((s2.read(3) - s2.read(2)) / (s2.read(3) + s2.read(2)))\n",
    "        }\n",
    "\n",
    "    # transformed_bands = quantile_transform(bands)\n",
    "    s2_nodata = s2.nodata  # Sentinel-2 NoData value\n",
    "\n",
    "    # --- Step 3: Flatten Bands ---\n",
    "    flat_bathy = bathy_data.flatten()\n",
    "    flat_bands = {key: band.flatten() for key, band in bands.items()}\n",
    "\n",
    "    # --- Step 4: Mask NoData Values ---\n",
    "    valid_mask = (\n",
    "        ~np.isnan(flat_bathy) &  # Valid bathy pixels\n",
    "        (flat_bathy != bathy_nodata)  # Exclude bathy NoData\n",
    "    )\n",
    "\n",
    "    for band in flat_bands.values():\n",
    "        valid_mask &= (band != s2_nodata)  # Exclude Sentinel-2 NoData\n",
    "        \n",
    "        \n",
    "    # Apply the mask\n",
    "    valid_bathy = flat_bathy[valid_mask].reshape(-1, 1)  # Reshape bathy to (n_pixels, 1)\n",
    "    valid_features = np.column_stack([band[valid_mask] for band in flat_bands.values()])\n",
    "\n",
    "    # --- Step 5: Combine Features and Targets ---\n",
    "    # combined_features = np.concatenate((valid_bathy, valid_features), axis=1)  # Combine bathy and S2\n",
    "    images_data.append((valid_features, valid_bathy.flatten()))  # Flatten bathy for targets\n",
    "\n",
    "    return images_data\n",
    "\n",
    "def prepare_data(pairs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for features, targets in pairs:\n",
    "        X.append(features[:,1:])  # Keep features from this pair\n",
    "        y.append(targets)   # Keep corresponding targets\n",
    "    return np.vstack(X), np.hstack(y)\n",
    "\n",
    "def survey_name_type(surveynames):\n",
    "    \"\"\"\n",
    "    Will take in the list of surveynames and extract the NCF channel ID and the survey type\n",
    "    \"\"\"\n",
    "    surveytypes = ['AD', 'BD', 'CS', 'PA', 'PR', 'XA', 'XB', 'XC', 'OT', 'DS']\n",
    "\n",
    "    extracted_parts = [re.match(r'^(.*?)_\\d{8}', path).group(1) for path in surveynames if re.match(r'^(.*?)_\\d{8}', path)]\n",
    "    channel_ids = [re.sub(r'^.*?_DIS_', '', path) for path in extracted_parts]\n",
    "\n",
    "    isolated_survey_types = []\n",
    "    for path in surveynames:\n",
    "        for type in surveytypes:\n",
    "            if type in path:\n",
    "                isolated_survey_types.append(type)\n",
    "                break  # Stop checking after the first match\n",
    "\n",
    "    return channel_ids, isolated_survey_types\n",
    "\n",
    "def create_composite_bands_with_existing(flattened_s2):\n",
    "    if flattened_s2.shape[1] != 4:\n",
    "        raise ValueError(\"Input array must have 4 columns representing B, G, R, NIR bands.\")\n",
    "\n",
    "    # Split the bands\n",
    "    blue = flattened_s2[:, 0]\n",
    "    green = flattened_s2[:, 1]\n",
    "    red = flattened_s2[:, 2]\n",
    "\n",
    "    # Compute composite bands\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        bluegreen = np.divide(blue, green, out=np.zeros_like(blue), where=green != 0)\n",
    "        greenblue = np.divide(green, blue, out=np.zeros_like(green), where=blue != 0)\n",
    "        stumpf = np.divide(np.log(blue + 1e-6), np.log(green + 1e-6), out=np.zeros_like(blue), where=(green > 0) & (blue > 0))\n",
    "        nsmi = np.divide(red, green, out=np.zeros_like(red), where=green != 0)\n",
    "        ti = np.divide((red - green), (red + green), out=np.zeros_like(red), where=(green > 0) & (red > 0))\n",
    "\n",
    "    # Normalize composite bands\n",
    "    bluegreen = normalize(bluegreen)\n",
    "    greenblue = normalize(greenblue)\n",
    "    stumpf = normalize(stumpf)\n",
    "    nsmi = normalize(nsmi)\n",
    "    ti = normalize(ti)\n",
    "\n",
    "    # Combine all bands\n",
    "    combined_array = np.hstack((flattened_s2, bluegreen[:, None], greenblue[:, None], stumpf[:, None], nsmi[:, None], ti[:, None]))\n",
    "\n",
    "    return combined_array\n",
    "\n",
    "def get_pixel_positions(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Get the affine transformation of the raster\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Read the first band to determine valid (non-NaN) pixels\n",
    "        band_data = src.read(1, masked=True)  # Read the first band as a masked array\n",
    "        valid_mask = ~band_data.mask          # Valid pixels where mask is False\n",
    "\n",
    "        # Get raster dimensions\n",
    "        height, width = src.height, src.width\n",
    "\n",
    "        # Create arrays of pixel indices\n",
    "        row_indices, col_indices = np.meshgrid(np.arange(height), np.arange(width), indexing=\"ij\")\n",
    "\n",
    "        # Compute x, y positions using the affine transform\n",
    "        xs, ys = rasterio.transform.xy(transform, row_indices, col_indices, offset='center')\n",
    "        xs = np.array(xs).flatten()\n",
    "        ys = np.array(ys).flatten()\n",
    "\n",
    "        # Filter x, y positions to include only valid pixels\n",
    "        valid_positions = np.column_stack((xs[valid_mask.flatten()], ys[valid_mask.flatten()]))\n",
    "\n",
    "    return valid_positions\n",
    "\n",
    "def prepare_train_data(surveynames):\n",
    "    pairs = [(os.path.join(S2_PATH, f'{name}.tif'), os.path.join(BATHY_PATH, f'{name}.tif')) for name in surveynames]\n",
    "    \n",
    "    good_pairs = []\n",
    "    goodnames = []\n",
    "    for name, pair in zip(surveynames, pairs):\n",
    "        with rasterio.open(pair[0]) as src:\n",
    "            band = src.read(1)\n",
    "            if band.shape[0] != 0:\n",
    "                good_pairs.append(pair)\n",
    "                goodnames.append(name)\n",
    "\n",
    "    images_data = [extract_raster_data(pair) for pair in good_pairs]\n",
    "    ncf_channels, survey_types = survey_name_type(goodnames)\n",
    "    # all_bands = [create_composite_bands_with_existing(pair[0]) for pair in images_data]\n",
    "    pixel_positions = [get_pixel_positions(os.path.join(S2_PATH, f'{name}.tif')) for name in goodnames]\n",
    "    \n",
    "    data = {}\n",
    "    for i, name in enumerate(goodnames):\n",
    "        # Extract data for the current iteration\n",
    "        bands = images_data[i]               # Shape (n_pixels, 7)\n",
    "        positions = pixel_positions[i]     # Shape (n_pixels, 2)\n",
    "    \n",
    "        data[name] = pd.DataFrame({\n",
    "                \"Blue\": bands[0][0][:, 0],\n",
    "                \"Green\": bands[0][0][:, 1],\n",
    "                \"Red\": bands[0][0][:, 2],\n",
    "                \"NIR\": bands[0][0][:, 3],\n",
    "                \"Blue/Green\": bands[0][0][:, 4],\n",
    "                \"Green/Blue\": bands[0][0][:, 5],\n",
    "                \"Stumpf\": bands[0][0][:, 6],\n",
    "                \"NSMI\": bands[0][0][:, 7],\n",
    "                \"TI\": bands[0][0][:, 8],\n",
    "                \"X\": positions[:, 0],\n",
    "                \"Y\": positions[:, 1],\n",
    "                \"Channel_Name\": [ncf_channels[i]] * len(bands[0][0]),  # Repeating value directly\n",
    "                \"Bathymetry\": bands[0][1]\n",
    "            })\n",
    "\n",
    "    combined_df = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    combined_df['Channel_Name_Encoded'] = encoder.fit_transform(combined_df['Channel_Name'])\n",
    "\n",
    "    output = open(os.path.join(WORK_DIR, 'Channel_Name_label_encoders.pkl'), 'wb')\n",
    "    pickle.dump(encoder, output)\n",
    "    output.close()\n",
    "    \n",
    "    # Drop original categorical columns\n",
    "    combined_df.drop(columns=['Channel_Name'], inplace=True)\n",
    "\n",
    "    X = combined_df.drop(columns=['Bathymetry'])\n",
    "    y = combined_df['Bathymetry']\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Split temp into validation (15%) and test (15%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return combined_df, X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "def plot_histograms(df):\n",
    "    num_columns = min(9, len(df.columns))  # Limit to 7 columns\n",
    "    plt.figure(figsize=(15, 10))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        plt.subplot(3, 3, i + 1)  # Create a grid for plots (3x3 max)\n",
    "        column = df.columns[i]\n",
    "        plt.hist(df[column], bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work directory\n",
    "WORK_DIR = '/home/clay/Documents/SDB/CESWG'\n",
    "\n",
    "S2_PATH =os.path.join(WORK_DIR, 'processed/S2')\n",
    "BATHY_PATH = os.path.join(WORK_DIR,'processed/Bathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 band manipulation and including other data (good reference is Chybicki et al. 2023)\n",
    "- Blue Green ratios (Blue/Green, Green/Blue)\n",
    "- NSMI (Red/Green)\n",
    "- Turbidity Index ((Red-Green) / (Red+Green))\n",
    "- Stumpf log ratio of blue green (https://aslopubs.onlinelibrary.wiley.com/doi/10.4319/lo.2003.48.1_part_2.0547)\n",
    "- Coordinates\n",
    "- NCF channel ID name\n",
    "\n",
    "\n",
    "The Chybicki 2023 paper had models perform extremely well when including all bands, the Stumpf log ratio, and the UTM coordinates. I think including more blue-green ratios and the survey type as well will increase my accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6752/2336007973.py:3: RuntimeWarning: All-NaN slice encountered\n",
      "  return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
      "/tmp/ipykernel_6752/2336007973.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
      "/tmp/ipykernel_6752/2336007973.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  \"stumpf\": normalize(np.log(s2.read(1) / np.log(s2.read(2)))),\n",
      "/tmp/ipykernel_6752/2336007973.py:41: RuntimeWarning: divide by zero encountered in divide\n",
      "  \"blue/green\": normalize(s2.read(1) / s2.read(2)),\n",
      "/tmp/ipykernel_6752/2336007973.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "  \"stumpf\": normalize(np.log(s2.read(1) / np.log(s2.read(2)))),\n",
      "/tmp/ipykernel_6752/2336007973.py:43: RuntimeWarning: invalid value encountered in log\n",
      "  \"stumpf\": normalize(np.log(s2.read(1) / np.log(s2.read(2)))),\n",
      "/tmp/ipykernel_6752/2336007973.py:3: RuntimeWarning: invalid value encountered in subtract\n",
      "  return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
      "/tmp/ipykernel_6752/2336007973.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  \"nsmi\": normalize(s2.read(3) / s2.read(2)),\n"
     ]
    }
   ],
   "source": [
    "combined_data, X_train, y_train, X_test, y_test, X_val, y_val = prepare_train_data(surveynames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glint correction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the different variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of values in the X and y variable test, train, and val sample sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,17))\n",
    "sns.pairplot(data=combined_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(combined_data.corr(), annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for test_train_split\n",
    "- Trains per pixel\n",
    "\n",
    "\n",
    "- k-fold segmentation for training?\n",
    "- try 3 regression models for now: SVM, RF, and XGBoost\n",
    "- may try ElasticNet from cuML, and some shallow NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 6088268\n",
      "Validation set size: 1304629\n",
      "Testing set size: 1304629\n"
     ]
    }
   ],
   "source": [
    "# Confirm dataset sizes\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])\n",
    "# X_test.to_csv(os.path.join(WORK_DIR, 'data.csv'), sep='\\t', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of X columns include 'Blue', 'Green', 'Red', 'NIR', 'Blue/Green', 'Green/Blue', 'Stumpf', 'NSMI', 'TI', 'X', 'Y', 'Channel_Name_Encoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcols = ['Blue', 'Green', 'Stumpf','X', 'Y', 'Channel_Name_Encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = X_train[testcols].copy()\n",
    "new_test = X_test[testcols].copy()\n",
    "new_val = X_val[testcols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = list(X_train.columns)\n",
    "constant_cols = Xcols[:4] + Xcols[-3:]\n",
    "non_constant_cols = list(set(Xcols).symmetric_difference(set(constant_cols)))\n",
    "\n",
    "traindfs = []\n",
    "testdfs = []\n",
    "valdfs = []\n",
    "\n",
    "for i in range(len(non_constant_cols)):\n",
    "    constant_cols.append(non_constant_cols[i])\n",
    "    new_X_train = X_train[constant_cols].copy()\n",
    "    new_X_test = X_test[constant_cols].copy()\n",
    "    new_X_val = X_val[constant_cols].copy()\n",
    "    traindfs.append(new_X_train)\n",
    "    testdfs.append(new_X_test)\n",
    "    valdfs.append(new_X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RF Regression:\n",
    "- interesting note, with RF (not sure if all bagging or tree-like) decreases accuracy with increasing variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gridsearch param tuning:\n",
    "1. {'criterion': 'squared_error',\n",
    " 'max_depth': 8,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 200}\n",
    "2. {'criterion': 'squared_error',\n",
    " 'max_depth': 12,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 100}\n",
    "3. {'criterion': 'squared_error',\n",
    " 'max_depth': 20,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 100}\n",
    " 4. {'criterion': 'friedman_mse',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_leaf': 2,\n",
    " 'min_samples_split': 2,\n",
    " 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cross-validation, and binneed splitting to ensure less bias during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 250],\n",
    "    'max_depth': [None, 500],\n",
    "    'min_samples_split': [1, 2],\n",
    "    'min_samples_leaf': [1, 2], \n",
    "    'max_features': ['sqrt'],\n",
    "    'criterion' :['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "rfr = RandomForestRegressor(n_jobs=18, random_state=42)\n",
    "# rfr.fit(X_train, y_train)\n",
    "\n",
    "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "CV_rfr.fit(new_test, y_test)\n",
    "CV_rfr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "# better performance without the blue/green and green/blue bands\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth= None, \n",
    "    criterion='squared_error',  \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt',                   # 'sqrt', 'log2', int, or float\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    n_jobs=-1, \n",
    "    random_state=42, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None, \n",
    "    monotonic_cst=None\n",
    ")\n",
    "\n",
    "# rf_model.fit(traindfs[i], y_train)\n",
    "rf_model.fit(new_train, y_train)\n",
    "\n",
    "\n",
    "# scores = -1 * cross_val_score(rf_model, testdfs[0], y_test, cv=3, scoring= 'neg_root_mean_squared_error', n_jobs=18)\n",
    "# display(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([new_test, new_val], [y_test, y_val], ['test','val']):\n",
    "    predictions = rf_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ['Blue', 'Green', 'Stumpf','X', 'Y', 'Channel_Name_Encoded']\n",
    "test\n",
    "Adjusted R2 score: 0.9556086812104388\n",
    "RMSE Score: 3.330541268867474 ft\n",
    "MAE Score: 1.9717861590043395 ft\n",
    "val\n",
    "Adjusted R2 score: 0.9556332613989742\n",
    "RMSE Score: 3.328907595959278 ft\n",
    "MAE Score: 1.9708214148581906 ft\n",
    "- ['Blue', 'Green', 'Stumpf', 'TI','X', 'Y', 'Channel_Name_Encoded']\n",
    "test\n",
    "Adjusted R2 score: 0.9396524517812164\n",
    "RMSE Score: 3.883250836483786 ft\n",
    "MAE Score: 2.340178418710885 ft\n",
    "val\n",
    "Adjusted R2 score: 0.9397703292743896\n",
    "RMSE Score: 3.878627440716498 ft\n",
    "MAE Score: 2.3377579017410937 ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost Regression\n",
    "\n",
    "n_estimators=500, learning_rate=0.3, max_depth=10, grow_policy= 'lossguide', booster= 'gbtree',:\n",
    "- R2 Score= 0.8529\n",
    "- RMSE= 6.0626\n",
    "- MAE= 3.9175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'min_child_weight': [3, 5, 7],\n",
    "    'gamma': [0.0, 0.1, 0.2],\n",
    "    'colsample_bytree': [0.3, 0.4]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=0)\n",
    "grid_search.fit(new_train, y_train)\n",
    "\n",
    "print(\"Best set of params:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'colsample_bytree': 0.4, 'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and configure the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=250,      # Number of trees\n",
    "    learning_rate=0.5,     # Learning rate\n",
    "    max_depth=20,           # Maximum tree depth\n",
    "    colsample_bytree=0.5,\n",
    "    min_child_weight = 5,\n",
    "    grow_policy= 'lossguide',\n",
    "    booster= 'gbtree',\n",
    "    gamma=0.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42        # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    new_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([new_test, new_val], [y_test, y_val], ['test', 'val']):\n",
    "    predictions = xgb_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test:\n",
    "- R2 Score: 0.8458\n",
    "- Adjusted R2 score: 0.845824251970853\n",
    "- RMSE: 6.2069\n",
    "- MAE: 3.9638\n",
    "\n",
    "val:\n",
    "- R2 Score: 0.8461\n",
    "- Adjusted R2 score: 0.8460827592967077\n",
    "- RMSE: 6.2003\n",
    "- MAE: 3.9620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.63626467, -0.64886007, -0.75592531,  0.42104427, -0.36590657,\n",
       "         0.16617521],\n",
       "       [-0.54603619, -0.66891472, -0.69739843,  0.54931304, -0.42860227,\n",
       "        -0.43332411],\n",
       "       [-0.8479208 ,  0.20874586,  0.57490546,  0.3162081 , -0.59269748,\n",
       "        -0.7730404 ],\n",
       "       ...,\n",
       "       [ 1.22947713,  0.99123907,  0.21191419,  0.8319503 , -0.2587899 ,\n",
       "         0.78565784],\n",
       "       [ 2.0853943 ,  2.44809208,  1.8101134 , -2.01319757,  2.27573198,\n",
       "        -1.31258978],\n",
       "       [ 0.15354529, -0.08303733, -0.60530637,  0.46582672, -0.41047225,\n",
       "         0.1461919 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/tools/miniforge/envs/gis/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 532us/step - loss: 716669760.0000 - root_mean_squared_error: 17322.3145 - val_loss: 249.8996 - val_root_mean_squared_error: 15.8082\n",
      "Epoch 2/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 530us/step - loss: 250.0350 - root_mean_squared_error: 15.8125 - val_loss: 249.7385 - val_root_mean_squared_error: 15.8031\n",
      "Epoch 3/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 533us/step - loss: 250.0809 - root_mean_squared_error: 15.8139 - val_loss: 249.7352 - val_root_mean_squared_error: 15.8030\n",
      "Epoch 4/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 540us/step - loss: 250.0757 - root_mean_squared_error: 15.8138 - val_loss: 249.7781 - val_root_mean_squared_error: 15.8044\n",
      "Epoch 5/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 529us/step - loss: 249.9780 - root_mean_squared_error: 15.8107 - val_loss: 249.7853 - val_root_mean_squared_error: 15.8046\n",
      "Epoch 6/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 531us/step - loss: 250.0691 - root_mean_squared_error: 15.8136 - val_loss: 250.2806 - val_root_mean_squared_error: 15.8203\n",
      "Epoch 7/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 537us/step - loss: 249.9689 - root_mean_squared_error: 15.8104 - val_loss: 249.7838 - val_root_mean_squared_error: 15.8046\n",
      "Epoch 8/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 526us/step - loss: 249.9060 - root_mean_squared_error: 15.8084 - val_loss: 249.7375 - val_root_mean_squared_error: 15.8031\n",
      "Epoch 9/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 525us/step - loss: 249.8463 - root_mean_squared_error: 15.8065 - val_loss: 249.7415 - val_root_mean_squared_error: 15.8032\n",
      "Epoch 10/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 523us/step - loss: 249.8390 - root_mean_squared_error: 15.8063 - val_loss: 249.7355 - val_root_mean_squared_error: 15.8030\n",
      "Epoch 11/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 525us/step - loss: 249.8365 - root_mean_squared_error: 15.8062 - val_loss: 249.7366 - val_root_mean_squared_error: 15.8031\n",
      "Epoch 12/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 526us/step - loss: 249.9237 - root_mean_squared_error: 15.8090 - val_loss: 249.8486 - val_root_mean_squared_error: 15.8066\n",
      "Epoch 13/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 522us/step - loss: 249.8934 - root_mean_squared_error: 15.8080 - val_loss: 249.7767 - val_root_mean_squared_error: 15.8043\n",
      "Epoch 14/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 523us/step - loss: 249.7534 - root_mean_squared_error: 15.8036 - val_loss: 250.0123 - val_root_mean_squared_error: 15.8118\n",
      "Epoch 15/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 521us/step - loss: 249.9431 - root_mean_squared_error: 15.8096 - val_loss: 249.7764 - val_root_mean_squared_error: 15.8043\n",
      "Epoch 16/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 522us/step - loss: 249.8591 - root_mean_squared_error: 15.8069 - val_loss: 249.8609 - val_root_mean_squared_error: 15.8070\n",
      "Epoch 17/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 519us/step - loss: 249.9706 - root_mean_squared_error: 15.8105 - val_loss: 250.0498 - val_root_mean_squared_error: 15.8130\n",
      "Epoch 18/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 522us/step - loss: 249.7117 - root_mean_squared_error: 15.8023 - val_loss: 249.7667 - val_root_mean_squared_error: 15.8040\n",
      "Epoch 19/100\n",
      "\u001b[1m152207/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 524us/step - loss: 249.9084 - root_mean_squared_error: 15.8085 - val_loss: 249.9355 - val_root_mean_squared_error: 15.8093\n",
      "Epoch 20/100\n",
      "\u001b[1m152140/152207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 249.8752 - root_mean_squared_error: 15.8074"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[RootMeanSquaredError()]\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:395\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    386\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    387\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    407\u001b[0m }\n\u001b[1;32m    408\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:484\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    483\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 484\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:138\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_type.py:391\u001b[0m, in \u001b[0;36mFunctionType.unpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    388\u001b[0m flat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sorted_parameters:\n\u001b[1;32m    390\u001b[0m   flat\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 391\u001b[0m       \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m   )\n\u001b[1;32m    394\u001b[0m dealiased_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    395\u001b[0m ids_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:251\u001b[0m, in \u001b[0;36mTypeSpec.to_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See TraceType base class for details. Do not override.\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 251\u001b[0m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_component_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/util/nest.py:628\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1065\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1065\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1103\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1101\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_pack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_composites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:919\u001b[0m, in \u001b[0;36m_tf_core_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flat_structure) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(flat_sequence):\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not pack sequence. Structure had \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m atoms, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflat_sequence had \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m items.  Structure: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, flat_sequence: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(flat_structure), \u001b[38;5;28mlen\u001b[39m(flat_sequence), structure, flat_sequence)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[0;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msequence_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:244\u001b[0m, in \u001b[0;36msequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, _wrapt\u001b[38;5;241m.\u001b[39mObjectProxy):\n\u001b[1;32m    241\u001b[0m   \u001b[38;5;66;03m# For object proxies, first create the underlying type and then re-wrap it\u001b[39;00m\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;66;03m# in the proxy type.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(instance)(sequence_like(instance\u001b[38;5;241m.\u001b[39m__wrapped__, args))\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCustomNestProtocol\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    245\u001b[0m   metadata \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39m__tf_flatten__()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    246\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m__tf_unflatten__(metadata, \u001b[38;5;28mtuple\u001b[39m(args))\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/typing.py:1502\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_runtime_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m _allow_reckless_class_checks(depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1497\u001b[0m ):\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstance and class checks can only be used with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1499\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m @runtime_checkable protocols\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m         \u001b[43m_is_callable_members_only\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m         \u001b[38;5;28missubclass\u001b[39m(instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mcls\u001b[39m)):\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_protocol:\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/typing.py:1427\u001b[0m, in \u001b[0;36m_is_callable_members_only\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_callable_members_only\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# PEP 544 prohibits using issubclass() with protocols that have non-method members.\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, attr, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_get_protocol_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/tools/miniforge/envs/gis/lib/python3.10/typing.py:1420\u001b[0m, in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(base, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(base\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(annotations\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m-> 1420\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_abc_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m attr \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m EXCLUDED_ATTRIBUTES:\n\u001b[1;32m   1421\u001b[0m             attrs\u001b[38;5;241m.\u001b[39madd(attr)\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attrs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(new_train)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(new_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=[RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    new_train, \n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Calculate MSE and R2 score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model (Increased epochs, let early stopping decide when to stop)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    epochs=100, batch_size=32,  # Increased epochs\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Function to inverse transform predictions back to feet\n",
    "def inverse_transform_predictions(y_pred_scaled):\n",
    "    return (y_pred_scaled * y_std) + y_mean  # Reverse standardization\n",
    "\n",
    "# Normalize test features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0)  # Replace NaNs with 0\n",
    "\n",
    "# Predict bathymetry\n",
    "y_pred_scaled = model.predict(X_test_scaled).flatten()  # Flatten ensures it's 1D\n",
    "y_pred = inverse_transform_predictions(y_pred_scaled)  # Convert back to feet\n",
    "\n",
    "# Compute RMSE only on valid values\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} ft\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
