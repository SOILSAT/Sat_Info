{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to develop and compare regression models to correlate the eHydro bathymetric surveys with cloud-masked Sentinel-2 surface refelctances. These models will hopefully provide USACE and the eHydro program with a new, robust, accurate tool for unmanned bathymetric estiamtes. This will be possible at 10-meter resolution at a frequency of up to 5 days.\n",
    "- First starting with XGBoost, RF, and SVM-RBF regressors in the SWG. May try some NN as well\n",
    "- band maths here with the green and blue bands (short wavelengths penetrate water columns more)\n",
    "- include some metadata (AD, CX, BD? Single vs dual beam?)? Will look into more that may be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling, calculate_default_transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize an array\n",
    "def normalize(array):\n",
    "    return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
    "\n",
    "# Function to read .tif files\n",
    "def read_tif(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()  # Read all bands\n",
    "        profile = src.profile  # Get metadata (optional, for reference)\n",
    "    return array, profile\n",
    "\n",
    "def extract_raster_data(pair_tuple):\n",
    "    images_data = []\n",
    "\n",
    "    for paths in pair_tuple:\n",
    "        bathy_path = paths[1]\n",
    "        s2_path = paths[0]\n",
    "\n",
    "        # --- Step 1: Open Bathymetry Raster ---\n",
    "        with rasterio.open(bathy_path) as bathy:\n",
    "            bathy_data = bathy.read(1)  # Bathymetry data (band 1)\n",
    "            bathy_nodata = bathy.nodata  # NoData value\n",
    "            bathy_transform = bathy.transform\n",
    "            bathy_shape = bathy.shape\n",
    "\n",
    "        # --- Step 2: Open Sentinel-2 Raster ---\n",
    "        with rasterio.open(s2_path) as s2:\n",
    "            if s2.shape != bathy_shape or s2.transform != bathy_transform:\n",
    "                raise ValueError(\n",
    "                    f\"Inconsistent shapes or transforms:\\n\"\n",
    "                    f\"Bathymetry Shape: {bathy_shape}, Sentinel-2 Shape: {s2.shape}.\\n\"\n",
    "                    f\"Bathymetry Transform: {bathy_transform}, Sentinel-2 Transform: {s2.transform}.\\n\"\n",
    "                    f\"Ensure rasters have identical extents and resolutions.\"\n",
    "                )\n",
    "\n",
    "            # Read Sentinel-2 bands\n",
    "            bands = {\n",
    "                \"red\": normalize(s2.read(3)),\n",
    "                \"green\": normalize(s2.read(2)),\n",
    "                \"blue\": normalize(s2.read(1)),\n",
    "                \"nir\": normalize(s2.read(4))\n",
    "            }\n",
    "            s2_nodata = s2.nodata  # Sentinel-2 NoData value\n",
    "\n",
    "        # --- Step 3: Flatten Bands ---\n",
    "        flat_bathy = bathy_data.flatten()\n",
    "        flat_bands = {key: band.flatten() for key, band in bands.items()}\n",
    "\n",
    "        # --- Step 4: Mask NoData Values ---\n",
    "        valid_mask = (\n",
    "            ~np.isnan(flat_bathy) &  # Valid bathy pixels\n",
    "            (flat_bathy != bathy_nodata)  # Exclude bathy NoData\n",
    "        )\n",
    "\n",
    "        for band in flat_bands.values():\n",
    "            valid_mask &= (band != s2_nodata)  # Exclude Sentinel-2 NoData\n",
    "\n",
    "        # Apply the mask\n",
    "        valid_bathy = flat_bathy[valid_mask].reshape(-1, 1)  # Reshape bathy to (n_pixels, 1)\n",
    "        valid_features = np.column_stack([band[valid_mask] for band in flat_bands.values()])\n",
    "\n",
    "        # --- Step 5: Combine Features and Targets ---\n",
    "        # combined_features = np.concatenate((valid_bathy, valid_features), axis=1)  # Combine bathy and S2\n",
    "        images_data.append((valid_features, valid_bathy.flatten()))  # Flatten bathy for targets\n",
    "\n",
    "    return images_data\n",
    "\n",
    "def prepare_data(pairs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for features, targets in pairs:\n",
    "        X.append(features[:,1:])  # Keep features from this pair\n",
    "        y.append(targets)   # Keep corresponding targets\n",
    "    return np.vstack(X), np.hstack(y)\n",
    "\n",
    "def survey_name_type(surveynames):\n",
    "    \"\"\"\n",
    "    Will take in the list of surveynames and extract the NCF channel ID and the survey type\n",
    "    \"\"\"\n",
    "    surveytypes = ['AD', 'BD', 'CS', 'PA', 'PR', 'XA', 'XB', 'XC', 'OT', 'DS']\n",
    "\n",
    "    extracted_parts = [re.match(r'^(.*?)_\\d{8}', path).group(1) for path in surveynames if re.match(r'^(.*?)_\\d{8}', path)]\n",
    "    channel_ids = [re.sub(r'^.*?_DIS_', '', path) for path in extracted_parts]\n",
    "\n",
    "    isolated_survey_types = []\n",
    "    for path in surveynames:\n",
    "        for type in surveytypes:\n",
    "            if type in path:\n",
    "                isolated_survey_types.append(type)\n",
    "                break  # Stop checking after the first match\n",
    "    else:\n",
    "        isolated_survey_types.append(None)  # Append None if no match is found`\n",
    "\n",
    "    return channel_ids, isolated_survey_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_PATH = '/home/clay/Documents/SDB/CESWG/processed/S2'\n",
    "BATHY_PATH = '/home/clay/Documents/SDB/CESWG/processed/Bathy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = os.listdir(BATHY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEED TO ADJUST THE EXTRACT_RASTER_DATA FUNCTION TO DEAL WITH THE S2 IMAGERY THAT MAY BE EMPTY (MIGHT JUST DO THIS IN 02 NOTEBOOK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13529/1639838812.py:3: RuntimeWarning: All-NaN slice encountered\n",
      "  return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
      "/tmp/ipykernel_13529/1639838812.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n"
     ]
    }
   ],
   "source": [
    "# get paths to each image for each eHydro-Sentinel2 image pair\n",
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]\n",
    "\n",
    "images = []\n",
    "for name in surveynames:\n",
    "    images.append((os.path.join(S2_PATH, f'{name}.tif'), os.path.join(BATHY_PATH, f'{name}.tif')))\n",
    "\n",
    "images_data = extract_raster_data(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pairs = []\n",
    "goodnames = []\n",
    "for name, pairs in zip(surveynames, images_data):\n",
    "    if pairs[0].shape[0] != 0:\n",
    "        good_pairs.append(pairs)\n",
    "        goodnames.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 band manipulation and including other data (good reference is Chybicki et al. 2023)\n",
    "- Blue Green ratios (Blue/Green, Green/Blue, looking for others)\n",
    "- Stumpf log ratio of blue green (https://aslopubs.onlinelibrary.wiley.com/doi/10.4319/lo.2003.48.1_part_2.0547)\n",
    "- Coordinates\n",
    "- NCF channel ID name\n",
    "- Survey type\n",
    "- Spectral indices?? (NDVI and NDWI, NDWI could make sense but would leave this to the end)\n",
    "\n",
    "\n",
    "The Chybicki 2023 paper had models perform extremely well when including all bands, the Stumpf log ratio, and the UTM coordinates. I think including more blue-green ratios and the survey type as well will increase my accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCF Channel ID from the file names\n",
    "# will be the string before _YYYYMMDD\n",
    "# some may contain district code and DIS ('CESWG_DIS'), will probably need to remove\n",
    "\n",
    "ncf_channels, survey_types = survey_name_type(goodnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_green_ratios(pairs):\n",
    "    s2 = pairs[0]     # in order B, G, R, NIR\n",
    "\n",
    "    bluegreen = s2[:, 1] / s2[:, 2] \n",
    "    greenblue = s2[:, 2] / s2[:, 1] \n",
    "\n",
    "    stumpf = np.log(s2[:, 1]) / np.log(s2[:, 2])\n",
    "\n",
    "    bluegreen = normalize(bluegreen)\n",
    "    greenblue = normalize(greenblue)\n",
    "    stumpf = normalize(stumpf)\n",
    "\n",
    "    return bluegreen, greenblue, stumpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create simple ratios of blue/green and green/blue\n",
    "# blue penetrates, while green attenuates\n",
    "# can check correlation between features and apply PCA if needed\n",
    "\n",
    "for i, pair in enumerate(images_data):\n",
    "    bluegreen, greenblue, stumpf = blue_green_ratios(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates (of survey or S2, they should match)\n",
    "# extract UTM coords\n",
    "\n",
    "file_name = [os.path.join(S2_PATH, f) for f in os.listdir(S2_PATH)][0]\n",
    "with rasterio.open(file_name) as src:\n",
    "            band1 = src.read(1)\n",
    "            print('Band1 has shape', band1.shape)\n",
    "            height = band1.shape[0]\n",
    "            width = band1.shape[1]\n",
    "            cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "            xs, ys = rasterio.transform.xy(src.transform, rows, cols)\n",
    "            lons= np.array(xs)\n",
    "            lats = np.array(ys)\n",
    "            print('lons shape', lons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correlation between each feature\n",
    "\n",
    "df = pd.DataFrame(X_train, columns=['Blue', 'Green', 'Red', 'NIR'])\n",
    "df['Bathymetry'] = y_train\n",
    "\n",
    "# Correlation matrix\n",
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for test_train_split\n",
    "- k-fold segmentation for training?\n",
    "- tiling of images or whole images?\n",
    "- try 3 regression models for now: SVM, RF, and XGBoost\n",
    "- may try ElasticNet from cuML, and some shallow NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split ratios\n",
    "train_ratio = 0.7  # 70% for training\n",
    "val_ratio = 0.2    # 20% for validation\n",
    "test_ratio = 0.1   # 10% for testing\n",
    "\n",
    "train_pairs, temp_pairs = train_test_split(images_data, test_size=(1 - train_ratio), random_state=42)\n",
    "val_pairs, test_pairs = train_test_split(temp_pairs, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for each split\n",
    "X_train, y_train = prepare_data(train_pairs)\n",
    "X_val, y_val = prepare_data(val_pairs)\n",
    "X_test, y_test = prepare_data(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define and configure the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=200,      # Number of trees\n",
    "    learning_rate=0.1,     # Learning rate\n",
    "    max_depth=6,           # Maximum tree depth\n",
    "    random_state=42        # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],  # For validation during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, xgb_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))\n",
    "mae = mean_absolute_error(y_test, xgb_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RF Regression:\n",
    "- R2 Score: 0.3354\n",
    "- RMSE: 12.8882\n",
    "- MAE: 9.9499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, rf_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "mae = mean_absolute_error(y_test, rf_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SVM regression:\n",
    "- R2 Score: \n",
    "- RMSE: \n",
    "- MAE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Model\n",
    "svm_model = SVR()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, svm_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, svm_predictions))\n",
    "mae = mean_absolute_error(y_test, svm_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
