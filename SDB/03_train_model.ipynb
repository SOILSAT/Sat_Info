{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to develop and compare regression models to correlate the eHydro bathymetric surveys with cloud-masked Sentinel-2 surface refelctances. These models will hopefully provide USACE and the eHydro program with a new, robust, accurate tool for unmanned bathymetric estiamtes. This will be possible at 10-meter resolution at a frequency of up to 5 days.\n",
    "- Will train an RF RRegression using RAPIDS/cuML, CatBoost using GPU, a custom NN, and maybe an XGBoost model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import rasterio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(df):\n",
    "    num_columns = min(9, len(df.columns))  # Limit to 7 columns\n",
    "    plt.figure(figsize=(15, 10))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        plt.subplot(3, 3, i + 1)  # Create a grid for plots (3x3 max)\n",
    "        column = df.columns[i]\n",
    "        plt.hist(df[column], bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work directory\n",
    "WORK_DIR = '/mnt/Crucial/SDB/CESWG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from parquet file saved in 02_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_parquet(os.path.join(WORK_DIR,'SDB_data.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(\n",
    "    combined_data.corr(),\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    linewidths=0.5,\n",
    "    annot_kws={\"size\": 10},\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for test_train_split\n",
    "- Trains per pixel\n",
    "\n",
    "\n",
    "- k-fold segmentation for training?\n",
    "- try 3 regression models for now: SVM, RF, and XGBoost\n",
    "- may try ElasticNet from cuML, and some shallow NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_data.drop(columns=['Bathymetry'])\n",
    "y = combined_data['Bathymetry']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp into validation (15%) and test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dataset sizes\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])\n",
    "# X_test.to_csv(os.path.join(WORK_DIR, 'data.csv'), sep='\\t', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of X columns include 'Blue', 'Green', 'Red', 'NIR', 'Blue/Green', 'Green/Blue', 'Stumpf', 'NSMI', 'TI', 'X', 'Y', 'Channel_Name_Encoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcols = ['Blue', 'Green', 'Stumpf','X', 'Y', 'Channel_Name_Encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = X_train[testcols].copy()\n",
    "new_test = X_test[testcols].copy()\n",
    "new_val = X_val[testcols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = list(X_train.columns)\n",
    "constant_cols = Xcols[:4] + Xcols[-3:]\n",
    "non_constant_cols = list(set(Xcols).symmetric_difference(set(constant_cols)))\n",
    "\n",
    "traindfs = []\n",
    "testdfs = []\n",
    "valdfs = []\n",
    "\n",
    "for i in range(len(non_constant_cols)):\n",
    "    constant_cols.append(non_constant_cols[i])\n",
    "    new_X_train = X_train[constant_cols].copy()\n",
    "    new_X_test = X_test[constant_cols].copy()\n",
    "    new_X_val = X_val[constant_cols].copy()\n",
    "    traindfs.append(new_X_train)\n",
    "    testdfs.append(new_X_test)\n",
    "    valdfs.append(new_X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RF Regression:\n",
    "- interesting note, with RF (not sure if all bagging or tree-like) decreases accuracy with increasing variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gridsearch param tuning:\n",
    "1. {'criterion': 'squared_error',\n",
    " 'max_depth': 8,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 200}\n",
    "2. {'criterion': 'squared_error',\n",
    " 'max_depth': 12,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 100}\n",
    "3. {'criterion': 'squared_error',\n",
    " 'max_depth': 20,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 100}\n",
    " 4. {'criterion': 'friedman_mse',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_leaf': 2,\n",
    " 'min_samples_split': 2,\n",
    " 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cross-validation, and binneed splitting to ensure less bias during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 250],\n",
    "    'max_depth': [None, 500],\n",
    "    'min_samples_split': [1, 2],\n",
    "    'min_samples_leaf': [1, 2], \n",
    "    'max_features': ['sqrt'],\n",
    "    'criterion' :['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "rfr = RandomForestRegressor(n_jobs=18, random_state=42)\n",
    "# rfr.fit(X_train, y_train)\n",
    "\n",
    "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "CV_rfr.fit(new_test, y_test)\n",
    "CV_rfr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "# better performance without the blue/green and green/blue bands\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth= None, \n",
    "    criterion='squared_error',  \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt',                   # 'sqrt', 'log2', int, or float\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    n_jobs=-1, \n",
    "    random_state=42, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None, \n",
    "    monotonic_cst=None\n",
    ")\n",
    "\n",
    "# rf_model.fit(traindfs[i], y_train)\n",
    "rf_model.fit(new_train, y_train)\n",
    "\n",
    "\n",
    "# scores = -1 * cross_val_score(rf_model, testdfs[0], y_test, cv=3, scoring= 'neg_root_mean_squared_error', n_jobs=18)\n",
    "# display(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([new_test, new_val], [y_test, y_val], ['test','val']):\n",
    "    predictions = rf_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ['Blue', 'Green', 'Stumpf','X', 'Y', 'Channel_Name_Encoded']\n",
    "test\n",
    "Adjusted R2 score: 0.9556086812104388\n",
    "RMSE Score: 3.330541268867474 ft\n",
    "MAE Score: 1.9717861590043395 ft\n",
    "val\n",
    "Adjusted R2 score: 0.9556332613989742\n",
    "RMSE Score: 3.328907595959278 ft\n",
    "MAE Score: 1.9708214148581906 ft\n",
    "- ['Blue', 'Green', 'Stumpf', 'TI','X', 'Y', 'Channel_Name_Encoded']\n",
    "test\n",
    "Adjusted R2 score: 0.9396524517812164\n",
    "RMSE Score: 3.883250836483786 ft\n",
    "MAE Score: 2.340178418710885 ft\n",
    "val\n",
    "Adjusted R2 score: 0.9397703292743896\n",
    "RMSE Score: 3.878627440716498 ft\n",
    "MAE Score: 2.3377579017410937 ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost Regression\n",
    "\n",
    "n_estimators=500, learning_rate=0.3, max_depth=10, grow_policy= 'lossguide', booster= 'gbtree',:\n",
    "- R2 Score= 0.8529\n",
    "- RMSE= 6.0626\n",
    "- MAE= 3.9175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'min_child_weight': [3, 5, 7],\n",
    "    'gamma': [0.0, 0.1, 0.2],\n",
    "    'colsample_bytree': [0.3, 0.4]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=0)\n",
    "grid_search.fit(new_train, y_train)\n",
    "\n",
    "print(\"Best set of params:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'colsample_bytree': 0.4, 'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and configure the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=250,      # Number of trees\n",
    "    learning_rate=0.5,     # Learning rate\n",
    "    max_depth=20,           # Maximum tree depth\n",
    "    colsample_bytree=0.5,\n",
    "    min_child_weight = 5,\n",
    "    grow_policy= 'lossguide',\n",
    "    booster= 'gbtree',\n",
    "    gamma=0.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42        # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    new_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([new_test, new_val], [y_test, y_val], ['test', 'val']):\n",
    "    predictions = xgb_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test:\n",
    "- R2 Score: 0.8458\n",
    "- Adjusted R2 score: 0.845824251970853\n",
    "- RMSE: 6.2069\n",
    "- MAE: 3.9638\n",
    "\n",
    "val:\n",
    "- R2 Score: 0.8461\n",
    "- Adjusted R2 score: 0.8460827592967077\n",
    "- RMSE: 6.2003\n",
    "- MAE: 3.9620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(new_train)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(new_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=[RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    new_train, \n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Calculate MSE and R2 score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model (Increased epochs, let early stopping decide when to stop)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    epochs=100, batch_size=32,  # Increased epochs\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Function to inverse transform predictions back to feet\n",
    "def inverse_transform_predictions(y_pred_scaled):\n",
    "    return (y_pred_scaled * y_std) + y_mean  # Reverse standardization\n",
    "\n",
    "# Normalize test features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0)  # Replace NaNs with 0\n",
    "\n",
    "# Predict bathymetry\n",
    "y_pred_scaled = model.predict(X_test_scaled).flatten()  # Flatten ensures it's 1D\n",
    "y_pred = inverse_transform_predictions(y_pred_scaled)  # Convert back to feet\n",
    "\n",
    "# Compute RMSE only on valid values\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} ft\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
