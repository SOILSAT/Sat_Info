{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to develop and compare regression models to correlate the eHydro bathymetric surveys with cloud-masked Sentinel-2 surface refelctances. These models will hopefully provide USACE and the eHydro program with a new, robust, accurate tool for unmanned bathymetric estiamtes. This will be possible at 10-meter resolution at a frequency of up to 5 days.\n",
    "- First starting with XGBoost, RF, and SVM-RBF regressors in the SWG. May try some NN as well\n",
    "- band maths here with the green and blue bands (short wavelengths penetrate water columns more)\n",
    "- include some metadata (AD, CX, BD? Single vs dual beam?)? Will look into more that may be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import rasterio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize an array\n",
    "def normalize(array):\n",
    "    return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
    "\n",
    "# Function to read .tif files\n",
    "def read_tif(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()  # Read all bands\n",
    "        profile = src.profile  # Get metadata (optional, for reference)\n",
    "    return array, profile\n",
    "\n",
    "def extract_raster_data(pair_tuple):\n",
    "    images_data = []\n",
    "\n",
    "    bathy_path = pair_tuple[1]\n",
    "    s2_path = pair_tuple[0]\n",
    "\n",
    "        # --- Step 1: Open Bathymetry Raster ---\n",
    "    with rasterio.open(bathy_path) as bathy:\n",
    "        bathy_data = bathy.read(1)  # Bathymetry data (band 1)\n",
    "        bathy_nodata = bathy.nodata  # NoData value\n",
    "        bathy_transform = bathy.transform\n",
    "        bathy_shape = bathy.shape\n",
    "\n",
    "        # --- Step 2: Open Sentinel-2 Raster ---\n",
    "    with rasterio.open(s2_path) as s2:\n",
    "        if s2.shape != bathy_shape or s2.transform != bathy_transform:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent shapes or transforms:\\n\"\n",
    "                f\"Bathymetry Shape: {bathy_shape}, Sentinel-2 Shape: {s2.shape}.\\n\"\n",
    "                f\"Bathymetry Transform: {bathy_transform}, Sentinel-2 Transform: {s2.transform}.\\n\"\n",
    "                f\"Ensure rasters have identical extents and resolutions.\"\n",
    "            )\n",
    "        \n",
    "        # Read Sentinel-2 bands\n",
    "        bands = {\n",
    "            \"red\": normalize(s2.read(3)),\n",
    "            \"green\": normalize(s2.read(2)),\n",
    "            \"blue\": normalize(s2.read(1)),\n",
    "            \"nir\": normalize(s2.read(4)),\n",
    "            \"blue/green\": normalize(s2.read(1) / s2.read(2)),\n",
    "            \"green/blue\": normalize(s2.read(2) / s2.read(1)),\n",
    "            \"stumpf\": normalize(np.log(s2.read(1) / np.log(s2.read(2)))),\n",
    "            \"nsmi\": normalize(s2.read(3) / s2.read(2)),\n",
    "            \"ti\": normalize((s2.read(3) - s2.read(2)) / (s2.read(3) + s2.read(2)))\n",
    "        }\n",
    "\n",
    "    # transformed_bands = quantile_transform(bands)\n",
    "    s2_nodata = s2.nodata  # Sentinel-2 NoData value\n",
    "\n",
    "    # --- Step 3: Flatten Bands ---\n",
    "    flat_bathy = bathy_data.flatten()\n",
    "    flat_bands = {key: band.flatten() for key, band in bands.items()}\n",
    "\n",
    "    # --- Step 4: Mask NoData Values ---\n",
    "    valid_mask = (\n",
    "        ~np.isnan(flat_bathy) &  # Valid bathy pixels\n",
    "        (flat_bathy != bathy_nodata)  # Exclude bathy NoData\n",
    "    )\n",
    "\n",
    "    for band in flat_bands.values():\n",
    "        valid_mask &= (band != s2_nodata)  # Exclude Sentinel-2 NoData\n",
    "        \n",
    "        \n",
    "    # Apply the mask\n",
    "    valid_bathy = flat_bathy[valid_mask].reshape(-1, 1)  # Reshape bathy to (n_pixels, 1)\n",
    "    valid_features = np.column_stack([band[valid_mask] for band in flat_bands.values()])\n",
    "\n",
    "    # --- Step 5: Combine Features and Targets ---\n",
    "    # combined_features = np.concatenate((valid_bathy, valid_features), axis=1)  # Combine bathy and S2\n",
    "    images_data.append((valid_features, valid_bathy.flatten()))  # Flatten bathy for targets\n",
    "\n",
    "    return images_data\n",
    "\n",
    "def prepare_data(pairs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for features, targets in pairs:\n",
    "        X.append(features[:,1:])  # Keep features from this pair\n",
    "        y.append(targets)   # Keep corresponding targets\n",
    "    return np.vstack(X), np.hstack(y)\n",
    "\n",
    "def survey_name_type(surveynames):\n",
    "    \"\"\"\n",
    "    Will take in the list of surveynames and extract the NCF channel ID and the survey type\n",
    "    \"\"\"\n",
    "    surveytypes = ['AD', 'BD', 'CS', 'PA', 'PR', 'XA', 'XB', 'XC', 'OT', 'DS']\n",
    "\n",
    "    extracted_parts = [re.match(r'^(.*?)_\\d{8}', path).group(1) for path in surveynames if re.match(r'^(.*?)_\\d{8}', path)]\n",
    "    channel_ids = [re.sub(r'^.*?_DIS_', '', path) for path in extracted_parts]\n",
    "\n",
    "    isolated_survey_types = []\n",
    "    for path in surveynames:\n",
    "        for type in surveytypes:\n",
    "            if type in path:\n",
    "                isolated_survey_types.append(type)\n",
    "                break  # Stop checking after the first match\n",
    "\n",
    "    return channel_ids, isolated_survey_types\n",
    "\n",
    "def create_composite_bands_with_existing(flattened_s2):\n",
    "    if flattened_s2.shape[1] != 4:\n",
    "        raise ValueError(\"Input array must have 4 columns representing B, G, R, NIR bands.\")\n",
    "\n",
    "    # Split the bands\n",
    "    blue = flattened_s2[:, 0]\n",
    "    green = flattened_s2[:, 1]\n",
    "    red = flattened_s2[:, 2]\n",
    "\n",
    "    # Compute composite bands\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        bluegreen = np.divide(blue, green, out=np.zeros_like(blue), where=green != 0)\n",
    "        greenblue = np.divide(green, blue, out=np.zeros_like(green), where=blue != 0)\n",
    "        stumpf = np.divide(np.log(blue + 1e-6), np.log(green + 1e-6), out=np.zeros_like(blue), where=(green > 0) & (blue > 0))\n",
    "        nsmi = np.divide(red, green, out=np.zeros_like(red), where=green != 0)\n",
    "        ti = np.divide((red - green), (red + green), out=np.zeros_like(red), where=(green > 0) & (red > 0))\n",
    "\n",
    "    # Normalize composite bands\n",
    "    bluegreen = normalize(bluegreen)\n",
    "    greenblue = normalize(greenblue)\n",
    "    stumpf = normalize(stumpf)\n",
    "    nsmi = normalize(nsmi)\n",
    "    ti = normalize(ti)\n",
    "\n",
    "    # Combine all bands\n",
    "    combined_array = np.hstack((flattened_s2, bluegreen[:, None], greenblue[:, None], stumpf[:, None], nsmi[:, None], ti[:, None]))\n",
    "\n",
    "    return combined_array\n",
    "\n",
    "def get_pixel_positions(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Get the affine transformation of the raster\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Read the first band to determine valid (non-NaN) pixels\n",
    "        band_data = src.read(1, masked=True)  # Read the first band as a masked array\n",
    "        valid_mask = ~band_data.mask          # Valid pixels where mask is False\n",
    "\n",
    "        # Get raster dimensions\n",
    "        height, width = src.height, src.width\n",
    "\n",
    "        # Create arrays of pixel indices\n",
    "        row_indices, col_indices = np.meshgrid(np.arange(height), np.arange(width), indexing=\"ij\")\n",
    "\n",
    "        # Compute x, y positions using the affine transform\n",
    "        xs, ys = rasterio.transform.xy(transform, row_indices, col_indices, offset='center')\n",
    "        xs = np.array(xs).flatten()\n",
    "        ys = np.array(ys).flatten()\n",
    "\n",
    "        # Filter x, y positions to include only valid pixels\n",
    "        valid_positions = np.column_stack((xs[valid_mask.flatten()], ys[valid_mask.flatten()]))\n",
    "\n",
    "    return valid_positions\n",
    "\n",
    "def prepare_train_data(surveynames):\n",
    "    pairs = [(os.path.join(S2_PATH, f'{name}.tif'), os.path.join(BATHY_PATH, f'{name}.tif')) for name in surveynames]\n",
    "    \n",
    "    good_pairs = []\n",
    "    goodnames = []\n",
    "    for name, pair in zip(surveynames, pairs):\n",
    "        with rasterio.open(pair[0]) as src:\n",
    "            band = src.read(1)\n",
    "            if band.shape[0] != 0:\n",
    "                good_pairs.append(pair)\n",
    "                goodnames.append(name)\n",
    "\n",
    "    images_data = [extract_raster_data(pair) for pair in good_pairs]\n",
    "    ncf_channels, survey_types = survey_name_type(goodnames)\n",
    "    # all_bands = [create_composite_bands_with_existing(pair[0]) for pair in images_data]\n",
    "    pixel_positions = [get_pixel_positions(os.path.join(S2_PATH, f'{name}.tif')) for name in goodnames]\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    for i, name in enumerate(goodnames):\n",
    "        # Extract data for the current iteration\n",
    "        bands = images_data[i]               # Shape (n_pixels, 7)\n",
    "        positions = pixel_positions[i]     # Shape (n_pixels, 2)\n",
    "    \n",
    "        data[name] = pd.DataFrame({\n",
    "                \"Blue\": bands[0][0][:, 0],\n",
    "                \"Green\": bands[0][0][:, 1],\n",
    "                \"Red\": bands[0][0][:, 2],\n",
    "                \"NIR\": bands[0][0][:, 3],\n",
    "                \"Blue/Green\": bands[0][0][:, 4],\n",
    "                \"Green/Blue\": bands[0][0][:, 5],\n",
    "                \"Stumpf\": bands[0][0][:, 6],\n",
    "                \"NSMI\": bands[0][0][:, 7],\n",
    "                \"TI\": bands[0][0][:, 8],\n",
    "                \"X\": positions[:, 0],\n",
    "                \"Y\": positions[:, 1],\n",
    "                \"Channel_Name\": [ncf_channels[i]] * len(bands[0][0]),  # Repeating value directly\n",
    "                \"Bathymetry\": bands[0][1]\n",
    "            })\n",
    "\n",
    "    combined_df = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    combined_df['Channel_Name_Encoded'] = encoder.fit_transform(combined_df['Channel_Name'])\n",
    "\n",
    "    output = open(os.path.join(WORK_DIR, 'Channel_Name_label_encoders.pkl'), 'wb')\n",
    "    pickle.dump(encoder, output)\n",
    "    output.close()\n",
    "    \n",
    "    # Drop original categorical columns\n",
    "    combined_df.drop(columns=['Channel_Name'], inplace=True)\n",
    "\n",
    "    X = combined_df.drop(columns=['Bathymetry'])\n",
    "    y = combined_df['Bathymetry']\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Split temp into validation (15%) and test (15%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "def plot_histograms(df):\n",
    "    num_columns = min(9, len(df.columns))  # Limit to 7 columns\n",
    "    plt.figure(figsize=(15, 10))  # Adjust the figure size\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        plt.subplot(3, 3, i + 1)  # Create a grid for plots (3x3 max)\n",
    "        column = df.columns[i]\n",
    "        plt.hist(df[column], bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work directory\n",
    "WORK_DIR = '/home/clay/Documents/SDB/CESWG'\n",
    "\n",
    "S2_PATH =os.path.join(WORK_DIR, 'processed/S2')\n",
    "BATHY_PATH = os.path.join(WORK_DIR,'processed/Bathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 band manipulation and including other data (good reference is Chybicki et al. 2023)\n",
    "- Blue Green ratios (Blue/Green, Green/Blue)\n",
    "- NSMI (Red/Green)\n",
    "- Turbidity Index ((Red-Green) / (Red+Green))\n",
    "- Stumpf log ratio of blue green (https://aslopubs.onlinelibrary.wiley.com/doi/10.4319/lo.2003.48.1_part_2.0547)\n",
    "- Coordinates\n",
    "- NCF channel ID name\n",
    "\n",
    "\n",
    "The Chybicki 2023 paper had models perform extremely well when including all bands, the Stumpf log ratio, and the UTM coordinates. I think including more blue-green ratios and the survey type as well will increase my accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = prepare_train_data(surveynames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glint correction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of values in the X and y variable test, train, and val sample sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, set in zip(['train', 'test', 'val'],[X_train, X_test, X_val]):\n",
    "    print(name)\n",
    "    plot_histograms(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, set in zip(['train', 'test', 'val'], [y_train, y_test, y_val]):\n",
    "    sns.histplot(set, kde=True, bins=50)\n",
    "    plt.title(f\"Distribution of Bathymetry Values ({name})\")\n",
    "    plt.xlabel(\"Bathymetry\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for test_train_split\n",
    "- Trains per pixel\n",
    "\n",
    "\n",
    "- k-fold segmentation for training?\n",
    "- try 3 regression models for now: SVM, RF, and XGBoost\n",
    "- may try ElasticNet from cuML, and some shallow NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dataset sizes\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])\n",
    "# X_test.to_csv(os.path.join(WORK_DIR, 'data.csv'), sep='\\t', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of X columns include 'Blue', 'Green', 'Red', 'NIR', 'Blue/Green', 'Green/Blue', 'Stumpf', 'NSMI', 'TI', 'X', 'Y', 'Channel_Name_Encoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = list(X_train.columns)\n",
    "constant_cols = Xcols[:4] + Xcols[-3:]\n",
    "non_constant_cols = list(set(Xcols).symmetric_difference(set(constant_cols)))\n",
    "\n",
    "traindfs = []\n",
    "testdfs = []\n",
    "valdfs = []\n",
    "\n",
    "for i in range(len(non_constant_cols)):\n",
    "    constant_cols.append(non_constant_cols[i])\n",
    "    new_X_train = X_train[constant_cols].copy()\n",
    "    new_X_test = X_test[constant_cols].copy()\n",
    "    new_X_val = X_val[constant_cols].copy()\n",
    "    traindfs.append(new_X_train)\n",
    "    testdfs.append(new_X_test)\n",
    "    valdfs.append(new_X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RF Regression:\n",
    "- interesting note, with RF (not sure if all bagging or tree-like) decreases accuracy with increasing variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gridsearch param tuning:\n",
    "1. {'criterion': 'squared_error',\n",
    " 'max_depth': 8,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 200}\n",
    "2. {'criterion': 'squared_error',\n",
    " 'max_depth': 12,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 100}\n",
    "3. {'criterion': 'squared_error',\n",
    " 'max_depth': 20,\n",
    " 'max_features': 'sqrt',\n",
    " 'n_estimators': 100}\n",
    " 4. {'criterion': 'friedman_mse',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_leaf': 2,\n",
    " 'min_samples_split': 2,\n",
    " 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cross-validation, and binneed splitting to ensure less bias during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_depth': [None, 500],\n",
    "    'min_samples_split': [1, 2],\n",
    "    'min_samples_leaf': [1, 2], \n",
    "    'max_features': ['sqrt'],\n",
    "    'criterion' :['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "rfr = RandomForestRegressor(n_jobs=18, random_state=42)\n",
    "# rfr.fit(X_train, y_train)\n",
    "\n",
    "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
    "CV_rfr.fit(X_test, y_test)\n",
    "CV_rfr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 191min of training (initial run):\n",
    "- R2 Score: 0.9818\n",
    "- RMSE: 2.1344      (ft)\n",
    "- MAE: 1.0686       (ft)\n",
    "\n",
    "Using GridSearchCV above, I narrowed down parameters:\n",
    "- R2 Score: \n",
    "- RMSE:       (ft)\n",
    "- MAE:        (ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth= None, \n",
    "    criterion='squared_error',  \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt',                   # 'sqrt', 'log2', int, or float\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    n_jobs=18, \n",
    "    random_state=42, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None, \n",
    "    monotonic_cst=None\n",
    ")\n",
    "\n",
    "rf_model.fit(traindfs[i], y_train)\n",
    "\n",
    "# scores = -1 * cross_val_score(rf_model, testdfs[0], y_test, cv=3, scoring= 'neg_root_mean_squared_error', n_jobs=18)\n",
    "# display(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([testdfs[i], valdfs[i]], [y_test, y_val], ['test','val']):\n",
    "    predictions = rf_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val(0):\n",
    "Adjusted R2 score: 0.8816193870084346\n",
    "RMSE Score: 5.437671243995512 ft\n",
    "MAE Score: 3.4949532730405344 ft\n",
    "\n",
    "val(1):\n",
    "Adjusted R2 score: 0.8955754452855905\n",
    "RMSE Score: 5.107093446409825 ft\n",
    "MAE Score: 3.191820378005034 ft\n",
    "\n",
    "val(2):\n",
    "Adjusted R2 score: 0.8845666169437363\n",
    "RMSE Score: 5.369551769105191 ft\n",
    "MAE Score: 3.4056150796380136 ft\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([new_X_test, new_X_val], [y_test, y_val], ['test', 'val']):\n",
    "    predictions = rf_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test:\n",
    "- Adjusted R2 score: 0.939464924386474\n",
    "- RMSE Score: 3.8892752017609955 ft\n",
    "- MAE Score: 2.329548394748036 ft\n",
    "\n",
    "\n",
    "val:\n",
    "- Adjusted R2 score: 0.9395559615574077\n",
    "- RMSE Score: 3.885519192325711 ft\n",
    "- MAE Score: 2.328350710126916 ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGBoost Regression\n",
    "\n",
    "n_estimators=500, learning_rate=0.3, max_depth=10, grow_policy= 'lossguide', booster= 'gbtree',:\n",
    "- R2 Score= 0.8529\n",
    "- RMSE= 6.0626\n",
    "- MAE= 3.9175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and configure the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    learning_rate=0.5,     # Learning rate\n",
    "    max_depth=,           # Maximum tree depth\n",
    "    grow_policy= 'lossguide',\n",
    "    booster= 'gbtree',\n",
    "    random_state=42        # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_metrics = {}\n",
    "\n",
    "for pair in zip([X_test, X_val], [y_test, y_val], ['test', 'val']):\n",
    "    predictions = xgb_model.predict(pair[0])\n",
    "\n",
    "    train_val_metrics[pair[2]] = [r2_score(pair[1], predictions), np.sqrt(mean_squared_error(pair[1], predictions)), mean_absolute_error(pair[1], predictions)]\n",
    "    \n",
    "    print(f\"{pair[2]}\")\n",
    "    # print(f\"R2 Score: {train_val_metrics[pair[2]][0]}\")\n",
    "\n",
    "    a_r2 = 1 - ((1 - train_val_metrics[pair[2]][0]) * (pair[0].shape[0] - 1)) / (pair[0].shape[0] - pair[0].shape[1] - 1)\n",
    "    print(f\"Adjusted R2 score: {a_r2}\")\n",
    "\n",
    "    print(f\"RMSE Score: {train_val_metrics[pair[2]][1]} ft\")\n",
    "    print(f\"MAE Score: {train_val_metrics[pair[2]][2]} ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test:\n",
    "- R2 Score: 0.8458\n",
    "- Adjusted R2 score: 0.845824251970853\n",
    "- RMSE: 6.2069\n",
    "- MAE: 3.9638\n",
    "\n",
    "val:\n",
    "- R2 Score: 0.8461\n",
    "- Adjusted R2 score: 0.8460827592967077\n",
    "- RMSE: 6.2003\n",
    "- MAE: 3.9620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input features (standardization: mean 0, std 1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Returns a NumPy array\n",
    "\n",
    "# Convert back to a DataFrame (Optional: If you prefer DataFrame operations)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# Standardize the target variable (bathymetry depth)\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()\n",
    "y_train_scaled = (y_train - y_mean) / y_std  # Standardization\n",
    "\n",
    "# Define the model using Pandas-compatible input\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled_df.shape[1],)),  # Input layer\n",
    "\n",
    "    layers.Dense(128),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Dense(64),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dropout(0.2),  # Only one dropout layer\n",
    "    \n",
    "    layers.Dense(32),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    \n",
    "    layers.Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "# model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model (Increased epochs, let early stopping decide when to stop)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    epochs=100, batch_size=32,  # Increased epochs\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Function to inverse transform predictions back to feet\n",
    "def inverse_transform_predictions(y_pred_scaled):\n",
    "    return (y_pred_scaled * y_std) + y_mean  # Reverse standardization\n",
    "\n",
    "# Normalize test features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0)  # Replace NaNs with 0\n",
    "\n",
    "# Predict bathymetry\n",
    "y_pred_scaled = model.predict(X_test_scaled).flatten()  # Flatten ensures it's 1D\n",
    "y_pred = inverse_transform_predictions(y_pred_scaled)  # Convert back to feet\n",
    "\n",
    "# Compute RMSE only on valid values\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} ft\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
