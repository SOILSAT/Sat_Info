{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to develop and compare regression models to correlate the eHydro bathymetric surveys with cloud-masked Sentinel-2 surface refelctances. These models will hopefully provide USACE and the eHydro program with a new, robust, accurate tool for unmanned bathymetric estiamtes. This will be possible at 10-meter resolution at a frequency of up to 5 days.\n",
    "- First starting with XGBoost, RF, and SVM-RBF regressors in the SWG. May try some NN as well\n",
    "- band maths here with the green and blue bands (short wavelengths penetrate water columns more)\n",
    "- include some metadata (AD, CX, BD? Single vs dual beam?)? Will look into more that may be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import rasterio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize an array\n",
    "def normalize(array):\n",
    "    return (array - np.nanmin(array)) / (np.nanmax(array) - np.nanmin(array))\n",
    "\n",
    "# Function to read .tif files\n",
    "def read_tif(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()  # Read all bands\n",
    "        profile = src.profile  # Get metadata (optional, for reference)\n",
    "    return array, profile\n",
    "\n",
    "def extract_raster_data(pair_tuple):\n",
    "    images_data = []\n",
    "\n",
    "    for paths in pair_tuple:\n",
    "        bathy_path = paths[1]\n",
    "        s2_path = paths[0]\n",
    "\n",
    "        # --- Step 1: Open Bathymetry Raster ---\n",
    "        with rasterio.open(bathy_path) as bathy:\n",
    "            bathy_data = bathy.read(1)  # Bathymetry data (band 1)\n",
    "            bathy_nodata = bathy.nodata  # NoData value\n",
    "            bathy_transform = bathy.transform\n",
    "            bathy_shape = bathy.shape\n",
    "\n",
    "        # --- Step 2: Open Sentinel-2 Raster ---\n",
    "        with rasterio.open(s2_path) as s2:\n",
    "            if s2.shape != bathy_shape or s2.transform != bathy_transform:\n",
    "                raise ValueError(\n",
    "                    f\"Inconsistent shapes or transforms:\\n\"\n",
    "                    f\"Bathymetry Shape: {bathy_shape}, Sentinel-2 Shape: {s2.shape}.\\n\"\n",
    "                    f\"Bathymetry Transform: {bathy_transform}, Sentinel-2 Transform: {s2.transform}.\\n\"\n",
    "                    f\"Ensure rasters have identical extents and resolutions.\"\n",
    "                )\n",
    "\n",
    "            # Read Sentinel-2 bands\n",
    "            bands = {\n",
    "                \"red\": normalize(s2.read(3)),\n",
    "                \"green\": normalize(s2.read(2)),\n",
    "                \"blue\": normalize(s2.read(1)),\n",
    "                \"nir\": normalize(s2.read(4))\n",
    "            }\n",
    "\n",
    "            s2_nodata = s2.nodata  # Sentinel-2 NoData value\n",
    "\n",
    "        # --- Step 3: Flatten Bands ---\n",
    "        flat_bathy = bathy_data.flatten()\n",
    "        flat_bands = {key: band.flatten() for key, band in bands.items()}\n",
    "\n",
    "        # --- Step 4: Mask NoData Values ---\n",
    "        valid_mask = (\n",
    "            ~np.isnan(flat_bathy) &  # Valid bathy pixels\n",
    "            (flat_bathy != bathy_nodata)  # Exclude bathy NoData\n",
    "        )\n",
    "\n",
    "        for band in flat_bands.values():\n",
    "            valid_mask &= (band != s2_nodata)  # Exclude Sentinel-2 NoData\n",
    "\n",
    "        # Apply the mask\n",
    "        valid_bathy = flat_bathy[valid_mask].reshape(-1, 1)  # Reshape bathy to (n_pixels, 1)\n",
    "        valid_features = np.column_stack([band[valid_mask] for band in flat_bands.values()])\n",
    "\n",
    "        # --- Step 5: Combine Features and Targets ---\n",
    "        # combined_features = np.concatenate((valid_bathy, valid_features), axis=1)  # Combine bathy and S2\n",
    "        images_data.append((valid_features, valid_bathy.flatten()))  # Flatten bathy for targets\n",
    "\n",
    "    return images_data\n",
    "\n",
    "def prepare_data(pairs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for features, targets in pairs:\n",
    "        X.append(features[:,1:])  # Keep features from this pair\n",
    "        y.append(targets)   # Keep corresponding targets\n",
    "    return np.vstack(X), np.hstack(y)\n",
    "\n",
    "def survey_name_type(surveynames):\n",
    "    \"\"\"\n",
    "    Will take in the list of surveynames and extract the NCF channel ID and the survey type\n",
    "    \"\"\"\n",
    "    surveytypes = ['AD', 'BD', 'CS', 'PA', 'PR', 'XA', 'XB', 'XC', 'OT', 'DS']\n",
    "\n",
    "    extracted_parts = [re.match(r'^(.*?)_\\d{8}', path).group(1) for path in surveynames if re.match(r'^(.*?)_\\d{8}', path)]\n",
    "    channel_ids = [re.sub(r'^.*?_DIS_', '', path) for path in extracted_parts]\n",
    "\n",
    "    isolated_survey_types = []\n",
    "    for path in surveynames:\n",
    "        for type in surveytypes:\n",
    "            if type in path:\n",
    "                isolated_survey_types.append(type)\n",
    "                break  # Stop checking after the first match\n",
    "\n",
    "    return channel_ids, isolated_survey_types\n",
    "\n",
    "def create_composite_bands_with_existing(flattened_s2):\n",
    "    if flattened_s2.shape[1] != 4:\n",
    "        raise ValueError(\"Input array must have 4 columns representing B, G, R, NIR bands.\")\n",
    "\n",
    "    # Split the bands\n",
    "    blue = flattened_s2[:, 0]\n",
    "    green = flattened_s2[:, 1]\n",
    "\n",
    "    # Compute composite bands\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        bluegreen = np.divide(blue, green, out=np.zeros_like(blue), where=green != 0)\n",
    "        greenblue = np.divide(green, blue, out=np.zeros_like(green), where=blue != 0)\n",
    "        stumpf = np.divide(\n",
    "            np.log(blue + 1e-6), np.log(green + 1e-6), out=np.zeros_like(blue), where=(green > 0) & (blue > 0)\n",
    "        )\n",
    "\n",
    "    # Normalize composite bands\n",
    "    bluegreen = normalize(bluegreen)\n",
    "    greenblue = normalize(greenblue)\n",
    "    stumpf = normalize(stumpf)\n",
    "\n",
    "    # Combine all bands\n",
    "    combined_array = np.hstack((flattened_s2, bluegreen[:, None], greenblue[:, None], stumpf[:, None]))\n",
    "\n",
    "    return combined_array\n",
    "\n",
    "def get_pixel_positions(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Get the affine transformation of the raster\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Read the first band to determine valid (non-NaN) pixels\n",
    "        band_data = src.read(1, masked=True)  # Read the first band as a masked array\n",
    "        valid_mask = ~band_data.mask          # Valid pixels where mask is False\n",
    "\n",
    "        # Get raster dimensions\n",
    "        height, width = src.height, src.width\n",
    "\n",
    "        # Create arrays of pixel indices\n",
    "        row_indices, col_indices = np.meshgrid(np.arange(height), np.arange(width), indexing=\"ij\")\n",
    "\n",
    "        # Compute x, y positions using the affine transform\n",
    "        xs, ys = rasterio.transform.xy(transform, row_indices, col_indices, offset='center')\n",
    "        xs = np.array(xs).flatten()\n",
    "        ys = np.array(ys).flatten()\n",
    "\n",
    "        # Filter x, y positions to include only valid pixels\n",
    "        valid_positions = np.column_stack((xs[valid_mask.flatten()], ys[valid_mask.flatten()]))\n",
    "\n",
    "    return valid_positions\n",
    "\n",
    "def prepare_train_data(surveynames):\n",
    "    images = [(os.path.join(S2_PATH, f'{name}.tif'), os.path.join(BATHY_PATH, f'{name}.tif')) for name in surveynames]\n",
    "    images_data = extract_raster_data(images)\n",
    "\n",
    "    good_pairs = []\n",
    "    goodnames = []\n",
    "    for name, pairs in zip(surveynames, images_data):\n",
    "        if pairs[0].shape[0] != 0:\n",
    "            good_pairs.append(pairs)\n",
    "            goodnames.append(name)\n",
    "\n",
    "    ncf_channels, survey_types = survey_name_type(goodnames)\n",
    "    all_bands = [create_composite_bands_with_existing(pair[0]) for pair in good_pairs]\n",
    "    pixel_positions = [get_pixel_positions(os.path.join(S2_PATH, f'{name}.tif')) for name in goodnames]\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    # Iterate through the names and corresponding data\n",
    "    for i, name in enumerate(goodnames):\n",
    "        # Extract data for the current iteration\n",
    "        bands = all_bands[i]               # Shape (n_pixels, 7)\n",
    "        positions = pixel_positions[i]     # Shape (n_pixels, 2)\n",
    "        bathymetry = good_pairs[i][1]      # Shape (n_pixels,)\n",
    "\n",
    "        # Create a dataframe directly using a dictionary comprehension\n",
    "        data[name] = pd.DataFrame({\n",
    "            \"Blue\": bands[:, 0],\n",
    "            \"Green\": bands[:, 1],\n",
    "            \"Red\": bands[:, 2],\n",
    "            \"NIR\": bands[:, 3],\n",
    "            \"Blue/Green\": bands[:, 4],\n",
    "            \"Green/Blue\": bands[:, 5],\n",
    "            \"Stumpf\": bands[:, 6],\n",
    "            \"X\": positions[:, 0],\n",
    "            \"Y\": positions[:, 1],\n",
    "            \"Channel_Name\": [ncf_channels[i]] * len(bands),  # Repeating value directly\n",
    "            \"Bathymetry\": bathymetry\n",
    "        })\n",
    "\n",
    "    combined_df = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    combined_df['Channel_Name_Encoded'] = encoder.fit_transform(combined_df['Channel_Name'])\n",
    "\n",
    "    output = open(os.path.join(WORK_DIR, 'Channel_Name_label_encoders.pkl'), 'wb')\n",
    "    pickle.dump(encoder, output)\n",
    "    output.close()\n",
    "    \n",
    "    # Drop original categorical columns\n",
    "    combined_df.drop(columns=['Channel_Name'], inplace=True)\n",
    "\n",
    "    X = combined_df.drop(columns=['Bathymetry'])\n",
    "    y = combined_df['Bathymetry']\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Split temp into validation (15%) and test (15%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return data, combined_df, X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work directory\n",
    "WORK_DIR = '/home/clay/Documents/SDB/CESWG'\n",
    "\n",
    "S2_PATH =os.path.join(WORK_DIR, 'processed/S2')\n",
    "BATHY_PATH = os.path.join(WORK_DIR,'processed/Bathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveynames = os.listdir(BATHY_PATH)\n",
    "surveynames = [f[:-4] for f in os.listdir(BATHY_PATH) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 band manipulation and including other data (good reference is Chybicki et al. 2023)\n",
    "- Blue Green ratios (Blue/Green, Green/Blue, looking for others)\n",
    "- Stumpf log ratio of blue green (https://aslopubs.onlinelibrary.wiley.com/doi/10.4319/lo.2003.48.1_part_2.0547)\n",
    "- Coordinates\n",
    "- NCF channel ID name\n",
    "- Survey type\n",
    "- Spectral indices?? (NDVI and NDWI, NDWI could make sense but would leave this to the end)\n",
    "\n",
    "\n",
    "The Chybicki 2023 paper had models perform extremely well when including all bands, the Stumpf log ratio, and the UTM coordinates. I think including more blue-green ratios and the survey type as well will increase my accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, combined_data, X_train, y_train, X_test, y_test, X_val, y_val = prepare_train_data(surveynames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for test_train_split\n",
    "- Trains per pixel\n",
    "\n",
    "\n",
    "- k-fold segmentation for training?\n",
    "- try 3 regression models for now: SVM, RF, and XGBoost\n",
    "- may try ElasticNet from cuML, and some shallow NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dataset sizes\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RF Regression:\n",
    "\n",
    "After 191min of training (initial run)\n",
    "- R2 Score: 0.9818\n",
    "- RMSE: 2.1344      (ft)\n",
    "- MAE: 1.0686       (ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    criterion='squared_error',  \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt',                   # 'sqrt', 'log2', int, or float\n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    n_jobs=-1, \n",
    "    random_state=42, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None, \n",
    "    monotonic_cst=None\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, rf_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "mae = mean_absolute_error(y_test, rf_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "r2 = r2_score(y_val, val_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "mae = mean_absolute_error(y_val, val_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. XGBoost Regression\n",
    "\n",
    "n_estimators=500, learning_rate=0.3, max_depth=10, grow_policy= 'lossguide', booster= 'gbtree',:\n",
    "- R2 Score= 0.8529\n",
    "- RMSE= 6.0626\n",
    "- MAE= 3.9175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and configure the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500,      # Number of trees\n",
    "    learning_rate=0.3,     # Learning rate\n",
    "    max_depth=10,           # Maximum tree depth\n",
    "    grow_policy= 'lossguide',\n",
    "    booster= 'gbtree',\n",
    "    random_state=42        # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, xgb_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))\n",
    "mae = mean_absolute_error(y_test, xgb_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = xgb_model.predict(X_val)\n",
    "\n",
    "r2 = r2_score(y_val, val_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "mae = mean_absolute_error(y_val, val_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SVM regression:\n",
    "- R2 Score: \n",
    "- RMSE: \n",
    "- MAE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Model\n",
    "svm_model = SVR()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, svm_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, svm_predictions))\n",
    "mae = mean_absolute_error(y_test, svm_predictions)\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
