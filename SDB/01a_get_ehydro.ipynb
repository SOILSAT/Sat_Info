{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is going to download eHydro bathymetry data from the USACE ArcGIS REST repository, as well as retrieve cloud masked imagery of the same location, at the same time, for training of Satellite Derived Bathymetry model(s) for the National Channel Framework (NCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.wkt import loads\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.merge import merge\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "from pyproj import CRS\n",
    "import re\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ehydro_date_convert(time):\n",
    "    return datetime.utcfromtimestamp(time / 1000).strftime('%Y-%m-%d')\n",
    "\n",
    "def download_file(url, destination):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(destination, 'wb') as file, tqdm(\n",
    "        desc=f\"Downloading {os.path.basename(destination)}\",\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "def export_rasterized_bathymetry_native_crs(gdf, outpath, resolution):\n",
    "    \"\"\"\n",
    "    Exports a rasterized bathymetry GeoDataFrame in its original CRS.\n",
    "\n",
    "    Args:\n",
    "        gdf: GeoDataFrame with bathymetry polygons and a \"depthMean\" column.\n",
    "        outpath: Output file path for the raster.\n",
    "    \"\"\"\n",
    "    # Get the bounding box and resolution\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "\n",
    "    # Define the transform\n",
    "    transform = from_origin(xmin, ymax, resolution, resolution)\n",
    "\n",
    "    # Prepare raster shapes\n",
    "    shapes = [(geom, value) for geom, value in zip(gdf.geometry, gdf[\"depthMean\"])]\n",
    "\n",
    "    # Calculate the raster dimensions\n",
    "    height = int((ymax - ymin) / resolution)\n",
    "    width = int((xmax - xmin) / resolution)\n",
    "\n",
    "    # Rasterize the bathymetry data\n",
    "    raster = rasterize(\n",
    "        shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=np.nan,  # No data outside polygons\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "\n",
    "    # Save the raster in its original CRS\n",
    "    with rasterio.open(\n",
    "        outpath,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=\"float32\",\n",
    "        crs=gdf.crs,  # Keep the original CRS\n",
    "        transform=transform,\n",
    "        nodata=np.nan,\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "\n",
    "    print(f\"Raster in original CRS saved to {outpath}\")\n",
    "\n",
    "def export_rasterized_bathymetry_native_crs_2(gdf, outpath, resolution, method='cubic'):\n",
    "    \"\"\"\n",
    "    Exports a rasterized bathymetry GeoDataFrame with improved interpolation.\n",
    "\n",
    "    Args:\n",
    "        gdf: GeoDataFrame with bathymetry polygons and a \"depthMean\" column\n",
    "        outpath: Output file path for the raster\n",
    "        resolution: Grid resolution\n",
    "        method: Interpolation method ('linear', 'cubic', 'nearest')\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import griddata\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get the bounding box\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    \n",
    "    # Create regular grid\n",
    "    x_grid = np.arange(xmin, xmax, resolution)\n",
    "    y_grid = np.arange(ymin, ymax, resolution)\n",
    "    xx, yy = np.meshgrid(x_grid, y_grid)\n",
    "    \n",
    "    # Extract points and values from geodataframe\n",
    "    points = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf.geometry])\n",
    "    values = gdf[\"depthMean\"].values\n",
    "    \n",
    "    # Perform interpolation\n",
    "    grid_z = griddata(points, values, (xx, yy), method=method, fill_value=np.nan)\n",
    "    \n",
    "    # Define the transform\n",
    "    transform = from_origin(xmin, ymax, resolution, resolution)\n",
    "    \n",
    "    # Save the interpolated raster\n",
    "    with rasterio.open(\n",
    "        outpath,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=grid_z.shape[0],\n",
    "        width=grid_z.shape[1],\n",
    "        count=1,\n",
    "        dtype=\"float32\",\n",
    "        crs=gdf.crs,\n",
    "        transform=transform,\n",
    "        nodata=np.nan,\n",
    "    ) as dst:\n",
    "        dst.write(grid_z.astype('float32'), 1)\n",
    "    \n",
    "    print(f\"Interpolated raster saved to {outpath}\")\n",
    "\n",
    "def visualize_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        bathy = src.read(1)\n",
    "        xmin, ymin, xmax, ymax = src.bounds\n",
    "    \n",
    "    plt.imshow(\n",
    "        bathy,\n",
    "        extent=(xmin, xmax, ymin, ymax),\n",
    "        origin=\"lower\",\n",
    "        cmap=\"viridis\"\n",
    "    )\n",
    "    plt.colorbar(label=\"Depth (Feet)\")\n",
    "    plt.title(\"Rasterized Bathymetry\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "\n",
    "def get_reach_date(surveyname):\n",
    "    pattern = re.compile(r'([A-Za-z]{2}_[A-Za-z0-9]{2}_[A-Za-z]{3,4}_\\d{8})')\n",
    "    match = pattern.search(surveyname)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:  \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query bathy data\n",
    "AVAILABLE FIELD NAMES:\n",
    "- Field Name: OBJECTID, Type: esriFieldTypeOID\n",
    "- Field Name: surveyjobidpk, Type: esriFieldTypeString\n",
    "- Field Name: sdsid, Type: esriFieldTypeString\n",
    "- Field Name: sdsfeaturename, Type: esriFieldTypeString\n",
    "- Field Name: sdsmetadataid, Type: esriFieldTypeString\n",
    "- Field Name: surveytype, Type: esriFieldTypeString\n",
    "- Field Name: channelareaidfk, Type: esriFieldTypeString\n",
    "- Field Name: dateuploaded, Type: esriFieldTypeDate\n",
    "- Field Name: usacedistrictcode, Type: esriFieldTypeString\n",
    "- Field Name: surveydatestart, Type: esriFieldTypeDate\n",
    "- Field Name: surveydateend, Type: esriFieldTypeDate\n",
    "- Field Name: sourcedatalocation, Type: esriFieldTypeString\n",
    "- Field Name: sourceprojection, Type: esriFieldTypeString\n",
    "- Field Name: mediaidfk, Type: esriFieldTypeString\n",
    "- Field Name: projectedarea, Type: esriFieldTypeDouble\n",
    "- Field Name: sdsfeaturedescription, Type: esriFieldTypeString\n",
    "- Field Name: dateloadedenterprise, Type: esriFieldTypeDate\n",
    "- Field Name: datenotified, Type: esriFieldTypeDate\n",
    "- Field Name: sourcedatacontent, Type: esriFieldTypeString\n",
    "- Field Name: plotsheetlocation, Type: esriFieldTypeString\n",
    "- Field Name: sourceagency, Type: esriFieldTypeString\n",
    "- Field Name: globalid, Type: esriFieldTypeGlobalID\n",
    "- Field Name: Shape__Area, Type: esriFieldTypeDouble\n",
    "- Field Name: Shape__Length, Type: esriFieldTypeDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate search parameters for eHydro\n",
    "\n",
    "s2_cloud_cov = 20 ## percentage of clouds in sentinel-2 multispectral imagery, less means you see more surface\n",
    "# search_date = '2015-06-27'  # Date threshold, getting data from Sentinel-2A launch date to now\n",
    "search_date = '2019-01-01'      # change this, but going to try just training on the past 5ish years\n",
    "usace_code = \"CESWG\"        # Galveston District (for now)\n",
    "\n",
    "NUM_OF_QUERIES = 5          # number of iterations for the request to run\n",
    "QUERY_TIME_DELAY = 2        # query time delay in seconds, used when requesting all features\n",
    "URL = \"https://services7.arcgis.com/n1YM8pTrFmm7L4hs/ArcGIS/rest/services/eHydro_Survey_Data/FeatureServer/0/query\"\n",
    "DOWNLOAD_DIR = f'/home/clay/Documents/SDB/{usace_code}/bathy'\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the initial query\n",
    "params = {\n",
    "    'where': f\"surveydatestart >= '{search_date}' AND usacedistrictcode='{usace_code}'\",\n",
    "    'outFields': '*',  # Retrieve all fields\n",
    "    'resultRecordCount': 2000,  # Maximum records per request\n",
    "    'resultOffset': 0,  # Starting offset\n",
    "    'f': 'json',  # Output format\n",
    "    'outSR': '4326',  # Spatial reference\n",
    "}\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for i in range(NUM_OF_QUERIES):\n",
    "    response = requests.get(URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        features = data.get('features', [])\n",
    "        if not features:\n",
    "            break\n",
    "        all_features.extend(features)\n",
    "        params['resultOffset'] += params['resultRecordCount']\n",
    "        print(f\"Retrieved {len(features)} features.\")\n",
    "        # time.sleep(QUERY_TIME_DELAY)  # Delay of 1 second\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I've already got a ton of the ACOLITE files processed, might try to just match what I've got with what pops up in this search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for overlapping S2SAFE data\n",
    "- if no matching images, remove the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveykeys = [feature['attributes']['surveyjobidpk'] for feature in all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to redownload the surveys for the already processed ACOLITE data\n",
    "acolite = '/home/clay/Documents/SDB/CESWG/s2_SAFE'\n",
    "acolitepaths = [f for f in os.listdir(acolite)]\n",
    "matching_strings = list(set(acolitepaths) & set(surveykeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathyinfo = {}\n",
    "for i, feature in enumerate(all_features):\n",
    "    # gdf = gpd.GeoDataFrame(geometry=[loads(Polygon(feature['geometry']['rings'][0]).wkt)], \n",
    "    #                        crs=CRS.from_user_input(feature['attributes']['sourceprojection'])) \n",
    "\n",
    "    # # Convert to EPSG:4326 (lat/lon)\n",
    "    # gdf_4326 = gdf.to_crs(epsg=4326)\n",
    "    # wkt_4326 = gdf_4326.geometry.iloc[0].wkt\n",
    "    \n",
    "    bathyinfo[surveykeys[i]] = feature['attributes']['sourcedatalocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for survey in matching_strings:\n",
    "    file_path = os.path.join(DOWNLOAD_DIR, f\"{survey}.zip\")\n",
    "    try:\n",
    "        download_file(bathyinfo[survey], file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {bathyinfo[survey]}: {e}\")\n",
    "\n",
    "print('='*250)\n",
    "print(\"All files downloaded.\")\n",
    "\n",
    "# for survey, zip in bathyinfo.items():\n",
    "#     file_path = os.path.join(DOWNLOAD_DIR, f\"{survey}.zip\")\n",
    "#     try:\n",
    "#         download_file(zip, file_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to download {zip}: {e}\")\n",
    "\n",
    "# print('='*250)\n",
    "# print(\"All files downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipnames = [f[:-4] for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.zip')]\n",
    "if len(zipnames) > 0:\n",
    "    for name in zipnames:\n",
    "        zipfile_path = os.path.join(DOWNLOAD_DIR, f'{name}.zip')\n",
    "        with zipfile.ZipFile(zipfile_path,'r') as zip_ref:\n",
    "            zip_ref.extractall(zipfile_path[:-4])\n",
    "            os.remove(zipfile_path)\n",
    "    surveynames = [f for f in os.listdir(DOWNLOAD_DIR) if not f.startswith('.')]\n",
    "else:\n",
    "    surveynames = [f for f in os.listdir(DOWNLOAD_DIR) if not f.startswith('.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the .gdb file, specifically the \"Bathymetry_Vector\" layer and the \"depthMean\" and \"geometry\" columns\n",
    "- gets bathymetry raster in the native crs within eHydro\n",
    "- Sentinel-2 retreival notebook will get the bbox and convert to EPSG:4326 for image searching\n",
    "- Outputs in 10 ft resolution to match CSAT, but uses a simpler method since it will be reprojected to a coarser 10 meter resolution to match S2 anyways\n",
    "\n",
    "LOOK INTO SURVEYS WITH THE SAME NAME. SOME MAY HAVE MULTIPLE SURVEYS ON THE SAME DATE BUT WITH DIFFERENT TYPES OF SURVEYS, E.G., SOME ARE XC AND OTHERS ARE LABELED AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'/media/clay/Crucial/SDB/{usace_code}/bathy_rasters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Group GeoDataFrames by matching id\n",
    "merged_gdfs = {}\n",
    "for name in surveynames:\n",
    "    id = get_reach_date(name)\n",
    "    folder_path = os.path.join(DOWNLOAD_DIR, name)\n",
    "    gdbfile = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.gdb')][0]\n",
    "    gdf = gpd.read_file(gdbfile, layer='Bathymetry_Vector')\n",
    "    merged_gdfs.setdefault(id, []).append(gdf)\n",
    "\n",
    "# Merge the GeoDataFrames for each id and rasterize\n",
    "for id, gdf_list in merged_gdfs.items():\n",
    "    # Concatenate GeoDataFrames for this id\n",
    "    merged_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "    outfile = os.path.join(output_dir, f\"{id}.tif\")\n",
    "    export_rasterized_bathymetry_native_crs(merged_gdf, outfile, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, go to 01b_get_s2.ipynb to use the extent of the valid data in the created bathymetry rasters to get cloud-masked Sentinel-2 L2A products from Google Earth Engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
