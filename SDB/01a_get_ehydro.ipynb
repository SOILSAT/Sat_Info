{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is going to download eHydro bathymetry data from the USACE ArcGIS REST repository, as well as retrieve cloud masked imagery of the same location, at the same time, for training of Satellite Derived Bathymetry model(s) for the National Channel Framework (NCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import ee\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "from pykrige.ok import OrdinaryKriging  # PyKrige for Kriging interpolation\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project = '') ##enter your project name here as a string to initialize exchanges with ee api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gee_search_dates(time):\n",
    "    date_obj = datetime.utcfromtimestamp(time / 1000)\n",
    "    return ((date_obj - timedelta(days=1)).strftime('%Y-%m-%d'), (date_obj + timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "\n",
    "def ehydro_date_convert(time):\n",
    "    return datetime.utcfromtimestamp(time / 1000).strftime('%Y-%m-%d')\n",
    "\n",
    "def download_file(url, destination):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(destination, 'wb') as file, tqdm(\n",
    "        desc=f\"Downloading {os.path.basename(destination)}\",\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date, cloud_filter):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', cloud_filter)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    combined_coll = ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))\n",
    "\n",
    "    return combined_coll\n",
    "\n",
    "def interpolate_bathymetry(surveyname, resolution, storage_dir):\n",
    "    folder_path = os.path.join(DOWNLOAD_DIR, surveyname)\n",
    "    input_shapefile = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.gdb')][0]\n",
    "    output_raster = \"/tmp/bathy_interp.tif\"  # Output raster file path\n",
    "    clipped_raster = '/tmp/bath_clip.tif'\n",
    "    resampled_raster = os.path.join(storage_dir, f'{surveyname}.tif')\n",
    "    z_field = \"depthMean\"  # Attribute containing bathymetry or depth values\n",
    "    resolution = 10  # Desired pixel resolution in meters\n",
    "\n",
    "    gdf = gpd.read_file(input_shapefile, layer=\"Bathymetry_Vector\")\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds  # Get the extent of the layer\n",
    "\n",
    "    # Calculate raster width and height\n",
    "    width = round((xmax - xmin) / resolution)\n",
    "    height = round((ymax - ymin) / resolution)\n",
    "\n",
    "    # --- Step 3: Create the Raster Using gdal.Grid ---\n",
    "    gdal.Grid(\n",
    "        output_raster,                # Output raster path\n",
    "        input_shapefile,              # Input vector data\n",
    "        format=\"GTiff\",               # Output file format\n",
    "        algorithm=\"invdist\",          # Interpolation method (IDW)\n",
    "        zfield=z_field,               # Attribute containing bathymetry values\n",
    "        outputBounds=[xmin, ymin, xmax, ymax],  # Set bounds\n",
    "        width=width,                  # Number of columns\n",
    "        height=height,                # Number of rows\n",
    "        layers=\"Bathymetry_Vector\",   # Specify the layer\n",
    "        z_multiply=-1                 # Flip depths to negative\n",
    "    )\n",
    "    \n",
    "    # --- Step 4: Clip the Raster to the GDF Geometry ---\n",
    "    # Combine all geometries into a single boundary\n",
    "    geometry = [gdf.geometry.union_all()]\n",
    "\n",
    "    # Open the created raster and clip it using the GDF boundary\n",
    "    with rasterio.open(output_raster) as src:\n",
    "        clipped_image, clipped_transform = mask(\n",
    "            src, geometry, crop=True, nodata=np.nan\n",
    "        )\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": clipped_image.shape[1],\n",
    "            \"width\": clipped_image.shape[2],\n",
    "            \"transform\": clipped_transform,\n",
    "            \"nodata\": np.nan\n",
    "        })\n",
    "\n",
    "    # Save the clipped raster to a new file\n",
    "    with rasterio.open(clipped_raster, \"w\", **clipped_meta) as dst:\n",
    "        dst.write(clipped_image[0], 1)  # Access first band\n",
    "\n",
    "    # --- Step 5: Resample the Clipped Raster to 10m Resolution ---\n",
    "    gdal.Warp(\n",
    "        resampled_raster,       # Output resampled raster path\n",
    "        clipped_raster,         # Input clipped raster\n",
    "        xRes=resolution,              # Set pixel size in x direction\n",
    "        yRes=resolution,              # Set pixel size in y direction\n",
    "        resampleAlg=gdal.GRA_Bilinear, # Bilinear interpolation for resampling\n",
    "        targetAlignedPixels=True,     # Align pixels to the grid\n",
    "        dstNodata=np.nan              # Set NoData value to NaN\n",
    "    )\n",
    "\n",
    "    os.remove(output_raster)\n",
    "    os.remove(clipped_raster)\n",
    "    print(f\"Resampled raster saved to: {resampled_raster}\")\n",
    "\n",
    "\n",
    "# Densify lines and extract points\n",
    "def extract_points_from_contours(gdf, spacing):\n",
    "    points = []\n",
    "    depths = []\n",
    "    for idx, row in gdf.iterrows():\n",
    "        line = row.geometry\n",
    "        depth = row[z_field]\n",
    "        # Sample points along the line at a regular interval\n",
    "        for i in np.arange(0, line.length, spacing):\n",
    "            point = line.interpolate(i)  # Interpolate point at distance `i`\n",
    "            points.append(point)\n",
    "            depths.append(depth)\n",
    "    return points, depths\n",
    "\n",
    "def interpolate_bathymetry_with_kriging_from_contours(surveyname, resolution, storage_dir):\n",
    "    folder_path = os.path.join(DOWNLOAD_DIR, surveyname)\n",
    "    input_shapefile = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.gdb')][0]\n",
    "    output_raster = \"/tmp/bathy_interp_kriging.tif\"\n",
    "    clipped_raster = '/tmp/bathy_clip.tif'\n",
    "    reprojected_raster = os.path.join(storage_dir, f'{surveyname}_epsg4326.tif')\n",
    "    z_field = \"elevation\"  # Attribute containing contour elevation values\n",
    "\n",
    "    # --- Step 1: Load Contours and Extract Points ---\n",
    "    gdf = gpd.read_file(input_shapefile, layer=\"ElevationContour_ALL\")\n",
    "    spacing = resolution / 2  # Half the grid resolution for point extraction\n",
    "    points, depths = extract_points_from_contours(gdf, spacing)\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    points_gdf = gpd.GeoDataFrame({'geometry': points, z_field: depths}, crs=gdf.crs)\n",
    "\n",
    "    # Extract coordinates and depth values\n",
    "    coords = np.array([(point.x, point.y) for point in points_gdf.geometry])\n",
    "    depths = points_gdf[z_field].values\n",
    "\n",
    "    # --- Step 2â€“6: Follow Original Workflow ---\n",
    "    xmin, ymin, xmax, ymax = points_gdf.total_bounds\n",
    "    grid_x = np.arange(xmin, xmax, resolution)\n",
    "    grid_y = np.arange(ymin, ymax, resolution)\n",
    "\n",
    "    # Perform Kriging Interpolation\n",
    "    kriging_model = OrdinaryKriging(\n",
    "        coords[:, 0], coords[:, 1], depths, variogram_model=\"linear\"\n",
    "    )\n",
    "    grid_z, _ = kriging_model.execute(\"grid\", grid_x, grid_y)\n",
    "\n",
    "    transform = rasterio.transform.from_origin(xmin, ymax, resolution, resolution)\n",
    "    meta = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"dtype\": \"float32\",\n",
    "        \"nodata\": np.nan,\n",
    "        \"width\": grid_x.shape[0],\n",
    "        \"height\": grid_y.shape[0],\n",
    "        \"count\": 1,\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform\n",
    "    }\n",
    "\n",
    "    with rasterio.open(output_raster, \"w\", **meta) as dst:\n",
    "        dst.write(grid_z.T.astype(np.float32), 1)\n",
    "\n",
    "    geometry = [gdf.geometry.unary_union]\n",
    "    with rasterio.open(output_raster) as src:\n",
    "        clipped_image, clipped_transform = mask(src, geometry, crop=True, nodata=np.nan)\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": clipped_image.shape[1],\n",
    "            \"width\": clipped_image.shape[2],\n",
    "            \"transform\": clipped_transform,\n",
    "            \"nodata\": np.nan\n",
    "        })\n",
    "\n",
    "    with rasterio.open(clipped_raster, \"w\", **clipped_meta) as dst:\n",
    "        dst.write(clipped_image[0], 1)\n",
    "\n",
    "    gdal.Warp(\n",
    "        reprojected_raster, clipped_raster, dstSRS=\"EPSG:4326\",\n",
    "        xRes=resolution, yRes=resolution, resampleAlg=gdal.GRA_Bilinear, dstNodata=np.nan\n",
    "    )\n",
    "\n",
    "    os.remove(output_raster)\n",
    "    os.remove(clipped_raster)\n",
    "\n",
    "    print(f\"Reprojected raster saved to: {reprojected_raster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query bathy data\n",
    "AVAILABLE FIELD NAMES:\n",
    "- Field Name: OBJECTID, Type: esriFieldTypeOID\n",
    "- Field Name: surveyjobidpk, Type: esriFieldTypeString\n",
    "- Field Name: sdsid, Type: esriFieldTypeString\n",
    "- Field Name: sdsfeaturename, Type: esriFieldTypeString\n",
    "- Field Name: sdsmetadataid, Type: esriFieldTypeString\n",
    "- Field Name: surveytype, Type: esriFieldTypeString\n",
    "- Field Name: channelareaidfk, Type: esriFieldTypeString\n",
    "- Field Name: dateuploaded, Type: esriFieldTypeDate\n",
    "- Field Name: usacedistrictcode, Type: esriFieldTypeString\n",
    "- Field Name: surveydatestart, Type: esriFieldTypeDate\n",
    "- Field Name: surveydateend, Type: esriFieldTypeDate\n",
    "- Field Name: sourcedatalocation, Type: esriFieldTypeString\n",
    "- Field Name: sourceprojection, Type: esriFieldTypeString\n",
    "- Field Name: mediaidfk, Type: esriFieldTypeString\n",
    "- Field Name: projectedarea, Type: esriFieldTypeDouble\n",
    "- Field Name: sdsfeaturedescription, Type: esriFieldTypeString\n",
    "- Field Name: dateloadedenterprise, Type: esriFieldTypeDate\n",
    "- Field Name: datenotified, Type: esriFieldTypeDate\n",
    "- Field Name: sourcedatacontent, Type: esriFieldTypeString\n",
    "- Field Name: plotsheetlocation, Type: esriFieldTypeString\n",
    "- Field Name: sourceagency, Type: esriFieldTypeString\n",
    "- Field Name: globalid, Type: esriFieldTypeGlobalID\n",
    "- Field Name: Shape__Area, Type: esriFieldTypeDouble\n",
    "- Field Name: Shape__Length, Type: esriFieldTypeDouble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, will probably want to include options for with:\n",
    "- usace district\n",
    "- time of year (date and season)\n",
    "- NCF ID\n",
    "- survey type (single vs dual beam; XC, BD, AD, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate search parameters for eHydro\n",
    "\n",
    "s2_cloud_cov = 20 ## percentage of clouds in sentinel-2 multispectral imagery, less means you see more surface\n",
    "search_date = '2018-01-01'  # Date threshold, getting data from 2018 to present\n",
    "usace_code = \"CESWG\"        # Galveston District (for now)\n",
    "\n",
    "NUM_OF_QUERIES = 3          # number of iterations for the request to run\n",
    "QUERY_TIME_DELAY = 2        # query time delay in seconds, used when requesting all features\n",
    "URL = \"https://services7.arcgis.com/n1YM8pTrFmm7L4hs/ArcGIS/rest/services/eHydro_Survey_Data/FeatureServer/0/query\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the initial query\n",
    "params = {\n",
    "    'where': f\"surveydatestart >= '{search_date}' AND usacedistrictcode='{usace_code}'\",\n",
    "    'outFields': '*',  # Retrieve all fields\n",
    "    'resultRecordCount': 2000,  # Maximum records per request\n",
    "    'resultOffset': 0,  # Starting offset\n",
    "    'f': 'json',  # Output format\n",
    "    'outSR': '4326',  # Spatial reference\n",
    "}\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for i in range(NUM_OF_QUERIES):\n",
    "    response = requests.get(URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        features = data.get('features', [])\n",
    "        if not features:\n",
    "            break\n",
    "        all_features.extend(features)\n",
    "        params['resultOffset'] += params['resultRecordCount']\n",
    "        print(f\"Retrieved {len(features)} features.\")\n",
    "        time.sleep(QUERY_TIME_DELAY)  # Delay of 1 second\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract date and aoi from the surveys for GEE\n",
    "- plan is to iterate through the queries (probably by district code) and check to see if GEE has a corresponding Sentinel-2 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geeinfo = {}\n",
    "dates = []\n",
    "for feature in all_features:\n",
    "    dates.append(ehydro_date_convert(feature['attributes']['surveydatestart']))\n",
    "    area = ee.Geometry.Polygon(feature['geometry']['rings'][0])\n",
    "\n",
    "    date_tuple = get_gee_search_dates(feature['attributes']['surveydatestart'])\n",
    "\n",
    "    geeinfo[feature['attributes']['surveyjobidpk']] = [area, date_tuple]\n",
    "surveykeys = list(geeinfo.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Iterate through responses and check if GEE has corresponding image(s)\n",
    "- if not, the response will be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for survey, items in geeinfo.items():\n",
    "    aoi = items[0]\n",
    "    dates = items[1]\n",
    "\n",
    "    coll = get_s2_sr_cld_col(aoi, dates[0], dates[1], s2_cloud_cov)\n",
    "\n",
    "    if coll.size().getInfo() > 0:\n",
    "        geeinfo[survey].append(coll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all surveys that have initial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodsurveys = []\n",
    "for survey, items in geeinfo.items():\n",
    "    if len(items) > 2:\n",
    "        goodsurveys.append(survey)\n",
    "\n",
    "if len(goodsurveys) == 0:\n",
    "    print('No appropriate images were found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract the eHydro bathy data download urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathyinfo = {}\n",
    "for i, feature in enumerate(all_features):\n",
    "    bathyinfo[surveykeys[i]] = feature['attributes']['sourcedatalocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Download the eHydro data locally for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = f'/media/clay/SamsungExt/SDB/eHydro/{usace_code}/bathy'\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "for survey in goodsurveys:\n",
    "    file_path = os.path.join(DOWNLOAD_DIR, f\"{survey}.zip\")\n",
    "    try:\n",
    "        download_file(bathyinfo[survey], file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {bathyinfo[survey]}: {e}\")\n",
    "\n",
    "print('='*250)\n",
    "print(\"All files downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipnames = [f[:-4] for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.zip')]\n",
    "if len(zipnames) > 0:\n",
    "    for name in zipnames:\n",
    "        zipfile_path = os.path.join(DOWNLOAD_DIR, f'{name}.zip')\n",
    "        with zipfile.ZipFile(zipfile_path,'r') as zip_ref:\n",
    "            zip_ref.extractall(zipfile_path[:-4])\n",
    "            os.remove(zipfile_path)\n",
    "    surveynames = [f for f in os.listdir(DOWNLOAD_DIR)]\n",
    "else:\n",
    "    surveynames = [f for f in os.listdir(DOWNLOAD_DIR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the .gdb files from the survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you need to unzip\n",
    "gdbinfo = {}\n",
    "for name in surveynames:\n",
    "    folder_path = os.path.join(DOWNLOAD_DIR, name)\n",
    "    gdb_file = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.gdb')][0]\n",
    "    bathyvector = gpd.read_file(gdb_file, layer='Bathymetry_Vector')\n",
    "    contours = gpd.read_file(gdb_file, layer=\"ElevationContour_ALL\")\n",
    "\n",
    "    gdbinfo[name] = [bathyvector, contours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    display(gdbinfo[surveynames[i]][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 10m pixel resolution bathymetry rasters from the .gdb files\n",
    "Each .zip file comes with required and optional files. One of the required files is the .gdb file. This contains needed bathymetry and geospatial extents. Two files within the .gdb can be used to get the bathymetry: Bathymetry_Vector or ElevationContour_ALL. Bathymetry_Vector is a shapefile containing polygons assigned a mean depth. ElevationContour_ALL is a shapefile containing multipart lines with each line denoting an elevation. This raster generation for the bathymetry is done using the Bathymetry_Vector multipolygon shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_DIR = f'/media/clay/SamsungExt/SDB/eHydro/{usace_code}/bathy_rasters'\n",
    "os.makedirs(STORAGE_DIR, exist_ok=True)\n",
    "\n",
    "for key in gdbinfo.keys():\n",
    "    interpolate_bathymetry_with_kriging(key, 10, STORAGE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, go to 01b_get_s2.ipynb to use the extent of the valid data in the created bathymetry rasters to get cloud-masked Sentinel-2 L2A products from Google Earth Engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
